For immediate release The Federal Reserve Board on Wednesday announced the annual adjustments in the amount of net transaction accounts used in the calculation of reserve requirements and the cutoff level used to determine the detail and frequency of deposit reporting. All depository institutions must retain a percentage of certain types of deposits in the form of vault cash, or as a deposit in a Federal Reserve Bank, or in a pass-through account at a correspondent institution. Reserve requirements currently are assessed on the depository institution's net transaction accounts (mostly checking accounts). For net transaction accounts in 2004, the first $6.6 million, up from $6.0 million in 2003, will be exempt from reserve requirements. A 3 percent reserve ratio will be assessed on net transaction accounts over $6.6 million up to and including $45.4 million, up from $42.1 million in 2003. A 10 percent reserve ratio will be applied above $45.4 million. These annual adjustments, known as the low reserve tranche adjustment and the reservable liabilities exemption adjustment, are based on growth in net transaction accounts and total reservable liabilities, respectively, at all depository institutions between June 30, 2002 and June 30, 2003. For depository institutions that report weekly, the low reserve tranche adjustment and the reservable liabilities exemption adjustment will apply to the reserve computation period that begins Tuesday, November 25, 2003 and the corresponding reserve maintenance period that begins Thursday, December 25, 2003. (more) For institutions that report quarterly, the low reserve tranche adjustment and the reservable liabilities exemption adjustment will apply to the reserve computation period that begins Tuesday, December 16, 2003, and the corresponding seven-day reserve maintenance period that begins Thursday, January 15, 2004. Additionally, the Board increased the deposit cutoff level that is used with the exemption level to determine the frequency and detail of deposit reporting. The attached Federal Register notice contains a description of the new boundaries for deposit reporting that will be effective September 2004.
Remarks by Governor Edward M. Gramlich Before the Economic Club of Toronto, Toronto, Canada October 1, 2003 Maintaining Price Stability One of the remarkable economic developments of the past two decades is the sharp, worldwide reduction in rates of inflation. Before the 1980s, research papers, economic commentary, and textbooks here and abroad were full of discussions of the causes and consequences of high inflation and of the political difficulty of bringing it under control. It looked then like inflation had become a more or less permanent feature of the economic landscape. Concepts associated with de flation such as liquidity traps and the zero bound on nominal interest rates had, for practical purposes, disappeared from economic thought. But, beginning around 1980, central banks around the world seem to have ganged up on inflation, fought the fight, and won. The International Monetary Fund's consumer price index for industrial countries registered an annual rate of inflation exceeding 12 percent in 1980, but by 2002 the rate had dropped to just 1.4 percent. In the United States, consumer price inflation excluding food and energy fell from 12.4 percent in 1980 to 2.4 percent in 2002. Japan has actually had negative rates of inflation for the past four years. The drop in inflation in many prominent emerging-market countries has been even more breathtaking (Rogoff, 2003). Perhaps just as remarkable as the abrupt fall in worldwide inflation is the widespread agreement on the reasons for its fall. Virtually all economic analysts would give a primary role to the implementation of plain old economic theory. Macroeconomists had long understood that, in the famous words of Milton Friedman, "inflation is everywhere and always a monetary phenomenon." But somehow this theoretical agreement never was translated into action. Perhaps because of advances in understanding the inflation process, perhaps because of worrisome increases in inflation rates, perhaps because of a discovery that the economic costs of fighting inflation were not as high as previously believed, central bankers in the 1980s suddenly began to use their monetary control to limit the rate of increase in prices. It is not stretching things to call this a revolution in central banking. Now that something like price stability has arrived, two important policy questions present themselves: Is a mere determination to achieve price stability adequate for central banks to stabilize prices, or should their determination be backed up by more formal institutional structures, such as in inflation-targeting regimes? Exactly how stable do we want prices to be--rising at a measured rate of zero or at some other rate? Maintaining Price Stability Price stability is widely understood to be a feasible objective and to be sustainable with little long-run cost in terms of output and unemployment. Indeed, many believe that greater price stability should also stabilize output, and that the improved allocation of resources coming from price stability should promote productivity and raise living standards in the long run. As a part of this consensus, central banks around the developed world are now firmly determined to achieve and maintain price stability. But there is a significant debate about whether determination alone is enough. Much of the world--countries in the British Commonwealth (the United Kingdom, Australia, Canada, and New Zealand), in Scandinavia (Finland, Norway, and Sweden), in eastern Europe (the Czech Republic, Hungary, and Poland) and many emerging-market countries (Brazil, Chile, Colombia, Mexico, Peru, the Philippines, South Africa, and Thailand)--have buttressed this determination to maintain price stability by adopting formal inflation-targeting regimes. The regimes vary, but the key element is an explicit commitment to meet a publicly stated numerical target rate of inflation within a particular time frame. The targets and commitments are accompanied by reports giving inflation forecasts and plans for meeting inflation goals. All regimes provide some leeway in meeting their goals--inflation rates do not have to be brought down to the target levels immediately, and special factors can be taken into account. But the regimes must have a credible plan for meeting inflation targets at least over the medium run. And because these plans are public, they can be embarrassing if not fulfilled. Recently the European Central Bank (ECB) has adopted a system that has some elements of an inflation-targeting regime, though it lacks many of the accountability features. A few industrial countries--the United States, Japan, and Switzerland--have resisted pressures to adopt formal inflation-targeting regimes. Debates continue in these countries about the desirability of adopting more formal inflation-targeting procedures. The inflation targeters point to the advantages of transparency, commitment, and accountability; non-targeters point to the loss of flexibility and the reduction in the ability to meet alternative goals such as high employment and financial stability. Rudebusch and Walsh (1998) give a good summary of this debate. To me, the verdict on theoretical grounds alone has always been a close call. I can see the advantages of transparency and commitment and of giving financial markets some indication about the central bank's inflation goals. I can also see the fear that, in practice, such regimes may in fact constrain actions in ways that could be rigid, inflexible, and perhaps unnecessary in bringing down inflation. Rather than rehash these theoretical issues one more time, let me instead invoke some empirical evidence. There are two ways to evaluate the effect of formal inflation-targeting regimes. One is by time-series analysis; the other by cross-section analysis. In the early days of inflation targeting, a number of researchers took the first approach and compared the inflation performance of targeting countries before and after the inflation-targeting regime was introduced. In these studies, which measured actual inflation rates and inflation premiums on long-term bonds (to measure expected inflation), one could find evidence that inflation targeting seemed to have worked to bring down inflation and its persistence (see, for example, Kuttner and Posen, 1999; and Freeman and Willis, 1995). But these early studies took place at the time of a worldwide drop in inflation rates. Just as inflation-targeting countries could show elements of improved inflation performance, nontargeting countries could as well. The second approach to studying the effect of targeting, cross-section analysis, compares inflation performance for targeting and nontargeting countries. The advent of the ECB created a huge impediment to such studies because it came into being midway through the relevant data period, and it initiated a centralized regime that had aspects of inflation targeting for many of the large countries that had previously not had such a regime. It is difficult to know whether the ECB countries should be considered inflation targeters in the last half of the time-series sample, and it is especially difficult to interpret the experience of these countries. For what they are worth, the raw cross-sectional data are shown in table 1. Over the 1994-2002 period, seven prominent inflation-targeting industrial countries had a mean inflation rate of consumer prices of 2.1 percent per year with a standard deviation of 1.7 percent. Both numbers are slightly higher than the mean and standard deviation of the eleven prominent nontargeting industrial countries. Eliminating the labeled ECB countries from both groups leads to a targeting mean of 2.0 percent and a targeting standard deviation of 1.7 percent, with both numbers again slightly higher than the mean and standard deviation for the three nontargeters. One could consider Japan a special case and eliminate it from the latter group, but the two-country results would still show slightly lower and more stable inflation in the nontargeting countries. Ingenious economists could devise much more elaborate tests. Ball and Sheridan (2003) have done that, also making an adjustment for initial rates of inflation. They again find no significant differences between targeting and nontargeting countries. Their conclusion is similar to an earlier cross-section finding of Cecchetti and Ehrmann (1999) that inflation aversion increased in both targeting and nontargeting countries. To summarize these international data, one might say that something brought inflation down in the 1980s and 1990s, but success was fairly uniform across both the inflation-targeting and nontargeting countries. When one does "before and after" tests for individual inflation-targeting countries spanning this period, inflation targeting often looks successful in bringing about low rates of inflation and in reducing inflation variability and persistence. But by the late 1990s, inflation had fallen in all industrial countries, and it is hard to find much difference in inflation performance between inflation-targeting and nontargeting countries--if anything, inflation has been lower and more stable in the latter group. One is therefore tempted to conclude that the general understanding of the inflation process and a firm determination to achieve price stability, more than the inflation-targeting regime itself, has been the key element in reducing inflation. How Stable Should Prices Be? Collectively, the central banks of the world have done well in reducing inflation, generally moving from an inflationary regime to one of reasonably stable prices. But now the goal question gets harder--how far should these central banks go? All the way to exactly stable prices with a measured inflation rate that averages zero, or somewhat short of that goal? I will argue strongly that central banks should stop short of zero--that a low positive rate of inflation is optimal. There are two strategic reasons for choosing a low, positive rate, and both have been debated extensively in the economic literature. One involves the so-called zero bound on nominal interest rates; the other involves labor markets. Regarding the zero bound, as inflation drops toward zero, nominal interest rates drop toward real rates, which themselves might be low if the reason inflation is dropping involves negative demand shocks. At these low rates, the central bank is poorly positioned to respond to further negative demand shocks. Because of the zero bound, it is impossible for nominal interest rates to decline further, but it is still possible for inflation to decline to below zero--that is, for deflation to set in. The combination implies that real interest rates may actually start to rise in the face of negative demand shocks. In this range, the central bank loses much of its ability to respond to these negative demand shocks by lowering real rates of interest. As many have pointed out, the central bank could still respond to negative demand shocks by expanding money growth in various "nontraditional" ways. But these nontraditional approaches are untested. Macro-model simulations indicate that if the central bank exercises only traditional policy instruments, the zero-bound problem could significantly increase the severity of recessions. The virulence of the Japanese recession also suggests that zero-bound problems could be serious. Today the standard thinking is that the central bank can and should avoid dealing with these difficulties by guiding the economy toward a low, positive rate of inflation, not a zero rate. The second argument for avoiding mathematical zero involves labor markets. A good deal of evidence suggests that, in the economy at large, employers are reluctant to cut workers' nominal wages or, at least, downward wage rigidities exceed upward rigidities. The normal market process of reallocation of labor in response to industry or sectoral shifts works by having real wages adjust to keep them in line with workers' marginal products of labor. If nominal wages cannot be cut for workers who, for whatever reason, become less productive, labor market flexibility is enhanced by having low positive overall rates of inflation that effectively cut real wages for these workers. To these strategic reasons for avoiding mathematical zero can be added a measurement reason. One of the great difficulties in compiling price indexes is in adjusting them for changes in the quality of goods and services. How does one measure the value of a new good or of a service that benefits from a new technology? Casual evidence indicates that failure to make accurate adjustments can create a bias that could be quite large--indeed, King (2002) gives some heuristic illustrations that suggest that the true prices of some consumer durables have not risen at all since as far back as 1915! The statistical agencies compiling consumer prices in the United States--the Bureau of Labor Statistics for the consumer price index (CPI) and the Department of Commerce for the personal consumption expenditure deflator (PCE)--try valiantly to deal with these quality-change biases. But the weight of professional opinion is that, even after years of trying to make corrections, the indexes are still biased upward. Since no ready methods exist for correcting some of these biases, it seems appropriate simply to adjust for them. It is generally agreed that the biases are about 1 percentage point per year for the CPI and about 1/2 percentage point per year for the PCE deflator (Lebow and Rudd, 2003). Whichever inflation target one uses, there is again a measurement argument for steering away from mathematical zero and toward a measured rate of inflation of from 1/2 point to 1 point per year above the desired economic cushion. A final argument for steering away from mathematical zero invokes popular preferences, always an elusive topic. When economists have asked people whether inflation is harmful and whether controlling it should be a high priority for government, overwhelming percentages of the population agree. But when people are probed further, their main reason for disliking inflation appears to be the perception that it hurts their standard of living (Shiller,1997). Popular opinion seems to view personal wage increases as given--that is, not related to inflation--and focuses only on price increases, in which case inflation does look like a destroyer of real income. To economists, on the other hand, inflation should be interpreted to mean a general rise in all wages and prices; and the costs of general inflation are the "shoe leather"-type costs of managing cash balances, which the respondents to Shiller's survey viewed as trivial. Hence while inflation is clearly unpopular with the general public, it is not clear that the public is properly identifying the true costs of inflation when making its judgment. A potentially more appropriate way to ascertain the true degree of inflation was suggested by Richard Ruggles of Yale University. I learned about this test in graduate school many years ago, and Ruggles may have made his suggestion many years before that. What inflation rates should really measure is the decline in the utility value of a nation's currency. In principle the right test is to offer a sample of the population a constant amount of currency, say $1,000, along with the opportunity to spend it on a menu of all goods and services available this year or a menu of all goods and services available a while back, say five years ago. If this sample of the population votes in equal numbers for this year's menu and for that available five years ago, one can conclude that prices have been stable over the five-year period. If the majority vote is for the earlier menu of goods and services, one can conclude that prices have risen, or that the utility value of the $1,000 has decreased. If the majority vote is for the recent menu, one can conclude the reverse--that true prices have actually declined. I have seen no rigorous polling evidence on this question. But for years in teaching college macroeconomics courses, and recently at the Fed, I have conducted such a poll among my audiences. All audiences have reported that their understanding of what inflation is all about was much improved by this thought experiment. Generally, college students have voted for the current menu even in times when the aggregate rate of price increase averaged 3 percent or more, implying that they felt that true prices had actually declined. College students may be unusually influenced by fads that do not truly improve goods (narrow or wide ties, etc.), and the implicit bias in measured price indexes may well be overstated by collegiate polls. Since coming to the Fed, I have had the opportunity to talk to and poll many banker groups about inflation, and as one would expect, they are generally more inclined to vote for the earlier menu of goods than were my college students, at any given rate of inflation. But these days, when measured rates of inflation are running at 1.5 percent to 2 percent, even bankers consistently vote for the current menu of goods by fairly wide margins. If even bankers feel that the implicit measurement bias in price indexes exceeds 2 percent, that may be a phenomenon worth noting. The upshot of this highly anecdotal test is that I have long suspected that true price stability might really be achieved in the vicinity of measured inflation rates of 2 percent or even more. It would be desirable to ground this type of information more firmly in modern-day techniques of data collection. Ideally, consumer survey groups would run polls of currency utility and make periodic reports, the way they already do for indicators of consumer confidence and spending plans. Short of that, there is one econometric calculation that provides some support to my informal voting results. Nordhaus (1998) compared real incomes as measured by the CPI with Survey Research Center data in which people were asked how their financial condition changed over the past year. He fitted a regression to the data, finding that equal shares of the population considered themselves better and worse off when the measured inflation rate was 1.5 percent, a result suggesting a 1.5 percent measurement bias in the CPI. Nordhaus's method requires that the entire income distribution move at the same rate. When Krueger and Siskind (1998) modified the test with more fine-grained distributional calculations, the estimated bias was reduced. Summary So where does this leave me on the two policy questions I promised to address? I find myself on the soft side of each of them. The first question is whether the U.S. central bank should adopt a more formal inflation-targeting regime. I personally would not go that far. My reading of the empirical evidence is that the key ingredient in keeping inflation low and stable is that the central bank be firmly determined to achieve and maintain stable prices in the long run. I believe the Fed is already so determined, with every member of the Federal Open Market Committee (FOMC) since I have been here repeating this mantra often. To me, there does not seem to be huge value in further tying down the committee through a formal inflation-targeting regime, and there could be some costs. On the other hand, it may be possible to get some of the transparency and accountability advantages of inflation targeting, and to lock in the gains from having reduced inflation, by going to an intermediate approach. The FOMC might simply announce its preferred long-run range for inflation. This range should be understood as a preferred range that would not bind the committee or override other important objectives of monetary policy. It should clearly be understood as a long-term objective, not a short-term objective. The FOMC would not have to defend any deviations from the preferred range. Perhaps such a step would increase transparency without limiting central bank flexibility to any appreciable degree. If we were to adopt a preferred range, what should it be? In light of the strategic considerations mentioned above, along with quantifiable measurement error, I would personally set the bottom of the range at slightly above 1 percent per year for the core PCE deflator, the Fed's preferred inflation measure. Because of audience polls, and at least until they are replaced by more rigorous information, I would set the top of the range at about 2.5 percent per year. The midpoint of this range is then slightly less than 2 percent per year, which turns out to be about what U.S. core PCE inflation has averaged over the past eight years. But I would stress the range more than the point estimate. I might close by stressing again the uncertainty involved in answering both questions addressed in the paper. There is theoretical uncertainty about how well formal inflation targeting should work and empirical uncertainty about how well it has worked in those countries that have tried it. If we were to move in the direction of a more systematic approach, I personally would go to a preferred range for inflation rather than a particular target and without all of the other trappings of a formal inflation-targeting regime. Finally, if the FOMC were to adopt a preferred range, I feel that efforts to quantify measurement bias in price indexes have, on the whole, been too conservative. My own personal preference is that the top of the range could go as high as 2.5 percent per year. Table 1 Inflation Performance in Selected Industrial Countries, 1994-2002 (Percent annual average; broad index of prices for all consumer goods) Country Mean Standard deviation Inflation targeters Australia 2.7 2.9 Canada 1.7 1.6 Finland (ECB) 1.6 1.3 New Zealand 2.1 1.8 Spain (ECB) 3.1 1.8 Sweden 1.2 1.5 United Kingdom 2.4 .7 Nontargeters Austria (ECB) 1.8 1.1 Belgium (ECB) 1.8 1.4 France (ECB) 1.4 1.0 Germany (ECB) 1.7 1.2 Ireland (ECB) 2.9 1.9 Italy (ECB) 2.9 1.4 Japan .1 1.3 Netherlands (ECB) 2.6 1.3 Portugal (ECB) 3.3 1.3 Switzerland 1.2 1.5 United States 2.3 1.0 Memo: Mean, targeters 2.1 1.7 Mean, nontargeters 2.0 1.3 Mean, non-ECB targeters 2.0 1.7 Mean, non-ECB nontargeters 1.2 1.3 Note: ECB means that as of 1999 the country had joined the European Central Bank and had a common monetary policy. References Ball, Laurence, and Niamh Sheridan. "Does Inflation Targeting Matter?" International Monetary Fund Working Paper WP/03/129, 2003. Cecchetti, Stephen G., and Michael Ehrmann. "Does Inflation Targeting Increase Output Volatility? An International Comparison of Policymakers' Preferences and Outcomes." National Bureau of Economic Research Working Paper 7426, 1999. Freeman, Richard T., and Jonathan L. Willis, "Inflation Targeting in the 1990s: Recent Challenges." Federal Reserve Board, International Finance Discussion Paper 525, 1995. King, Mervyn. "The Inflation Target Ten Years On." Address to the London School of Economics, November 19, 2002. Krueger, Alan B., and Aaron Siskind. "Using Survey Data to Assess Bias in the Consumer Price Index," Monthly Labor Review, April 1998, pp. 24-33. Kuttner, Kenneth N., and Adam S. Posen. "Does Talk Matter After All? Inflation Targeting and Central Bank Behavior." Federal Reserve Bank of New York, Staff Reports 88, October 1999. Lebow, David E., and Jeremy B. Rudd. "Measurement Error in the Consumer Price Index: Where Do We Stand?" Journal of Economic Literature, March 2003, pp. 59-201. Nordhaus, William D. "Quarterly Change in Price Indexes," Journal of Economic Perspectives, 12:1, Winter 1998, pp. 59-68. Rogoff, Kenneth. "Globalization and Global Disinflation," in Federal Reserve Bank of Kansas City, Monetary Policy and Uncertainty: Adapting to a Changing Economy, forthcoming. Rudebusch, Glenn D., and Carl E. Walsh. "U.S. Inflation Targeting: Pro and Con." Federal Reserve Bank of San Francisco, FRBSF Economic Letter, 98-18, May 29, 1998. Shiller, Robert J. "Why Do People Dislike Inflation?" in Christina D. Romer and David H. Romer, eds., Reducing Inflation: Motivation and Strategy. Chicago: University of Chicago Press, 1997, pp. 13-65. Inflation targeters Australia 2.7 2.9 Canada 1.7 1.6 Finland (ECB) 1.6 1.3 New Zealand 2.1 1.8 Spain (ECB) 3.1 1.8 Sweden 1.2 1.5 United Kingdom 2.4 .7 Nontargeters Austria (ECB) 1.8 1.1 Belgium (ECB) 1.8 1.4 France (ECB) 1.4 1.0 Germany (ECB) 1.7 1.2 Ireland (ECB) 2.9 1.9 Italy (ECB) 2.9 1.4 Japan .1 1.3 Netherlands (ECB) 2.6 1.3 Portugal (ECB) 3.3 1.3 Switzerland 1.2 1.5 United States 2.3 1.0 Memo: Mean, targeters 2.1 1.7 Mean, nontargeters 2.0 1.3 Mean, non-ECB targeters 2.0 1.7 Mean, non-ECB nontargeters 1.2 1.3
For immediate release The Federal Reserve Board announced today that the Consumer Advisory Council will hold its next meeting on Thursday, October 23. The meeting will take place in Dining Room E, Terrace level, in the Board's Martin Building. The session will begin at 9:00 a.m. and is open to the public. Anyone planning to attend the meeting should, for security purposes, register no later than Tuesday, October 21, by completing the form found on-line at: Additionally, attendees must present photo identification to enter the building. The Council's function is to advise the Board on the exercise of its responsibilities under various consumer financial services laws and on other matters on which the Board seeks its advice. Time permitting, the Council will discuss the following topics: Payroll Cards Funding and Long-term Sustainability of Non-profit Organizations Convenience Checks Issued in Connection with Credit Cards Reports by committees and other matters initiated by the Council members may also be discussed. The Board invites comments from the public on any of these matters. The Board's notice is attached.
Remarks by Governor Ben S. Bernanke At the Fall 2003 Banking and Finance Lecture, Widener University, Chester, Pennsylvania October 2, 2003 Governor Bernanke presented identical remarks at the London School of Economics Public Lecture, London, England, October 9, 2003 Monetary Policy and the Stock Market: Some Empirical Results The ultimate objective of monetary policymakers is to promote the health of the U.S. economy, which we do by pursuing our mandated goals of price stability and maximum sustainable output and employment. However, the effects of our policy instruments, such as the short-term interest rate, on these goal variables are indirect at best. Instead, monetary policy actions have their most direct and immediate effects on the broader financial markets, including the stock market, government and corporate bond markets, mortgage markets, markets for consumer credit, foreign exchange markets, and many others. If all goes as planned, the changes in financial asset prices and returns induced by the actions of monetary policymakers lead to the changes in economic behavior that the policy was trying to achieve. Thus, understanding how monetary policy affects the broader economy necessarily entails understanding both how policy actions affect key financial markets, as well as how changes in asset prices and returns in these markets in turn affect the behavior of households, firms, and other decisionmakers. Studying these links is an ongoing enterprise of monetary economists both within and outside the Federal Reserve System. The link between monetary policy and the stock market is of particular interest. Stock prices are among the most closely watched asset prices in the economy and are viewed as being highly sensitive to economic conditions. Stock prices have also been known to swing rather widely, leading to concerns about possible "bubbles" or other deviations of stock prices from fundamental values that may have adverse implications for the economy. It is of great interest, then, to understand more precisely how monetary policy and the stock market are related. In my talk today, I will report the results of research that I have done on this topic with Kenneth Kuttner of the Federal Reserve Bank of New York, as well as the findings of some related work done both within and outside the Federal Reserve System. The views I will express today, however, are my own and not necessarily those of my colleagues on the Federal Open Market Committee (FOMC) or the Board of Governors of the Federal Reserve System. In our research, Kuttner and I asked two questions. First, by how much do changes in monetary policy affect equity prices? As you will see, we focus on changes in monetary policy that are unanticipated by market participants because anticipated changes in policy should already be discounted by stock market investors and, hence, are unlikely to affect equity prices at the time they are announced. We find an effect of moderate size: Monetary policy matters for the stock market but, on the other hand, it is not one of the major influences on equity prices. Our second question, both more interesting and more difficult, is, why do changes in monetary policy affect stock prices? We come up with a rather surprising answer, at least one that was surprising to us. We find that unanticipated changes in monetary policy affect stock prices not so much by influencing expected dividends or the risk-free real interest rate, but rather by affecting the perceived riskiness of stocks. A tightening of monetary policy, for example, leads investors to view stocks as riskier investments and thus to demand a higher return to hold stocks. For a given path of expected dividends, a higher expected return can be achieved only by a fall in the current stock price. As we will see, this finding has interesting implications for several issues, including the role of stock prices in transmitting the effects of monetary policy actions to the broader economy and the potential effectiveness of monetary policy in "pricking" putative bubbles in the stock market. I will come back to these issues at the end of my talk. I start, however, with the problem of measuring the effect of monetary policy on the stock market. The Effect of Monetary Policy Actions on the Stock Market Normally, the FOMC, the monetary policymaking arm of the Federal Reserve, announces its interest rate decisions at around 2:15 p.m. following each of its eight regularly scheduled meetings each year. An air of expectation reigns in financial markets in the few minutes before to the announcement. If you happen to have access to a monitor that tracks key market indexes, at 2:15 p.m. on an announcement day you can watch those indexes quiver as if trying to digest the information in the rate decision and the FOMC's accompanying statement of explanation. Then the black line representing each market index moves quickly up or down, and the markets have priced the FOMC action into the aggregate values of U.S. equities, bonds, and other assets. On occasion, if economic conditions warrant, the FOMC may decide to make a change in monetary policy on a day that falls between regularly scheduled meetings, a so-called intermeeting move. Intermeeting moves, typically agreed upon during a conference call of the Committee, nearly always take financial markets by surprise, at least in their precise timing, and they are often followed by dramatic swings in asset prices. Even the casual observer can have no doubt, then, that FOMC decisions move asset prices, including equity prices. Estimating the size and duration of these effects, however, is not so straightforward. Because traders in equity markets, as in most other financial markets, are generally highly informed and sophisticated, any policy decision that is largely anticipated will already be factored into stock prices and will elicit little reaction when announced. To measure the effects of monetary policy changes on the stock market, then, we need to have a measure of the portion of a given change in monetary policy that the market had not already anticipated before the FOMC's formal announcement. Fortunately, the financial markets themselves are a source of useful information about monetary policy expectations. As you may know, the FOMC implements its decisions about monetary policy by changing its target for a particular short-term interest rate, the federal funds rate. The federal funds rate is the rate at which depository institutions borrow and lend reserves to and from each other overnight; although the Federal Reserve does not control the federal funds rate directly, it can do so indirectly by varying the supply of reserves available to be traded in this market. Since October 1988, financial investors have been able to hedge and speculate on future values of the federal funds rate by trading contracts in a futures market, overseen by the Chicago Board of Trade. Investors in this market have a strong financial incentive to try to guess correctly what the federal funds rate will be, on average, at various points in the future. The existence of a market in federal funds futures is a boon not only to investors, such as banks, which want to protect themselves against changes in the cost of reserves, but also to both policymakers and researchers, because it allows any observer to infer from the sale prices of futures contracts the values of the federal funds rate that market participants anticipate at various future dates. Previous research (Krueger and Kuttner, 1996; Owens and Webb, 2001) has shown that participants in this market collectively do a good job of forecasting future values of the funds rate, efficiently incorporating available information about likely future monetary policy actions. By using data from the federal funds futures market, then, it is possible to estimate the value at which financial market participants expect the FOMC to set its target for the federal funds rate on any given date. By comparing this expected value to what the FOMC actually did at each date, we can determine the portion of the Fed's interest rate decision that came as a surprise to financial markets. In our research, Kuttner and I considered all the dates of scheduled FOMC meetings plus all the dates on which the FOMC changed the federal funds rate between meetings, or made intermeeting moves, for the period May 1989 through December 2002, amounting to a total of 131 observations. For each of these dates, we used the expected value of the federal funds rate as inferred from the futures market to divide the actual change in the federal funds rate on that day into the part that was anticipated by the markets and the part that was unanticipated. So, for example, on November 6, 2002, the Federal Reserve cut the federal funds rate by 50 basis points. (A basis point equals 1/100 of a percentage point, so a 50-basis-point cut equals a cut of 1/2 percentage point.) However, this cut in the federal funds rate was not entirely unexpected; indeed, according to the federal funds futures market, investors were expecting a cut of about 31 basis points, on average, from the Fed at that meeting. So, of the 50 basis points that the FOMC lowered its target for the federal funds rate last November 6, only 19 basis points were a surprise to financial markets and thus should have been expected to affect asset prices. Note, by the way, that if the Fed had not changed interest rates at all that day, our method would have treated that action as the equivalent of a surprise tightening of policy of 31 basis points because the Fed would have done nothing while the market was expecting an easing of 31 basis points. To evaluate the effect of monetary policy on the stock market, we looked at how broad measures of stock prices moved on days on which the Fed made unanticipated changes to policy. I can illustrate our method by continuing the example of the Fed's cut in the federal funds rate last November 6. On that day, the broad stock market index we used in our study (the value-weighted index constructed by the Center for Research in Securities Prices at the University of Chicago) rose in value by 0.96 percentage point. Dividing the 96-basis-point gain in the stock market by the 19-basis-point downward surprise in the funds rate, we obtain a value of approximately 5 for the "stock price multiplier" relating policy changes to stock market changes. If this one day were representative, we would conclude that each basis point of surprise monetary easing leads to about a 5-basis-point increase in the value of stocks. Or, choosing magnitudes that might be more helpful to the intuition, we could just as well say that a surprise cut of 25 basis points in the federal funds rate should lead the stock market to rise, on the same day, about 1.25 percentage points--about 120 points on the Dow Jones index at its current value. In fact, applying a formal regression analysis to the full sample from 1989 to 2002, we found a number fairly close to this one, namely, a stock price multiplier for monetary policy of about 4.7. We also found, as expected, that changes in monetary policy that were anticipated by the market had small and statistically unimportant effects on stock prices, presumably because these changes had already been priced into stocks. Although a stock price multiplier of about five for unanticipated changes in the federal funds rate is certainly not negligible, we should appreciate that unexpected changes in monetary policy account for a tiny portion of the overall variability of the stock market. Unanticipated movements in the federal funds rate of 20 basis points or more are relatively rare (we observed only thirteen examples in our fourteen-year sample). Yet the change of one percent or so in the stock market induced by the typical 20-basis-point "surprise" in the funds rate is swamped by the overall variability of stock prices. For example, over the past five years, the broad stock market has moved one percent or more on about 40 percent of all trading days. Thus, news about monetary policy contributes very little to the day-to-day fluctuations in stock prices. We explored our empirical results with some care. We noted, for example, that a few of the monetary policy changes in our sample were followed by what seemed to be excessive or otherwise unusual stock market responses. A number of these responses occurred rather recently, during the Fed's series of rate cuts in 2001. The Fed's surprise intermeeting cuts of 50 basis points each on January 3 and April 18 of that year were both greeted euphorically by the stock market, with one-day increases in stock values of 5.3 percent and 4.0 percent, respectively. By contrast, the rate cut of 50 basis points on March 20, 2001, was received less enthusiastically. Even though the cut was more or less what the futures market had been anticipating, the financial press reported that many equity market participants were "disappointed" that the rate cut hadn't been an even larger 75-basis-point action. In any event, the market lost more than 2 percent that day. To ensure that our results did not depend on a few unusual observations, or "outliers," we re-ran our regression, omitting the days with the most extreme or unusual market moves. This more conservative analysis led to a smaller estimate of the effect of policy actions on the stock market, a stock price multiplier of about 2.6 rather than 4.7. However, the effect remains quite sharp in statistical terms. We considered other variations as well. For example, we investigated whether the magnitude of the effect on the stock market of a surprise policy tightening (that is, an increase in interest rates) differs from that of a surprise easing of comparable size. It does not. Yet another experiment consisted of asking whether an unanticipated policy change has a larger effect if it is thought by the market to signal a longer-lasting change in policy. We measured the perceived permanence of policy changes by observing the effects of unanticipated policy changes on the expected federal funds rate three months in the future, as measured by the futures market. The stock market multiplier associated with unanticipated policy moves that are perceived to be more permanent is a bit higher, as would be expected; its value is about 6. In short, the statistical evidence is strong for a stock price multiplier of monetary policy of something between 3 and 6, the higher values corresponding to policy changes that investors perceive to be relatively more permanent. That is, according to our findings, a surprise easing by the Fed of 25 basis points will typically lead broad stock indexes to rise from between 3/4 percentage point and 1-1/2 percentage points. Incidentally, similar results obtain for stock values of industry groups: We find almost all industry stock portfolios respond significantly to changes in monetary policy, with telecommunications, high-tech, and durables goods industry stocks being the most sensitive to monetary policy news, and energy, utilities, and health care stocks being the least sensitive. These results can be broadly explained by the tendency of each industry group to move with the broad market, or (to use the language of the standard capital asset pricing theory), by their industry "betas." Why Does Monetary Policy Affect Stock Prices? It is interesting, though perhaps not terribly surprising, to know that Federal Reserve policy actions affect stock prices. An even more interesting question, though, is, why does this effect occur? Answering this question will give us some insight into how monetary policy affects the economy, as well as the role that the stock market should play in policy decisions. A share of stock is a claim on the current and future dividends (or other cash flows, such as stock buybacks) to be paid by a company. Suppose, for just a moment, that financial investors do not care about risk. Then only two types of news ought to affect current stock values: news that affects investor forecasts of current or future (after-tax) dividends or news that affects forecasts of current or future short-term interest rates. News that current or future dividends (which I want to think of here as being measured in real, or inflation-adjusted, terms) are likely to be higher than previously expected--say, because the company is expecting to be more profitable--should raise the current stock price. News that current or future short-term interest rates (also measured in real, or inflation-adjusted, terms) are likely to be higher than previously expected should depress the stock price. There are two essentially equivalent ways of understanding why expectations of higher short-term real interest rates should lower stock prices. First, to value future dividends, an investor must discount them back to the present; as higher interest rates make a given future dividend less valuable in today's dollars, higher interest rates reduce the value of a share of stock. Second, higher real interest rates make investments other than stocks, such as bonds, more attractive, raising the required return on stocks and reducing what investors are willing to pay for them. Under either interpretation, expectations of higher real interest rates are bad news for stocks. So, to reiterate, in a world in which investors do not care about risk, stock prices should change only with news about current or future dividends or about current or future real interest rates. However, investors do care about risk, of course. Because investors care about risk, and because stocks are viewed as relatively risky investments, investors generally demand a higher average return, relative to other assets perceived to be safer, to hold stocks. Using long historical averages, one finds that, in the United States, a diversified portfolio of stocks has paid 5 to 6 percentage points more per year, on average, than has a portfolio of government bonds. This extra return, known as the risk premium on stocks, or the equity premium, presumably reflects, in part, the extra compensation that investors demand to be willing to hold relatively more risky stocks. Like news about dividends and real interest rates, news that affects the risk premium on stocks also affects stock prices. For example, news of an impending recession could raise the risk premium on stocks in two ways. First, the macroeconomic environment is more volatile than usual during a recession, so stocks themselves may become riskier investments. Second, the incomes and wealth of financial investors tend to fall during a downturn, giving them a smaller cushion to support the lifestyles to which they are accustomed (that is, to make house payments and meet other obligations). With less discretionary income and wealth to absorb potential losses, people may become less willing to bear the risks of more volatile financial investments (Campbell and Cochrane, 1999). For both reasons, the extra return that investors demand to hold stocks is likely to rise when bad times loom. With expected dividends and the real interest rate on alternative assets held constant, the expected yield on stocks can rise only through a decline in the current stock price. We now have a list of three key factors that should affect stock prices. First, news that current or future dividends will be higher should raise stock prices. Second, news that current or future real short-term interest rates will be higher should lower stock prices. And third, news that leads investors to demand a higher risk premium on stocks should lower stock prices. How does all this relate to the effects of monetary policy on stock prices? According to our analysis, Fed actions should affect stock prices only to the extent that they affect investor expectations about dividends, short-term real interest rates, or the riskiness of stocks. The trick is to determine quantitatively which of these sets of investor expectations is likely to be most affected when the Fed unexpectedly changes the federal funds rate. To make this determination, we used a methodology first applied by the financial economist John Campbell, of Harvard University, and by Campbell and John Ammer of the Federal Reserve Board staff (Campbell, 1991; Campbell and Ammer, 1993). Putting the details aside, we can describe the basic idea as follows. Imagine that the expectations of stock market investors can be mimicked by a statistical forecasting model that takes relevant current data as inputs and projects estimated future values of aggregate dividends, real interest rates, and equity risk premiums as outputs. In principle, investors could use such a model to make forecasts of these key variables and hence to estimate what they are willing to pay for stocks. Besides a number of standard variables that have been shown to be helpful in making forecasts of such financial variables, suppose we include in the forecasting model our measure of unanticipated changes in the federal funds rate. That is, we use the information contained in these unanticipated changes in making our forecasts of future dividends, interest rates, and risk premiums. Now we can consider the following thought experiment. Suppose we have run our computer model, made our forecasts, and inferred the appropriate values for stocks. But then we receive news that the Fed has unexpectedly raised the federal funds rate by 25 basis points. Based on our forecasting model, by how much would that information change our previous forecasts of future dividends, interest rates, and risk premiums? The answer to this question clarifies the channel by which monetary policy affects stock prices. If we were to find, for example, that the news of an unexpected increase in the funds rate significantly changed the forecast of future dividends but did not much affect the forecasts of interest rates or risk premiums, then we could conclude that monetary policy affects stock prices primarily by affecting investor expectations of future dividends. By contrast, if news of the policy action changed the model forecasts for real interest rates but did not change our forecasts for the other two variables, we would decide that unanticipated policy actions affect stock prices primarily by influencing the interest rates expected by stock investors. What we actually found when conducting this statistical experiment was quite interesting. It appears that, for example, an unanticipated tightening of monetary policy leads to only a modest change in forecasts of future dividends and to still less of a change in forecasts of future real interest rates (beyond a few quarters). Quantitatively, according to our methodology, the most important effect of a policy tightening is on the forecasted risk premium. Specifically, an unanticipated tightening of monetary policy raises expected risk premiums on stocks for a protracted period. For a given expected stream of dividend payouts and real interest rates, the risk premium and hence the return to holding stocks can only rise if the current stock price falls. In short, our analysis suggests that an unanticipated monetary tightening lowers stock prices only to a small extent by lowering investor expectations about future dividend payouts, and by still less by raising expected real interest rates. The most powerful effect of an unanticipated monetary tightening is to increase the perceived risk premium on stocks, either by increasing the riskiness of stocks, by reducing people's willingness to bear risk, or both. Reduced willingness of investors to hold relatively more risky stocks drives down stock prices. Our analysis does not explain precisely how monetary policy affects risk, but we can make reasonable conjectures. For example, tighter monetary policy may raise the riskiness of shares themselves by raising the interest costs and weakening the balance sheets of publicly owned firms (Bernanke and Gertler, 1995). In the macroeconomy more generally, by reducing spending and economic activity, tighter money raises the risks of unemployment or bankruptcy faced by individual households or firms. In each case, tighter monetary policy increases risk by reducing financial buffers or otherwise increasing the vulnerability of individuals or firms to future shocks to the economy. Implications of the Results for Monetary Policy So far I have discussed two principal conclusions from the empirical analysis: First, the stock price multiplier of monetary policy is between 3 and 6--in other words, an unexpected change in the federal funds rate of 25 basis points leads, on average, to a movement of stock prices in the opposite direction of between 3/4 percentage point and 1-1/2 percentage points. Second, the main reason that unanticipated changes in monetary policy affect stock prices is that they affect the risk premium on stocks. In particular, a surprise tightening of policy raises the risk premium, lowering current stock prices, and a surprise easing lowers the risk premium, raising current stock prices. What implications do these results have for our broader understanding and for the practice of monetary policy? I will briefly discuss two issues: first, the role of the stock market in the transmission of monetary policy changes to the economy; and second, the efficacy of monetary policy as a tool for controlling stock market "bubbles." A long-held element of the conventional wisdom is that the stock market is an important part of the transmission mechanism for monetary policy. The logic goes as follows: Easier monetary policy, for example, raises stock prices. Higher stock prices increase the wealth of households, prompting consumers to spend more--a result known as the wealth effect. Moreover, high stock prices effectively reduce the cost of capital for firms, stimulating increased capital investment. Increases in both types of spending--consumer spending and business spending--tend to stimulate the economy. This simple story can be elaborated somewhat in light of our results. It is true, as I have discussed, that an easier monetary policy raises stock prices, whereas a tighter policy lowers them. However, easier monetary policy not only raises stock prices; as we have seen, it also lowers risk premiums, presumably reflecting both a reduction in economic and financial volatility and an increase in the capacity of financial investors to bear risk. Thus, our results suggest that easier monetary policy not only allows consumers to enjoy a capital gain in their stock portfolios today, but it also reduces the effective amount of economic and financial risk they must face. This reduction in risk may cause consumers to trim their precautionary saving, that is, to reduce the amount of income that they put aside to protect themselves against unforeseen contingencies. Reduced precautionary saving in turn implies more spending by households. Thus, the reduction in risk associated with an easing of monetary policy and the resulting reduction in precautionary saving may amplify the short-run impact of policy operating through the traditional channel based on increased asset values. Likewise, reduced risk and volatility may provide an extra kick to capital expenditure in the short run, as firms are more likely to undertake investments in new structures or equipment in a more stable macroeconomic environment. A second issue concerns the role of monetary policy in the management of large swings in stock values, or "bubbles." In an earlier speech (Bernanke, 2002), I gave a number of reasons why I believe that using monetary policy--as opposed to microeconomic, prudential policies--is not a good way to address the problem of asset-market bubbles. These included the difficulty of identifying bubbles in advance; the questionable wisdom, in the context of a free-market economy, of setting up the central bank as the arbiter of asset values; the problem that arises when a bubble occurs in only one asset class rather than in all asset classes; and other reasons. A major concern that I have about the bubble-popping strategy, however, is that attempts to bring down stock prices by a significant amount using monetary policy are likely to have highly deleterious and unwanted side effects on the broader economy. The research I have described today allows me to address this issue more concretely. Here I will make just two points. First, this research suggests that relatively small changes in monetary policy would not do much to curb a major overvaluation in the stock market. As we have seen, a surprise tightening of 25 basis points should be expected to lower stock prices by only a little more than 1 percent, which, as already noted, is a trivial movement relative to the overall variability of the stock market. It would not be appropriate to extrapolate these results to try to estimate how much tightening would be needed to correct a substantial putative overvaluation in stock prices, but it seems clear that a light tapping of the brakes will not be sufficient. What we can say is that the necessary policy move would have to be quite large--many percentage points on the federal funds rate--and we would be highly uncertain about its magnitude or its ultimate effects on stock prices and the economy. , Second, we have seen that monetary tightening reduces stock prices primarily by increasing the risk premium for holding stocks, as opposed to raising the real interest rate or lowering expected dividends. The risk premium for stocks will rise only to the extent that broad macroeconomic risk rises, or that people experience declines in income and wealth that reduce their ability or willingness to absorb risk (Campbell and Cochrane, 1999). This evidence supports the proposition that monetary policy can lower stock values only to the extent that it weakens the broader economy, and in particular that it makes households considerably worse off. Indeed, according to our analysis, policy would have to weaken the general economy quite significantly to obtain a large decline in stock prices. Conclusion I have reported today on empirical work, by my coauthor and me as well as by others, about the links between monetary policy and the stock market. I have only touched on a large literature, and I apologize to the many researchers whose work I have not been able to describe today. But I hope that I have given you a flavor of how empirical research can help us to refine our understanding of how monetary policy works and how policy should be conducted. REFERENCES Bernanke, Ben (2002). "Asset-Price 'Bubbles' and Monetary Policy." Speech before the New York chapter of the National Association for Business Economics, New York, New York, October 15. Bernanke, Ben and Mark Gertler (1995). "Inside the Black Box: The Credit Channel of Monetary Transmission," Journal of Economic Perspectives, 9 (Fall), pp. 27-48. Bernanke, Ben and Mark Gertler (2001). "Should Central Banks Respond to Movements in Asset Prices?", American Economic Review, 91 (May), pp. 253-57. Bernanke, Ben and Kenneth Kuttner (2003). "What Explains the Stock Market's Reaction to Federal Reserve Policy?" Working paper, Federal Reserve Bank of New York, October. Campbell, John (1991). "A Variance Decomposition for Stock Returns," Economic Journal, 101 (March), pp. 157-79. Campbell, John, and John Ammer (1993). "What Moves the Stock and Bond Markets? A Variance Decomposition for Long-Term Asset Returns," Journal of Finance, 48 (March), pp. 3-37. Campbell, John, and John Cochrane (1999). "By Force of Habit: A Consumption-Based Explanation of Aggregate Stock Market Behavior," Journal of Political Economy, 107 (April), pp. 205-51. D'Amico, Stefania, and Mira Farka (2002). "The Fed and the Stock Market: A Proxy and Instrumental Variable Identification." Working paper, Columbia University. Greenspan, Alan (2002). "Economic Volatility." Speech before a symposium sponsored by the Federal Reserve Bank of Kansas City, Jackson Hole, Wyoming, August 30. Guo, Hui (2002). "Stock Prices, Firm Size, and Changes in the Federal Funds Rate Target." Working paper, Federal Reserve Bank of St. Louis, January. Grkaynak, Refet, Brian Sack, and Eric Swanson (2002). "Market-Based Measures of Monetary Policy Expectations." Working paper, Board of Governors of the Federal Reserve System, June. Krueger, Joel, and Kenneth Kuttner (1996). "The Fed Funds Futures Rate as a Predictor of Federal Reserve Policy," Journal of Futures Markets, 16 (December), pp. 865-79. Kuttner, Kenneth (2001). "Monetary Policy Surprises and Interest Rates: Evidence from the Fed Funds Futures Market," Journal of Monetary Economics, 47 (June), pp. 523-44. Lettau, Martin, and Sydney Ludvigson (2001). "Consumption, Aggregate Wealth, and Expected Stock Returns," Journal of Finance, 56 (June), pp. 815-49. Lettau, Martin, and Sydney Ludvigson (2002). "Time-Varying Risk Premiums and the Cost of Capital: An Alternative Interpretation of the Q Theory of Investment," Journal of Monetary Economics, 49 (January), pp. 31-66. Ludvigson, Sydney, Charles Steindel, and Martin Lettau (2002). "Monetary Policy Transmission through the Consumption-Wealth Channel," Federal Reserve Bank of New York, Economic Policy Review, 8 (May), pp. 117-133. Owens, Raymond and Roy Webb (2001). "Using the Federal Funds Futures Market to Predict Monetary Policy Actions," Federal Reserve Bank of Richmond, Economic Quarterly, 87 (Spring), pp. 69-77. Poole, William, Robert Rasche, and Daniel Thornton (2002). "Market Anticipations of Monetary Policy Actions," Federal Reserve Bank of St. Louis, Review, 84 (July/August), pp. 65-93. Rigobon, Roberto, and Brian Sack (2002). "The Impact of Monetary Policy on Asset Prices," Finance and Economics Discussion Series 2002-4, Board of Governors of the Federal Reserve System, January. Sack, Brian (2002). "Extracting the Expected Path of Monetary Policy from Futures Rates," Finance and Economics Discussion Series 2002-56, Board of Governors of the Federal Reserve System, December. Footnotes Bernanke and Kuttner (2003); http://www.ny.frb.org/research/staff_reports/sr174.html. The futures contract is based on monthly averages of the federal funds rate, so that some manipulation is needed to obtain the daily expectations of the funds rate used in this paper. See Bernanke and Kuttner (2003) or Kuttner (2001) for further details. Allowing for risk premiums creates another complication; see Sack (2002). I ignore these technicalities here. Other financial instruments, such as eurodollar futures rates, can and have been used to forecast changes in the federal funds rate. Although each of the various alternatives has advantages, Grkaynak, Sack, and Swanson (2002) find that the federal funds futures rate is the best predictor of monetary policy actions for horizons out to several months. The beginning of the sample corresponds to the availability of the futures data. We excluded the observation corresponding to September 17, 2001, the first day of trading following the September 11 terrorist attacks. That the Federal Reserve has only been formally announcing its policy moves since 1994 added a measure of complexity to our research. Before then, market participants generally did not become aware of the FOMC's policy decisions until those decisions were actually implemented in the market for bank reserves, often the day after the FOMC decision. To the extent possible, we dated the policy change as of the day that the market would have become aware of it, not the day of the decision itself. See the paper for details. Investors would not literally expect the Fed to cut the funds rate by 31 basis points, since the Fed usually moves in 25-basis-point increments. An average expectation of a 31-basis-point cut would be consistent with, for example, 62 percent of investors expecting a 50-basis-point and 38 percent expecting no cut. In principle, news other than the policy decision might affect the federal funds futures contract during the day, so that the measure of unanticipated policy changes we use here might be a "noisy" one. If so, our approach would underestimate the effect of policy changes on the stock market. However, Poole, Rasche and Thornton (2002, pp. 68-69) perform an analysis that suggests that the mismeasurement may be small in practice. Further confirmation is provided by D'Amico and Farka (2002), who find results similar to ours using ten-minute windows around the announcement; the benefit of a tight window is that the policy announcement is highly likely to dominate movements in the contract over that period. Technically, we removed outlier observations based on their so-called influence statistics, which measure the importance of individual observations to the overall results. Another correction was needed because, in the early part of the sample, particularly between 1989 and 1992, it was not uncommon for intermeeting rate cuts to take place on the same day that the government issued weaker-than-expected reports about employment growth. In such cases, our method cannot distinguish cleanly between the effects of the employment news and the effects of the rate cut itself on the stock market. If we eliminate both the outlier observations and the observations in which employment reports coincided with rate changes, we find the multiplier effect of policy changes on the stock market to be about 3.6 and again statistically significant. To focus on policy surprises of longer duration, Rigobon and Sack (2002) derive their measure of the unexpected policy change on the three-month eurodollar deposit rate, rather than the current month's federal funds rate, as in this paper and in Kuttner (2001). Using a methodology that also attempts to correct for two-way causality between the funds rate and asset prices, and data for post-1993 scheduled FOMC meetings and Chairman's testimony dates only, they find comparable though slightly higher values for the effect of monetary policy on the stock market. For example, they find a policy multiplier for the Standard and Poor's 500 index of 7.7. However, when they use data on the federal funds rate futures market to measure policy shocks, Rigobon and Sack find results similar to ours, using their sample and methodology. Using methods similar to ours, Guo (2002) found that the impact of monetary policy actions on stock prices does not seem to depend on firm size. The existence of a large equity premium in the past is, of course, no guarantee of an equally large equity premium in the future. The fact that equities are more widely held today than in the past, implying that the risk of equities is more widely shared, is one reason that the equity premium may be lower in the future than it has been in the past. Of course, a looming recession is likely also to lower expected dividends (bad for stocks) and lower interest rates (good for stocks). Generally, stock prices are a leading indicator, falling ahead of recessions and rising in advance of recoveries (although with many false signals). Variables used in our forecasting model, besides the excess return on stocks, the one-month real interest rate, and the unanticipated change in the funds rate, include the relative bill rate (defined as the three-month Treasury bill rate minus its 12-month moving average), the change in the bill rate, the smoothed dividend-price ratio, and the spread between 10-year and one-month Treasury yields. There is a bit more to this analysis. An additional complexity arises from the fact that, although easier monetary policy allows consumers to enjoy a capital gain in their stock portfolios today, it also "takes back" some of that gain, so to speak, by affording shareholders a lower rate of return on their holdings, on average, in subsequent periods. Research by Sydney Ludvigson and Martin Lettau of New York University and Charles Steindel of the Federal Reserve Bank of New York (Ludvigson, Steindel, and Lettau, 2002; Lettau and Ludvigson, 2001) suggests that, because the gain in share prices induced by a monetary easing is partly transitory, consumers will not increase their spending in response to stock price changes induced by monetary policy as much as they will in response to stock price changes induced by other factors. The estimates in our paper suggest that this differential effect will be relatively small, however. Also, to the extent that the capital gains induced by monetary policy are perceived as partly transitory, the short-run response of investment spending will be strengthened, as firms prefer to invest while stock prices remain high; see Lettau and Ludvigson, 2002, for evidence. In short, if changes in stock values induced by monetary policy are perceived as relatively more transitory, the effects of policy will be concentrated more on investment spending and less on consumption spending than the conventional wisdom suggests. Greenspan (2002) notes several episodes in which increases in the federal funds rate of several hundred basis points did not materially slow stock appreciation. He argues that "such data suggest that nothing short of a sharp increase in short-term rates that engenders a significant economic retrenchment is sufficient to check a nascent bubble." The late Fischer Black once defined an efficient stock market as one in which prices are between half and double fundamental values; if Black's view is to be believed, then identifiable deviations of prices from fundamentals would have to be quite large indeed. Implicitly I am considering here the case of a central bank that responds only sporadically to stock prices, in those situations in which it perceives a bubble to be forming. Irregular deviations from a policy rule focused on output and inflation seem appropriately modeled as unanticipated movements in policy. An alternative policy strategy would be to incorporate regular reactions to stock values into the systematic part of the monetary policy reaction function. That strategy has some advantages, but it has the important disadvantage that it does not discriminate between fundamental and nonfundamental sources of changes in stock values. Bernanke and Gertler (2001) present simulations showing that such a strategy is unlikely to be beneficial in terms of overall macroeconomic stability.
For immediate release The Federal Reserve Board on Friday announced the move to a quarterly publication schedule for the Federal Reserve Bulletin and the creation of a new monthly statistical supplement. Beginning in the first quarter of 2004, the Bulletin will be enhanced and published four times a year. A quarterly report on the condition of the banking system and an annual report on changes in consumer regulations are among the new materials to be presented in the Bulletin . The Bulletin will continue to include topical research articles and summaries of Board survey findings, the Board's semiannual Monetary Policy Reports, a Legal Developments section, and other features such as lists of staff members, councils, committees, lists of Federal Reserve publications, and maps of the Federal Reserve Districts. The Legal Developments section of the quarterly Bulletin will contain Board orders issued under the Bank Holding Company Act, the Bank Merger Act, the Federal Reserve Act, and the International Banking Act. Final rules and pending cases involving the Board are available on the Board's web site under "Legal Developments" at The revised publication schedule responds to the results of customer surveys, the increased use of the Internet to access information on a more timely basis, and the Board's desire to provide a broader range of articles on topics of interest to Bulletin readers. A quarterly schedule will also make the planning and production of the Bulletin more efficient. The tables that now appear in the Financial and Business Statistics section of the Bulletin will be published monthly as a separate publication titled Statistical Supplement to the Federal Reserve Bulletin . All tables that now appear in the Federal Reserve Bulletin , including special tables, will appear in the Statistical Supplement . All statistical series will be published with the same frequency that they have currently in the Bulletin . The first issue of the Statistical Supplement will be published in January 2004. The Publications Committee will monitor the usefulness of this publication in meeting the needs of the public over time, especially in light of the widespread dissemination of data through the Internet. A Bulletin editorial board has been established under the direction of Lucretia Boyer, the Federal Reserve Board's Chief of Publications, to oversee the quality of content of these two publications and to ensure a diverse range of Bulletin articles. Separate subscriptions for the two publications will be available starting with the January 2004 issue of the Statistical Supplement . For additional subscription information, contact Publications Fulfillment at 202-452-3244 or 202-452-3245 or send an e-mail to . Articles published in the Bulletin will continue to be available online at
Joint Press Release Board of Governors of the Federal Reserve System Department of Housing and Urban Development Department of Justice Federal Deposit Insurance Corporation Federal Housing Finance Board Federal Trade Commission National Credit Union Administration Office of the Comptroller of the Currency Office of Federal Housing Enterprise Oversight Office of Thrift Supervision For Immediate Release October 7, 2003 HUD Michael Fluharty (202) 708-0685 Ext. 6605 DOJ Jorge Martinez (202) 514-2007 FDIC David Barr (202) 898-6992 David Barr (202) 898-6992 (202) 898-6992 FHFB Carter Wood (202) 408-2817 Carter Wood (202) 408-2817 (202) 408-2817 FRB Susan Stawick (202) 452-2955 Susan Stawick (202) 452-2955 (202) 452-2955 FTC Brenda Mack (202) 326-2182 Brenda Mack (202) 326-2182 (202) 326-2182 NCUA Cliff Northup (703) 518-6331 Cliff Northup (703) 518-6331 (703) 518-6331 OCC Dean DeBuck (202) 874-5770 Dean DeBuck (202) 874-5770 (202) 874-5770 OFHEO Corinne Russell (202) 414-6922 Corinne Russell (202) 414-6922 (202) 414-6922 OTS Chris Smith (202) 906-6677 Chris Smith (202) 906-6677 (202) 906-6677
For immediate release The Board of Governors of the Federal Reserve on Wednesday requested comment on proposed changes to its cash services policy. The changes would address a shift by depository institutions away from traditional patterns of currency activity toward greater reliance on Reserve Bank cash processing and provide incentives for depository institutions to recirculate currency among their customers. To reduce depository institutions' overuse of Reserve Bank cash-processing services that are provided at no charge, the Board proposes revising its cash services policy by adding two elements: (1) a custodial inventory program that provides an incentive to depository institutions to hold currency in their vaults to meet customers' demand; and (2) a fee to depository institutions that deposit fit currency to, and order currency from, Reserve Banks within the same week. Initially the policy changes would apply only to the $5, $10, and $20 denominations. The Reserve Banks estimate that the proposed changes would affect approximately 100 of their largest cash customers. The Board proposes to implement the recirculation policy in phases. In early 2004, the Reserve Banks will accept applications for a custodial inventory proof-of-concept, or trial, program. The Board will evaluate the results of the program after about six months of operation and will decide whether to implement a permanent custodial inventory program in 2005. Reserve Banks would begin assessing the recirculation fee in 2006. In 2007, the Board would extend the recirculation policy to one-dollar notes if the Reserve Banks are unable, by working collaboratively with depository institutions, to achieve significant savings. The Board requests comment by January 15, 2004 The Board's notice is attached.
Remarks by Governor Susan Schmidt Bies At the Annual Economic Outlook Conference, Middle Tennessee State University, Murfreesboro, Tennessee October 8, 2003 Comments on the Current State of the Economy
Joint Press Release Board of Governors of the Federal Reserve System Bureau of Engraving and Printing For Immediate Release October 9, 2003 More Secure, Colorful $20 Bill Makes Its Debut Banks Begin Distributing Newly Redesigned Currency Today The most secure currency in U.S. history was introduced into the economy today, as a newly redesigned, colorful $20 bill was issued by the Federal Reserve System. In dozens of communities from coast to coast, U.S. government officials and local business, banking and civic leaders participated in transactions with the new $20 notes, marking the first opportunity for the public to spend the new currency. Today is the first day banks will receive the new bills from the Federal Reserve System, and in turn begin distributing them to their customers. It will take several days or even weeks for the bills to make their way to all communities in the U.S. and internationally. The new designs will co-circulate with old-design $20 notes, until, gradually, the old-design notes become worn and are pulled from circulation. " The New Color of Money starts making its way into cash registers and wallets today," said Tom Ferguson, director of the U.S. Treasury's Bureau of Engraving and Printing (BEP). "This is a historic milestone on two fronts: for the first time in modern history, U.S. currency features background colors other than black and green, and, more importantly, this currency is the most secure U.S. currency ever, to protect against counterfeiting." "While much of the public will be anxious to see and handle this newly designed $20 bill, we want to emphasize that older-design $20 notes are still in circulation, and still maintain their value," said Marsha Reidhill, the Federal Reserve Board's assistant director for cash. "A genuine U.S. $20 bill--whether it has the new background colors or the familiar green and black--is legal tender, worth $20. It is important to remember that all bills are good, for good. The stability and integrity of U.S. currency has kept worldwide trust and confidence high, and the government is committed to keeping it that way." The BEP and the Federal Reserve System have been educating the public worldwide about the new bills in professional and community settings, in preparation for a smooth transition this fall. Over 37 million items of training materials such as brochures, posters, training videos and CD-ROMS, have been ordered by businesses large and small to train their cash-handling employees on the bill's new look and updated security features. Additionally, there have been more than 2 million visits to the new money Web site ( ) for information. The public education program continues globally with broadcast, print, Internet and other public education advertising; and integration of the new money's look and security features will be featured in the story lines of television programs that reach millions of viewers. Ferguson and Reidhill marked today's historic issue of the new $20 bill in New York City's Times Square, where they will spend the new twenties in Times Square area businesses. In Washington, D.C., Michael Lambert, the Federal Reserve Systems Financial Services Manager who is responsible for cash, and the BEP's Chief of the Office of Currency Production, James Brent, demonstrated the effectiveness of the government's advance preparation for the new money by using a new $20 note to buy stamps from a vending machine at a U.S. Postal Service facility. The government began working with the vending machine industry and transit authorities more than a year ago to ensure there was ample time for adjustments so machines will accept the new bills. Events marking the first purchases with the new $20 notes were held today in more than 30 U.S. cities. Later this month, the issue of the new $20 bill will be marked by international events in dollarized economies and in countries where U.S. currency is widely held, such as Russia and countries throughout Latin America. The New Color of Money: Safer. Smarter. More Secure. The most noticeable difference in the new $20 notes is the subtle green, peach and blue colors featured in the background. New designs for the $50 and $100 notes are scheduled for introduction in 2004 and 2005, respectively. Different colors will be used for different denominations, which will help everyone--particularly those who are visually impaired--to tell denominations apart. Redesign of the $5 and $10 notes is under consideration, but the $1 and $2 notes will not be redesigned. While consumers should not use color to check the authenticity of their currency (relying instead on user-friendly security features--see below), color does add complexity to the note, making counterfeiting more difficult. The new $20 bills maintain the traditional U.S. currency appearance, are the same size, and use the same, but enhanced portraits and historical images of Andrew Jackson on the face of the note and the White House on the back. The redesign also features new symbols of freedom--a blue eagle in the background, and a metallic green eagle and shield to the right of Jackson's portrait. Security Features The new $20 note design retains three important security features that were first introduced in the late 1990s and are easy for consumers and merchants alike to check: The watermark --the faint image similar to the large portrait, which is part of the paper itself and is visible from both sides when held up to the light. The security thread --also visible from both sides when held up to the light, this vertical strip of plastic is embedded in the paper. "USA TWENTY" and a small flag are visible along the thread. The color-shifting ink --the numeral "20" in the lower-right corner on the face of the note changes from copper to green when the note is tilted. The color shift is more dramatic and easier to see on the new-design notes. Because these features are difficult for counterfeiters to reproduce well, they often do not try. Counterfeiters are hoping that cash-handlers and the public will not check their money closely. Counterfeiting: Increasingly Digital Currency counterfeiters are increasingly turning to digital methods, as advances in technology make digital counterfeiting of currency easier and cheaper. In 1995, for example, less than 1 percent of counterfeit notes detected in the U.S. were digitally produced. By 2002, that number had grown to nearly 40 percent, according to the U.S. Secret Service. Yet despite the efforts of counterfeiters, U.S. currency counterfeiting has been kept at low levels, with current estimates putting the level of counterfeit notes in circulation worldwide at between 0.01 and 0.02 percent, or about 1-2 notes in every 10,000 genuine notes. To learn more about the new currency and to download an image of the new $20 note, visit . The most secure currency in U.S. history was introduced into the economy today, as a newly redesigned, colorful $20 bill was issued by the Federal Reserve System. In dozens of communities from coast to coast, U.S. government officials and local business, banking and civic leaders participated in transactions with the new $20 notes, marking the first opportunity for the public to spend the new currency. Today is the first day banks will receive the new bills from the Federal Reserve System, and in turn begin distributing them to their customers. It will take several days or even weeks for the bills to make their way to all communities in the U.S. and internationally. The new designs will co-circulate with old-design $20 notes, until, gradually, the old-design notes become worn and are pulled from circulation. " The New Color of Money starts making its way into cash registers and wallets today," said Tom Ferguson, director of the U.S. Treasury's Bureau of Engraving and Printing (BEP). "This is a historic milestone on two fronts: for the first time in modern history, U.S. currency features background colors other than black and green, and, more importantly, this currency is the most secure U.S. currency ever, to protect against counterfeiting." "While much of the public will be anxious to see and handle this newly designed $20 bill, we want to emphasize that older-design $20 notes are still in circulation, and still maintain their value," said Marsha Reidhill, the Federal Reserve Board's assistant director for cash. "A genuine U.S. $20 bill--whether it has the new background colors or the familiar green and black--is legal tender, worth $20. It is important to remember that all bills are good, for good. The stability and integrity of U.S. currency has kept worldwide trust and confidence high, and the government is committed to keeping it that way." The BEP and the Federal Reserve System have been educating the public worldwide about the new bills in professional and community settings, in preparation for a smooth transition this fall. Over 37 million items of training materials such as brochures, posters, training videos and CD-ROMS, have been ordered by businesses large and small to train their cash-handling employees on the bill's new look and updated security features. Additionally, there have been more than 2 million visits to the new money Web site ( ) for information. The public education program continues globally with broadcast, print, Internet and other public education advertising; and integration of the new money's look and security features will be featured in the story lines of television programs that reach millions of viewers. Ferguson and Reidhill marked today's historic issue of the new $20 bill in New York City's Times Square, where they will spend the new twenties in Times Square area businesses. In Washington, D.C., Michael Lambert, the Federal Reserve Systems Financial Services Manager who is responsible for cash, and the BEP's Chief of the Office of Currency Production, James Brent, demonstrated the effectiveness of the government's advance preparation for the new money by using a new $20 note to buy stamps from a vending machine at a U.S. Postal Service facility. The government began working with the vending machine industry and transit authorities more than a year ago to ensure there was ample time for adjustments so machines will accept the new bills. Events marking the first purchases with the new $20 notes were held today in more than 30 U.S. cities. Later this month, the issue of the new $20 bill will be marked by international events in dollarized economies and in countries where U.S. currency is widely held, such as Russia and countries throughout Latin America. The New Color of Money: Safer. Smarter. More Secure. The most noticeable difference in the new $20 notes is the subtle green, peach and blue colors featured in the background. New designs for the $50 and $100 notes are scheduled for introduction in 2004 and 2005, respectively. Different colors will be used for different denominations, which will help everyone--particularly those who are visually impaired--to tell denominations apart. Redesign of the $5 and $10 notes is under consideration, but the $1 and $2 notes will not be redesigned. While consumers should not use color to check the authenticity of their currency (relying instead on user-friendly security features--see below), color does add complexity to the note, making counterfeiting more difficult. The new $20 bills maintain the traditional U.S. currency appearance, are the same size, and use the same, but enhanced portraits and historical images of Andrew Jackson on the face of the note and the White House on the back. The redesign also features new symbols of freedom--a blue eagle in the background, and a metallic green eagle and shield to the right of Jackson's portrait. Security Features The new $20 note design retains three important security features that were first introduced in the late 1990s and are easy for consumers and merchants alike to check: The watermark --the faint image similar to the large portrait, which is part of the paper itself and is visible from both sides when held up to the light. The security thread --also visible from both sides when held up to the light, this vertical strip of plastic is embedded in the paper. "USA TWENTY" and a small flag are visible along the thread. The color-shifting ink --the numeral "20" in the lower-right corner on the face of the note changes from copper to green when the note is tilted. The color shift is more dramatic and easier to see on the new-design notes. Because these features are difficult for counterfeiters to reproduce well, they often do not try. Counterfeiters are hoping that cash-handlers and the public will not check their money closely. Counterfeiting: Increasingly Digital Currency counterfeiters are increasingly turning to digital methods, as advances in technology make digital counterfeiting of currency easier and cheaper. In 1995, for example, less than 1 percent of counterfeit notes detected in the U.S. were digitally produced. By 2002, that number had grown to nearly 40 percent, according to the U.S. Secret Service. Yet despite the efforts of counterfeiters, U.S. currency counterfeiting has been kept at low levels, with current estimates putting the level of counterfeit notes in circulation worldwide at between 0.01 and 0.02 percent, or about 1-2 notes in every 10,000 genuine notes. To learn more about the new currency and to download an image of the new $20 note, visit . CONTACT : Bureau of Engraving and Printing Dawn Haley (202) 874-3545 Federal Reserve Board Rose Pianalto or David Skidmore (202) 452-2955 New Color of Money Media Support Penny Kozakos (202) 530-4887
Remarks by Governor Edward M. Gramlich At the Texas Association of Bank Counsel 27th Annual Convention, South Padre Island, Texas October 9, 2003 An Update on the Predatory Lending Issue
For immediate release The Federal Reserve Board on Friday announced the appointment of the chairmen and deputy chairmen of the twelve Federal Reserve Banks for 2004. Each Reserve Bank has a nine-member board of directors. The Board of Governors in Washington appoints three of these directors and each year designates one of its appointees as chairman and a second as deputy chairman. Following are the names of the chairmen and deputy chairmen appointed by the Board for 2004: Boston Samuel O. Thier, M.D., Professor of Medicine and Professor of Health Care Policy, Harvard Medical School, Massachusetts General Hospital, Boston, Massachusetts, named Chairman. Blenda J. Wilson, President and Chief Executive Officer, Nellie Mae Education Foundation, Quincy, Massachusetts, named Deputy Chairman. New York John E. Sexton, President, New York University, New York, New York, named Chairman. Jerry I. Speyer, President and Chief Executive Officer, Tishman Speyer Properties, New York, New York, named Deputy Chairman. Philadelphia Ronald J. Naples, Chairman and Chief Executive Officer, Quaker Chemical Corporation, Conshohocken, Pennsylvania, named Chairman. Doris M. Damm, President and Chief Executive Officer, ACCU Staffing Services, Cherry Hill, New Jersey, named Deputy Chairman. Cleveland Robert W. Mahoney, Retired Chairman and Chief Executive Officer, Diebold, Incorporated, Canton, Ohio, renamed Chairman. Charles E. Bunch, President and Chief Operating Officer, PPG Industries, Inc., Pittsburgh, Pennsylvania, renamed Deputy Chairman. Richmond Wesley S. Williams, Jr., Partner, Covington & Burling, Washington, D.C., renamed Chairman. Thomas J. Mackell, Jr., President and Chief Operating Officer, The Kamber Group, Washington, D.C., renamed Deputy Chairman. Atlanta David M. Ratcliffe, President and Chief Executive Officer, Georgia Power Company, Atlanta, Georgia, named Chairman. V. Larkin Martin, Managing Agent, Martin Farm, Courtland, Alabama, named Deputy Chairman. Chicago W. James Farrell, Chairman and Chief Executive Officer, Illinois Tool Works Inc., Glenview, Illinois, named Chairman. Miles D. White, Chairman and Chief Executive Officer, Abbott Laboratories, Abbott Park, Illinois, named Deputy Chairman. St. Louis Walter L. Metcalfe, Jr., Chairman, Bryan Cave LLP, St. Louis, Missouri, named Chairman. Gayle P. W. Jackson, Managing Director, FondElec Clean Energy Group, Inc., St. Louis, Missouri, named Deputy Chairman. Minneapolis Linda Hall Whitman, Chief Executive Officer, QuickMedx, Inc., Minneapolis, Minnesota, named Chairman. Frank L. Sims, Corporate Vice President, Transportation, Cargill, Inc., Wayzata, Minnesota, named Deputy Chairman. Kansas City Richard H. Bard, Chief Executive Officer, Bard & Co., Inc., Denver, Colorado, renamed Chairman. Robert A. Funk, Chairman and Chief Executive Officer, Express Personnel Services International, Oklahoma City, Oklahoma, renamed Deputy Chairman. Dallas Ray L. Hunt, Chairman, President, and Chief Executive Officer, Hunt Consolidated, Inc., Dallas, Texas, renamed Chairman. Patricia M. Patterson, President, Patterson Investments, Inc., Dallas, Texas, renamed Deputy Chairman. San Francisco George M. Scalise, President, Semiconductor Industry Association, San Jose, California, renamed Chairman. Sheila D. Harris, Director, Arizona Department of Housing, Phoenix, Arizona, renamed Deputy Chairman.
Testimony of Vice Chairman Roger W. Ferguson, Jr. Statement on his renomination as Vice Chairman of the Board Before the Committee on Banking, Housing, and Urban Affairs, U.S. Senate October 14, 2003 Chairman Shelby, Senator Sarbanes, and members of the Committee, I am pleased to appear before you today as President Bush's nominee to serve as Vice Chairman of the Board of Governors of the Federal Reserve System. I am honored that the President has nominated me to serve a second term in that capacity. I thank you for holding this hearing. It has been my privilege to serve our fellow citizens as a member of the Federal Reserve Board since 1997 and as Vice Chairman since 1999. I have given this role my undivided attention, and I hope to be able to continue in that service. The policy decisions of the Federal Reserve influence the economic well-being of all Americans. During my tenure, we have faced challenges in many of our areas of responsibility, and I would like to review briefly some of those developments and our responses to them. Congress has given the Federal Reserve three monetary policy objectives: maximum employment, stable prices and moderate long-term interest rates. We have viewed these objectives as congruent with a goal of maximum sustainable growth that can occur only in the context of long-run price stability. Fostering financial conditions in which Americans can realize their full potential has presented a number of challenges in recent years. The impressive step-up in the advance of technological and organizational efficiencies and a rapid accumulation of physical capital in the late 1990s have been the key factors affecting our economy's performance in the past decade. These developments have made workers increasingly productive. But faster productivity growth, despite its long-term benefits, has not insulated the economy from cyclical swings. The sharp reevaluation that occurred in equity markets and the retrenchment in business investment and spending that occurred over the past several years--together with the effects of terrorist attacks, wars, and corporate scandals--battered the confidence of households and businesses. In response, the Federal Reserve made substantial adjustments to its policy interest rate in order to cushion the effects of these developments on the broader economy. Other forces--particularly the growing interconnectedness of the global economy--have been important background factors in setting monetary policy. Of late, policymakers have been mindful of the virtual eradication of inflation and their need to set policy so that the economy remains in the zone of price stability. But all of our policy changes have been undertaken in pursuit of maximum sustainable growth and stable prices. Making monetary policy has been only part of the challenge. During my tenure at the Federal Reserve, we have also worked diligently to communicate to the public what we are doing and why. Transparency in policymaking is a key part of the democratic process and fosters efficient decisionmaking in the private sector. Becoming more transparent has been an important goal of the central bank in recent years, keeping in mind that we must balance being open and accountable with the need to maintain an effective process of decisionmaking by the Federal Open Market Committee. Transparency requires that we periodically review our procedures, as we did in 1999 and again last month, to ensure that they appropriately balance these considerations. I do not know what future changes, if any, might be called for in how we communicate, but I am confident that the Federal Reserve will continue to look for ways to communicate and explain our policies clearly. While macroeconomic conditions are of central importance, the role of the Federal Reserve is broader than monetary policy. Financial stability is an essential precondition for maintaining a strong economy, and the Federal Reserve played a key role in maintaining financial and economic stability in the aftermath of the terrorist attacks on September 11, 2001. As the only Board member in Washington, D.C., on that day, I had responsibility for overseeing the Federal Reserve System's response to the terrorist attacks. Working with many able colleagues in the System, the U.S. government and the private sector, we at the Federal Reserve responded effectively to the attacks. By providing ample liquidity and reassuring the public and the banking community, we helped our financial markets and the supporting infrastructure recover very quickly. Since that terrible day, I have done all in my power to enhance the resilience of the financial system of the United States, and I pledge to continue to work on these issues in the years ahead. The Federal Reserve executes its important financial stability responsibilities in less stressful times through its role in supervising and regulating our nation's banking system. The Federal Reserve and other regulators must foster a competitive environment that will benefit the users of financial services, while also promoting safety and soundness. I believe that we should achieve these objectives with a minimum of regulatory burden and without leaving the impression that any institution is too big to fail. Currently, we face the challenge of meeting these goals by developing a new capital accord to apply to the largest, most complex internationally active institutions. As I have testified before this Committee, the existing accord no longer suffices for these institutions. Now we need to work with our financial institutions and other regulators to replace the existing accord with a new one that is more risk-sensitive, builds on advances in risk measurement and management, and provides proper incentives. And we must do so without unnecessary complexity and without creating undesirable competitive imbalances or other unintended consequences. Technology and deregulation have encouraged consolidation in the financial sector. With central bank and treasury officials from twelve other major industrial economies, I have reviewed the likely effects of the global trend toward consolidation and its implications for central banks and regulators. Because financial systems will continue to consolidate, the regulatory community needs to monitor developments closely. But our study also found that existing policies appear adequate to allow regulators to maintain safe and sound financial industries now and in the intermediate term. This is true both for financial stability and for the maintenance of markets through which monetary policy can continue to work using the same mechanisms as in the past. Lastly, our payment system is a real presence in the economic lives of every consumer and business. This system too has been, and will continue to be, changed greatly by emerging technologies. From its very founding, the Federal Reserve has had the responsibility to foster an efficient, safe and accessible payment system. In a dynamic economy, markets appropriately play the key role in guiding the development of the payments infrastructure. This means that innovation and competition will be central to the future development of the payment system--as they are in other areas of the economy. Regulators and Congress should strive to remove barriers to innovation when we can do so without sacrificing important public policy objectives. I have been privileged to work with this Committee on one such initiative, the Check Truncation Act, or Check 21. This legislation removes a legal impediment and should, over time, foster greater use of electronics in the check clearing process while also preserving the right of consumers and banks to receive paper checks. Ultimately, Check 21 should allow depository institutions to provide new and beneficial services to their customers. I look forward, as I know you do, to its prompt enactment. And I thank the Committee and its staff for the strong support you have provided. Mr. Chairman and members of the Committee, during my years on the Board of Governors, I have done my best to contribute positively to all aspects of the Federal Reserve's many responsibilities. I look forward to the opportunity to continue to work with you and serve the nation as Vice Chairman of the Board of Governors. Thank you for your attention and for considering my nomination. I would be pleased to answer any questions. Chairman Shelby, Senator Sarbanes, and members of the Committee, I am pleased to appear before you today as President Bush's nominee to serve as Vice Chairman of the Board of Governors of the Federal Reserve System. I am honored that the President has nominated me to serve a second term in that capacity. I thank you for holding this hearing. It has been my privilege to serve our fellow citizens as a member of the Federal Reserve Board since 1997 and as Vice Chairman since 1999. I have given this role my undivided attention, and I hope to be able to continue in that service. The policy decisions of the Federal Reserve influence the economic well-being of all Americans. During my tenure, we have faced challenges in many of our areas of responsibility, and I would like to review briefly some of those developments and our responses to them. Congress has given the Federal Reserve three monetary policy objectives: maximum employment, stable prices and moderate long-term interest rates. We have viewed these objectives as congruent with a goal of maximum sustainable growth that can occur only in the context of long-run price stability. Fostering financial conditions in which Americans can realize their full potential has presented a number of challenges in recent years. The impressive step-up in the advance of technological and organizational efficiencies and a rapid accumulation of physical capital in the late 1990s have been the key factors affecting our economy's performance in the past decade. These developments have made workers increasingly productive. But faster productivity growth, despite its long-term benefits, has not insulated the economy from cyclical swings. The sharp reevaluation that occurred in equity markets and the retrenchment in business investment and spending that occurred over the past several years--together with the effects of terrorist attacks, wars, and corporate scandals--battered the confidence of households and businesses. In response, the Federal Reserve made substantial adjustments to its policy interest rate in order to cushion the effects of these developments on the broader economy. Other forces--particularly the growing interconnectedness of the global economy--have been important background factors in setting monetary policy. Of late, policymakers have been mindful of the virtual eradication of inflation and their need to set policy so that the economy remains in the zone of price stability. But all of our policy changes have been undertaken in pursuit of maximum sustainable growth and stable prices. Making monetary policy has been only part of the challenge. During my tenure at the Federal Reserve, we have also worked diligently to communicate to the public what we are doing and why. Transparency in policymaking is a key part of the democratic process and fosters efficient decisionmaking in the private sector. Becoming more transparent has been an important goal of the central bank in recent years, keeping in mind that we must balance being open and accountable with the need to maintain an effective process of decisionmaking by the Federal Open Market Committee. Transparency requires that we periodically review our procedures, as we did in 1999 and again last month, to ensure that they appropriately balance these considerations. I do not know what future changes, if any, might be called for in how we communicate, but I am confident that the Federal Reserve will continue to look for ways to communicate and explain our policies clearly. While macroeconomic conditions are of central importance, the role of the Federal Reserve is broader than monetary policy. Financial stability is an essential precondition for maintaining a strong economy, and the Federal Reserve played a key role in maintaining financial and economic stability in the aftermath of the terrorist attacks on September 11, 2001. As the only Board member in Washington, D.C., on that day, I had responsibility for overseeing the Federal Reserve System's response to the terrorist attacks. Working with many able colleagues in the System, the U.S. government and the private sector, we at the Federal Reserve responded effectively to the attacks. By providing ample liquidity and reassuring the public and the banking community, we helped our financial markets and the supporting infrastructure recover very quickly. Since that terrible day, I have done all in my power to enhance the resilience of the financial system of the United States, and I pledge to continue to work on these issues in the years ahead. The Federal Reserve executes its important financial stability responsibilities in less stressful times through its role in supervising and regulating our nation's banking system. The Federal Reserve and other regulators must foster a competitive environment that will benefit the users of financial services, while also promoting safety and soundness. I believe that we should achieve these objectives with a minimum of regulatory burden and without leaving the impression that any institution is too big to fail. Currently, we face the challenge of meeting these goals by developing a new capital accord to apply to the largest, most complex internationally active institutions. As I have testified before this Committee, the existing accord no longer suffices for these institutions. Now we need to work with our financial institutions and other regulators to replace the existing accord with a new one that is more risk-sensitive, builds on advances in risk measurement and management, and provides proper incentives. And we must do so without unnecessary complexity and without creating undesirable competitive imbalances or other unintended consequences. Technology and deregulation have encouraged consolidation in the financial sector. With central bank and treasury officials from twelve other major industrial economies, I have reviewed the likely effects of the global trend toward consolidation and its implications for central banks and regulators. Because financial systems will continue to consolidate, the regulatory community needs to monitor developments closely. But our study also found that existing policies appear adequate to allow regulators to maintain safe and sound financial industries now and in the intermediate term. This is true both for financial stability and for the maintenance of markets through which monetary policy can continue to work using the same mechanisms as in the past. Lastly, our payment system is a real presence in the economic lives of every consumer and business. This system too has been, and will continue to be, changed greatly by emerging technologies. From its very founding, the Federal Reserve has had the responsibility to foster an efficient, safe and accessible payment system. In a dynamic economy, markets appropriately play the key role in guiding the development of the payments infrastructure. This means that innovation and competition will be central to the future development of the payment system--as they are in other areas of the economy. Regulators and Congress should strive to remove barriers to innovation when we can do so without sacrificing important public policy objectives. I have been privileged to work with this Committee on one such initiative, the Check Truncation Act, or Check 21. This legislation removes a legal impediment and should, over time, foster greater use of electronics in the check clearing process while also preserving the right of consumers and banks to receive paper checks. Ultimately, Check 21 should allow depository institutions to provide new and beneficial services to their customers. I look forward, as I know you do, to its prompt enactment. And I thank the Committee and its staff for the strong support you have provided. Mr. Chairman and members of the Committee, during my years on the Board of Governors, I have done my best to contribute positively to all aspects of the Federal Reserve's many responsibilities. I look forward to the opportunity to continue to work with you and serve the nation as Vice Chairman of the Board of Governors. Thank you for your attention and for considering my nomination. I would be pleased to answer any questions.
For immediate release The Federal Reserve Board on Wednesday announced the execution of a Written Agreement by and among the First American Bank, Carpentersville, Illinois; the Illinois Office of Banks and Real Estate; and the Federal Reserve Bank of Chicago. A copy of the Written Agreement is attached.
For immediate release The Federal Reserve Board on Thursday announced its approval of the application by Wells Fargo & Company, San Francisco, California, pursuant to section 3 of the Bank Holding Company Act to acquire all the voting shares of Pacific Northwest Bancorp and thereby acquire its subsidiary, Pacific Northwest Bank, both in Seattle, Washington. Attached is the Board's Order relating to this action.
Remarks by Governor Ben S. Bernanke At the 28th Annual Policy Conference: Inflation Targeting: Prospects and Problems, Federal Reserve Bank of St. Louis, St. Louis, Missouri October 17, 2003 Panel Discussion Should the Federal Reserve announce a quantitative inflation objective? Those opposed to the idea have noted, correctly, that the Fed has built strong credibility as an inflation-fighter without taking that step, and that that credibility has allowed the Fed to be relatively flexible in responding to short-run disturbances to output and employment without destabilizing inflation expectations. So, the opponents argue, why reduce that flexibility unnecessarily by announcing an explicit target for inflation? It would be foolish to deny that the Fed has been quite successful on the whole over the past two decades. Whether the U.S. central bank would have been even more successful, had it announced an explicit objective for inflation at some point, is impossible to say. We just dont know. We cant re-run history; and although empirical cross-country comparisons can be useful, they are far from being controlled experiments. However, the relevant question at this point is not the unknowable outcome of the historical counterfactual but whether, given the initial conditions we face today, the adoption of an explicit inflation objective might not improve U.S. monetary policy in the future. The Feds environment today is different from that of the 1980s and 1990s in at least one important respect, which is that price stability is no longer just over the horizon but has been achievedcore inflation rates are currently not much above one percent. Thus, in contrast to the experience of the past 35 years or so, in which there could be little doubt about the Feds desired direction for inflation, today the risks to inflation are more nearly symmetrical; that is, inflation can be too low as well as too high. A case can be made, I believe, that when the economy is operating in the region of price stability, public expectations and beliefs about the central banks plans and objectives, always important, become even more so. First, because the public can no longer safely assume that the central bank prefers lower to higher inflation, expectations about future policy actions and future inflation may become highly sensitive to what the public perceives to be the Feds "just right" level of inflation. Uncertainty about this "just right" level of inflation thus may translate, in turn, into broader economic and financial uncertainty. Second, at very low inflation rates, the zero lower bound on the policy interest rate is more likely to become relevant, which increases the potential importance of effective expectations management by monetary policymakers. For example, when interest rates are very low, the best way to ease policy may be to explain to the public that the current low interest rate will be maintained for a longer period, rather than simply lowering the current rate. The enhanced importance of public beliefs and expectations about monetary policy in the region of price stability argues, it seems to me, for greater attention by the central bank to its methods of communication with the public in that region. On the premise that effective communication is even more crucial near price stability, I will focus today on how an incremental move toward inflation targeting, in the form of the announcement of a long-run inflation objective, might help the Fed communicate better and perhaps improve policy decisions as well, without the costs feared by those concerned about potential loss of flexibility. As usual, my views are not to be attributed to my colleagues on the Board of Governors of the Federal Reserve System or the Federal Open Market Committee. As a preliminary, I need to introduce the idea of the optimal long-run inflation rate, or OLIR for short. (Suggestions for a catchier name are welcome.) The OLIR is the long-run (or steady-state) inflation rate that achieves the best average economic performance over time with respect to both the inflation and output objectives. Note that the OLIR is the relevant concept for dual-mandate central banks, like the Federal Reserve. Thus it is not necessarily equivalent to literal price stability, or zero inflation adjusted for the usual measurement error bias. Rather, under a dual mandate, a strong case can be made that, below a certain inflation rate, the benefits of reduced microeconomic distortions gained from price stability are outweighed by the costs of toofrequent encounters of the funds rate with the zero-lower-bound on nominal interest rates. (This argument underlies the common view that there should be a buffer zone against deflation.) Hence, in general, the OLIR will be greater than zero inflation, correctly measured. Note also that the OLIR is an average long-run rate; variation of actual inflation around the OLIR over the business cycle would be expected and acceptable (Meyer, 2003). What is the OLIR for the U.S. economy? A fairly extensive recent literature has attempted to quantify the OLIR. (See, e.g., Coenen, Orphanides, and Wieland, 2003, and references therein.) Because direct measures of the benefits of low inflation are not available, in practice papers in this literature estimate the OLIR to be the lowest inflation rate for which the risk of the funds rate hitting the lower bound appears to be acceptably small. Interestingly, the results using this approach seem fairly consistent across models and specifications, with several papers (including work using the FRBUS model, see Reifschneider and Williams, 2000) having concluded that the risk of hitting the zero bound seems to decline sharply once the long-run average inflation rate rises to about 2 percent. In addition, other studies of the costs of very low inflation (such as the supposed effects of downward nominal wage rigidity on the allocation of labor) have found that these costs are also largely eliminated at inflation rates of about 2 percent (Akerlof, Dickens, and Perry, 1996; see also Altig, 2003). Fortuitously, then, it may be the case that something in the vicinity of 2 percent is the optimal long-run average inflation rate for a variety of assumptions about the costs of inflation, the structure of the economy, the distribution of shocks, etc. However, before we embrace that number, many details remain to be filled in. For example, in practice, much might depend on the specification of the inflation index, on assumptions made about the steady-state value of the real interest rate, and other factors. Also important would be getting a better sense of the range of uncertainty around this number. More research on this issue would be highly worthwhile. As the economy seems currently to be moving toward a sustainable expansion path, with a stabilizing rate of inflation, having an estimate of the OLIR likewise seems crucial to making good policy in the next few years. The issue is one that, in my view, the FOMC and the staff should be looking at carefully. Suppose, as I believe would be feasible, that the FOMC were able to agree on a value or central tendency for the OLIR, based on the results of staff research and discussion among Committee members. Of course, the value of OLIR would only be a rough approximation to the truth, but one cannot avoid making such approximations in policymaking, whether implicitly or explicitly. Should the FOMC then take the next step and announce this number to the public? Some have argued that such an announcement would be unnecessary because the Feds implicit inflation objective is already well understood by the market. I am skeptical. Publicly expressed preferences by FOMC members for long-run inflation have ranged considerably, from less than 1 percent to 2.5 percent or more. Long-run inflation expectations implicit in the pricing of inflation-indexed securities vary significantly over time, and the apparently high sensitivity of long-term nominal interest rates to Fed actions suggests some uncertainty about the Feds long-run inflation target (Gurkaynak, Sack, and Swanson, 2003). Gavin (2003) points out that the range of private-sector forecasts for inflation is typically higher for the U.S. than for inflation-targeting countries. If announcing the OLIR does not constrain short-run policy unduly, I really cannot see any argument against it. To reassure those worried about possible loss of short-run flexibility, my proposal is that the FOMC announce its value for the OLIR to the public with the following provisos (not necessarily in these exact words): (i) The FOMC believes that the stated inflation rate is the one that best promotes its output, employment, and price stability goals in the long run. Hence, in the long run, the FOMC will try to guide the inflation rate toward the stated value and maintain it near that value on average over the business cycle. (ii) However, the FOMC regards this inflation rate as a long-run objective only and sets no fixed time frame for reaching it. In particular, in deciding how quickly to move toward the long-run inflation objective, the FOMC will always take into account the implications for near-term economic and financial stability. As you can see, stating the OLIR with these provisos places no unwanted constraints on short-run monetary policy, leaving the Committee free to deal with current financial and cyclical conditions as the Committee sees fit. In this respect, the proposal is very similar to one recently advanced by Governor Gramlich (2003). To be clear, because neither the horizon at which the inflation objective is to be attained nor the expected path of inflation and output is specified under this proposal, what I am suggesting is not equivalent to inflation targeting as commonly understood. Instead, what is being proposed is an incremental step that I believe would provide important benefits in itself and which would leave the door open for further steps later if that seemed appropriate. In the language of Faust and Henderson (2003) at this conference, my objective is to get the mean of inflation right while leaving the determination of the variance open for future discussion and debate. Without any fixed time frames for reaching the optimal long-run inflation rate, would an announced value for the OLIR carry any credibility? I think it would, for the important reason that the OLIR is not an arbitrarily selected value. In particular, because this inflation rate would have been judged by the Committee to be the one under which the economy operates best in the long run, the FOMC would have an incentive to try to reach it eventually, even if it were not an announced long-run objective of policy . Thus, despite the lack of a time frame, the OLIR should have long-run credibility, that is, it should be the best (lowest-forecast-error) answer to the question: "What do you expect the average inflation rate in the United States to be over the ten-year period that begins (say) three years from now?" Additional reasons that the announcement would carry weight are the accumulated credibility of the Fed and the fact that we are presumably starting from a point near the optimal inflation rate, so that a period of costly disinflation will not be needed to reach the OLIR. In other words, this relatively unconstrained approach might not work for other central banks, and it might not have worked for the Fed at other times (e.g., when we were at early stages of the disinflation process); but given the current configuration of circumstances, it should work now. I have argued that announcing the OLIR would not have significant costs. What are the benefits? In my view, the announcement of the OLIR should serve as a useful clarification of the long-run objective of the Fed and would thereby provide a long-run anchor to monetary policy. Among other benefits, the announcement of the OLIR should help participants in financial markets price long-term bonds and other financial assets more efficiently; help to lower inflation risk in financial markets and in other forms of contracting; and tend to stabilize long-term inflation expectations more broadly, which in turn would make short-run stabilization policy more effective (Orphanides and Williams, 2003). Although the announcement of the OLIR would not constrain short-run policymaking in undesirable ways, it would nevertheless also help the market make inferences about the likely timing and extent of tightening and easing cycles, since all else equal the FOMC would want the inflation rate to move asymptotically toward the long-run desired level. For example, if the current inflation rate were known to be below the OLIR, that fact would convey some information about how long it will likely be before the Fed begins its tightening cycle. Because some of the principal benefits of announcing the OLIR would arise from the reduction of uncertainty in financial markets and in the economy more broadly, I prefer the announcement of a single number for the OLIR, or at least a number with a surrounding tolerance range that is as narrow as the Committee can live with. I acknowledge that the OLIR cannot be determined precisely. Nevertheless, to the extent that the FOMC is fairly indifferent over a modest range of long-run inflation rates, there would be a positive benefit to choosing a single number within that range and trying to coordinate public expectations on that number. Agreeing on and announcing a value for the OLIR might improve policymaking more directly, at least on the margin. In particular, the stated inflation objective would help guide policy during periods, like now, in which the economy is (we hope) returning to a sustainable growth path; at all times, it would also serve as a reminder to policymakers to keep one eye on the long run at the same time that they are reacting to current developments in the economy. But, to reiterate, it seems likely that the biggest gains would be in the area of communications. Sharing the OLIR with the public would address the most important information asymmetry in the system: namely, the publics imperfect knowledge of the FOMCs objectives. I believe this step would help to reduce the reliance of the Fed on complex and easily misinterpreted qualitative language in its communications with the public. I conclude with a word on the politics of this proposal. One concern frequently expressed about announcing an inflation objective is that the Congress would interpret the introduction of an inflation target as a repudiation of the dual mandate. This would be a misinterpretation, but I understand why some legislators might draw the wrong conclusion. However, it seems to me that the recent attention to the risk of deflation changes the political calculus. There now exists a broad awareness that an inflation rate that is too low, by raising the probability of deflation and a binding zero bound on the nominal interest rate, poses a threat to output and employment stability. Therefore the connection between the announced OLIR and the real side of the economy will be much more apparent to non-economists. Indeed, the entire rationale for the OLIR can be expressed in terms of jobs and growth. The FOMC might say to Congress: We dont want longrun inflation to be too high, because low inflation promotes growth and productivity. On the other hand, inflation shouldnt be too low, because we want to have all the room we need to respond to the dangers that deflation poses for output and employment. We pose the objective in terms of inflation only because that is what the Fed can control in the long run. It does not seem to me to be such a difficult case to make in terms of the existing dual mandate. In addition we would have the explicit proviso that important short-run economic and financial goals will not be sacrificed in order to reach the longterm inflation objective more quickly. Although it would be important to vet these ideas thoroughly with the relevant Congressional committees before proceeding, I am hopeful that a change of the type I am proposing would be acceptable to Congress as being within the spirit of existing legislation. REFERENCES Akerlof, George, William Dickens, and George Perry, "The Macroeconomics of Low Inflation," Brookings Papers on Economic Activity, 1996:1, 1-76. Altig, David, "What is the Right Inflation Rate?", Federal Reserve Bank of Cleveland, Economic Commentary, September 15, 2003. Coenen, Gunter, Athanasios Orphanides, and Volker Wieland, "Price Stability and Monetary Policy Effectiveness When Nominal Interest Rates are Bounded at Zero," ECB working paper series no. 231, May 2003. Gavin, William, "Inflation Targeting: Why it Works and How to Make it Work Better," Federal Reserve Bank of St. Louis, working paper 2003-027B, September 2003. Gramlich, Edward, "Maintaining Price Stability," remarks before the Economic Club of Toronto, Toronto, Canada, October 1, 2003. Gramlich, Edward, Maintaining Price Stability, remarks before the Economic Club of Toronto, Toronto, Canada, October 1, 2003. Gurkaynak, Refet, Brian Sack, and Eric Swanson, "The Excess Sensitivity of Long-Term Interest Rates: Evidence and Implications for Macroeconomic Models," working paper, Board of Governors of the Federal Reserve System, 2003. Meyer, Laurence, "Practical Problems and Obstacles to Inflation Targeting," presented at Economic Policy Conference, Federal Bank of St. Louis, October 16-17, 2003. Orphanides, Athanasios and John C. Williams, "Imperfect Knowledge, Inflation Expectations, and Monetary Policy," working paper, Board of Governors of the Federal Reserve System, June 2003. Reifschneider, David and John C. Williams, "Three Lessons for Monetary Policy in a Low-Inflation Era," Journal of Money, Credit, and Banking, November 2000, Part 2, 936-72.
No content found
No content found
No content found
Remarks by Governor Ben S. Bernanke At the Federal Reserve Bank of Dallas Conference on the Legacy of Milton and Rose Friedman's Free to Choose , Dallas, Texas October 24, 2003 It is an honor and a pleasure to have this opportunity, on the anniversary of Milton and Rose Friedman's popular classic, Free to Choose , to speak on Milton Friedman's monetary framework and his contributions to the theory and practice of monetary policy. About a year ago, I also had the honor, at a conference at the University of Chicago in honor of Milton's ninetieth birthday, to discuss the contribution of Friedman's classic work with Anna Schwartz, A Monetary History of the United States (Bernanke, 2002). I mention this earlier talk not only to indicate that I am ready and willing to praise Friedman's contributions wherever and whenever anyone will give me a venue, but also because of the critical influence of A Monetary History on both Friedman's own thought and on the views of a generation of monetary policymakers. In their Monetary History , Friedman and Schwartz reviewed nearly a century of American monetary experience in painstaking detail, providing an historical analysis that demonstrated the importance of monetary forces in the economy far more convincingly than any purely theoretical or even econometric analysis could ever do. Friedman's close attention to the lessons of history for economic policy is an aspect of his approach to economics that I greatly admire. Milton has never been a big fan of government licensing of professionals, but maybe he would make an exception in the case of monetary policymakers. With an appropriately designed licensing examination, focused heavily on the fine details of the Monetary History , perhaps we could ensure that policymakers had at least some of the appreciation of the lessons of history that always informed Milton Friedman's views on monetary policy. Today I will pass over Friedman's contributions to our knowledge of monetary history and focus instead on how his ideas have influenced our understanding both of how monetary policy works and how it should be used. That is, I will discuss both the positive and the normative implications of Friedman's thought. The usual disclaimer applies, that is, I speak for myself and not necessarily for my colleagues at the Federal Reserve. In preparing this talk, I encountered the following problem. Friedman's monetary framework has been so influential that, in its broad outlines at least, it has nearly become identical with modern monetary theory and practice. I am reminded of the student first exposed to Shakespeare who complained to the professor: "I don't see what's so great about him. He was hardly original at all. All he did was string together a bunch of well-known quotations." The same issue arises when one assesses Friedman's contributions. His thinking has so permeated modern macroeconomics that the worst pitfall in reading him today is to fail to appreciate the originality and even revolutionary character of his ideas, in relation to the dominant views at the time that he formulated them. To illustrate, I begin with the descriptive or positive side of Friedman's work on monetary policy. Here is a short summary of Friedman's own list of eleven key monetarist propositions, as put forth in the conclusion to his 1970 (note well that date) lecture, "The Counter-Revolution in Monetary Theory." These propositions are a reasonable description, I believe, of Friedman's basic views on how money affects the economy. Here they are (in my summary of slightly more detailed language in the original): There is a consistent though not precise relationship between the rate of growth of money and the rate of growth of nominal income. That relationship is not obvious, however, because there is a lag between money growth and nominal income growth, a lag that itself can be variable. On average, however, the lag between money growth and nominal income growth is six to nine months. The change in the rate of nominal income growth shows up first in output and hardly at all in prices. However, with a further lag of six to nine months, the effects of money growth show up in prices. Again, the empirical relationship is far from perfect. Although money growth can affect output in the short run, in the long run output is determined strictly by real factors, such as enterprise and thrift. Inflation is always a monetary phenomenon, in the sense that it can be produced only by money growth more rapid than output. However, there are many possible sources of money growth. The inflationary impact of government spending depends on its financing. Monetary expansion works by affecting prices of all assets, not just the short-term interest rate. Monetary ease lowers interest rates in the short run but raises them in the long run. Let me emphasize again that these propositions reflected Friedman's view as of some thirty-five years ago. At the time, they were far from being the conventional wisdom, as suggested by the term "Counter-Revolution" in the essay's title. What do we make of these propositions today? First, the empirical description of the dynamic effects of money on the economy given in the first six propositions would be viewed by most policymakers and economists today as being, as the British would put it, "spot on." As a minor illustration of this point, in my own academic research I contributed to a large modern econometric literature that has used vector autoregression and other types of time series models to try to quantify how monetary policy affects the economy. The economic dynamics estimated by these methods correspond very closely to those outlined in Friedman's propositions. These methods confirm that a monetary expansion (for example) leads with a lag of one to two quarters to an increase in nominal income. Perhaps more importantly, as Friedman emphasized, the responses of the quantity and price components of nominal income have distinctly different timing. In particular, as Friedman told us, a monetary expansion has its more immediate effects on real variables such as output, consumption, and investment, with the bulk of these effects occurring over two to three quarters. (I was going to say, as Friedman first told us, but perhaps the credit for that should go to David Hume. Milton's work is, after all, part of a long and great tradition of classical monetary analysis.) These real effects tend to dissipate over time, however, so that at a horizon of twelve to eighteen months the effects of a monetary expansion or contraction are felt primarily on the rate of inflation. The same patterns have been found in empirical studies for virtually all countries, not only by vector autoregression analysis but by more structural methods as well. They are reflected in essentially all contemporary econometric models used for forecasting and policy analysis, such as the FRBUS model at the Federal Reserve. The lag between monetary policy changes and the inflation response is the reason that modern inflation-targeting central banks, such as the Bank of England, set a horizon of up to two years for achieving their inflation objectives. Thus Friedman's description of the economic dynamics set in train by a monetary expansion or contraction, summarized in his first six propositions, has been largely validated by modern research. What about the other propositions? Friedman's seventh point, that money affects real outcomes in the short run but that in the long run output is determined entirely by real factors, such as enterprise and thrift, is of particular importance for both theory and policy. The proposition that money has no real effects in the long run, referred to as the principle of long-run neutrality, is universally accepted today by monetary economists. When Friedman wrote, however, the conventional view held that monetary policy could be used to affect real outcomes--for example, to lower the rate of unemployment--for an indefinite period. The idea that monetary policy had long-run effects--or, in technical language, that the Phillips curve relationship between inflation and unemployment could be exploited in the long run--proved not only wrong but quite harmful. Attempts to exploit the Phillips curve tradeoff, which persisted despite Friedman's warnings in his 1968 presidential address to the American Economic Association, contributed significantly to the Great Inflation of the 1970s--after the Great Depression, the second most serious monetary policy mistake of the twentieth century. The diagnosis of inflation in Friedman's eighth proposition, also controversial when he wrote, is likewise widely accepted today. Of course, as we all know, Friedman noted the close connection between inflation and money growth, though carefully acknowledging that excessive money growth could have many causes. As Milton and Rose discussed in Chapter 9 of the 1980 edition of Free to Choose , popular views in the 1960s and 1970s (and even the views of some Federal Reserve officials) held that inflation could arise from a variety of non-monetary sources, including the power of unions and corporations and the greediness of oil-producing countries. An unfortunate implication of these views, whose deficiencies were revealed by bitter experience under President Nixon, was that wage-price controls and other administrative measures could successfully address inflation. We understand today that the Great Inflation would simply not have been possible without the excessively expansionist monetary policies of the late 1960s and 1970s. Some of Friedman's descriptive propositions remain the subject of active research. For example, much research has investigated both theoretically and empirically the interactions of fiscal policy, monetary policy, and inflation. Friedman's view that fiscal deficits are inflationary only if they result in money creation, his ninth proposition, remains broadly accepted, but work by scholars such as Thomas Sargent, Neil Wallace, and Michael Woodford has shown that these links can be subtle. For example, Sargent and Wallace's "unpleasant monetarist arithmetic" suggested that a near-term tightening of monetary policy, by making the long-term fiscal situation less tenable, could (in principle at least) lead to inflation, because the public will anticipate that the fiscal deficit must be financed eventually by money creation. More recently, Woodford's fiscal theory of the price level suggests that nonsustainable fiscal policies can drive inflation, even if the central bank resists monetization. Following Woodford, Olivier Blanchard has recently argued that tight money policies in Brazil, by raising the government's financing costs and thus worsening the fiscal situation, might have had inflationary consequences. Although this subsequent work has refined our understanding of the relationship between monetary and fiscal policy, these analyses are not inconsistent with the spirit of monetarist propositions, which place the blame for inflation on overissuance of nominal government liabilities. Another area of pressing current interest derives from Friedman's tenth proposition, that monetary policy works by affecting all asset prices, not just the short-term interest rate. This classical monetarist view of the monetary transmission process has become highly relevant in Japan, for example, where the short-term interest rate has reached zero, forcing the Bank of Japan to use so-called quantitative easing methods. The idea behind quantitative easing is that increases in the money stock will raise asset prices and stimulate the economy, even after the point that the short-term nominal interest rate has reached zero. There is some evidence that quantitative easing has beneficial effects (including evidence drawn from the Great Depression by Chris Hanes and others), but the magnitude of these effects remains an open and hotly debated question. The only aspect of Friedman's 1970 framework that does not fit entirely with the current conventional wisdom is the monetarists' use of money growth as the primary indicator or measure of the stance of monetary policy. Clearly, monetary policy works in the first instance by affecting the supply of bank reserves and the monetary base. However, in the financially complex world we live in, money growth rates can be substantially affected by a range of factors unrelated to monetary policy per se , including such things as mortgage refinancing activity (in the short run) and the pace of financial innovation (in the long run). Hence, it would not be safe to conclude (for example) that the recent decline in M2 is indicative of a tight-money policy by the Fed. The imperfect reliability of money growth as an indicator of monetary policy is unfortunate, because we don't really have anything satisfactory to replace it. As emphasized by Friedman (in his eleventh proposition) and by Allan Meltzer, nominal interest rates are not good indicators of the stance of policy, as a high nominal interest rate can indicate either monetary tightness or ease, depending on the state of inflation expectations. Indeed, confusing low nominal interest rates with monetary ease was the source of major problems in the 1930s, and it has perhaps been a problem in Japan in recent years as well. The real short-term interest rate, another candidate measure of policy stance, is also imperfect, because it mixes monetary and real influences, such as the rate of productivity growth. In addition, the value of specific policy indicators can be affected by the nature of the operating regime employed by the central bank, as shown for example in empirical work of mine with Ilian Mihov. The absence of a clear and straightforward measure of monetary ease or tightness is a major problem in practice. How can we know, for example, whether policy is "neutral" or excessively "activist"? I will return to this issue shortly. Besides describing the effects of money on the economy, Friedman also made recommendations for monetary policy--the normative part of his framework. I will discuss just three of the most important of these. First, Friedman has emphasized the Hippocratic principle for monetary policy: "First, do no harm." Chapter 9 of Free to Choose contains a famous quote of John Stuart Mill, as follows: "Like many other kinds of machinery, (money) only exerts a distinct and independent influence of its own when it gets out of order." On this quote, Milton and Rose commented: "Perfectly true, as a description of the role of money, provided we recognize that society possesses hardly any other contrivance that can do more damage when it gets out of order." Friedman's emphasis on avoiding monetary disruptions arose, like many of his other ideas, from his study of U.S. monetary history. He had observed that, in many episodes, the actions of the monetary authorities, despite possibly good intentions, actively destabilized the economy. The leading case, of course, was the Great Depression, or as Friedman and Schwartz called it, the Great Contraction, in which the Fed's tightening in the late 1920s and (most importantly) its failure to prevent the bank failures of the early 1930s were a major cause of the massive decline in money, prices, and output. It is likely that Friedman's study of the Depression led him to look for means, such as his proposal for constant money growth, to ensure that the monetary machine did not get out of order. I hope, though of course I cannot be certain, that two decades of relative monetary stability have not led contemporary central bankers to forget the basic Hippocratic principle. A second normative recommendation, worth recalling here, was Friedman's preference for floating rather than fixed exchange rates. At times, at least in popular writing, Friedman rationalized this position as following from free market principles. This argument is a bit disingenuous, I think, as a fixed nominal exchange rate is just one method of anchoring the aggregate price level and is perfectly consistent with free adjustment of the relative prices of goods and services. In a more serious vein, Friedman understood that, in a world in which monetary policymakers put domestic economic stability above balance of payments considerations, a fixed exchange rate system is likely to be unstable during periods of economic stress. He saw that this was the case during the 1930s, when the world was on a modified gold standard called the gold exchange standard, and it was likewise the case under the postwar Bretton Woods system. To reconcile a fixed exchange rate and an emphasis on domestic stability, policymakers must impose capital controls or restrictions on trade, which have undesirable effects on economic efficiency. If policymakers' first priority is stability of the domestic economy, Friedman reasoned, then why not adopt a system--namely, flexible exchange rates--that provides the necessary monetary independence without restrictions on the flow of capital or goods? When Friedman wrote about fixed and flexible exchange rates, a switch from the Bretton Woods fixed-exchange-rate system to a floating-rate system seemed quite unlikely. In this, as in many other matters, he was prescient, as the major currencies have now been successfully floating since the breakup of the Bretton Woods system in the early 1970s. These two recommendations have had major effects on institutional design and policy practice. However, in my view, the most fundamental policy recommendation put forth by Milton Friedman is the injunction to policymakers to provide a stable monetary background for the economy. I take this to be a stronger statement than the Hippocratic injunction to avoid major disasters; rather, there is a positive argument here that monetary stability actively promotes efficiency and growth. (Hence Friedman's suggestion that the long-run Phillips curve, rather than vertical, might be positively sloped.) Also implicit in Friedman's focus on nominal stability is the view that central banks should avoid excessively ambitious attempts to manage the real economy, which in practice may exacerbate both nominal and real volatility. In Friedman's classic 1960 work, A Program for Monetary Stability , he suggested that monetary stability might be attained by literally keeping money stable: that is, by fixing the rate of growth of a specific monetary aggregate and forswearing the use of monetary policy to "fine-tune" the economy. Do contemporary monetary policymakers provide the nominal stability recommended by Friedman? The answer to this question is not entirely straightforward. As I discussed earlier, for reasons of financial innovation and institutional change, the rate of money growth does not seem to be an adequate measure of the stance of monetary policy, and hence a stable monetary background for the economy cannot necessarily be identified with stable money growth. Nor are there other instruments of monetary policy whose behavior can be used unambiguously to judge this issue, as I have already noted. In particular, the fact that the Federal Reserve and other central banks actively manipulate their instrument interest rates is not necessarily inconsistent with their providing a stable monetary background, as that manipulation might be necessary to offset shocks that would otherwise endanger nominal stability. Ultimately, it appears, one can check to see if an economy has a stable monetary background only by looking at macroeconomic indicators such as nominal GDP growth and inflation. On this criterion it appears that modern central bankers have taken Milton Friedman's advice to heart. Over the past two decades, inflation has fallen sharply and stabilized around the world, not only in the industrialized nations but in emerging-market economies and in even the poorest developing nations. Some central banks, so-called inflation targeters, have set explicit, quantitative targets for inflation; but all central banks, certainly including the Federal Reserve, have emphasized the importance of achieving and maintaining price stability. On the issue of inflation control, Friedman may be judged to have been a bit too pessimistic; his concerns that central banks would have neither the technical ability nor the correct incentives to control inflation led him to recommend his money-growth rule, for which a central bank could certainly be held accountable. Evidently, however, determined central banks can stabilize inflation directly, at least they have been able to do so thus far. However, on the benefits of monetary stability, or as I would prefer to say, nominal stability, Friedman was not wrong. Many theories popular even today might lead one to conclude that increased stability in inflation could be purchased only at the cost of reduced stability in output and employment. In fact, over the past two decades, increased inflation stability has been associated with marked increases in the stability of output and employment as well, both in the United States and elsewhere. It has been argued that a lower incidence of exogenous shocks explains these favorable developments, and that may be part of the story. But I believe that there is an important causal relationship as well. For example, low and stable inflation has not only promoted growth and productivity, but it has also reduced the sensitivity of the economy to shocks. One important mechanism has been the anchoring of inflation expectations. When the public is confident that the central bank will maintain low and stable inflation, shocks such as sharp increases in oil prices or large exchange rate movements tend to have at most transitory price-level effects and do not result in sustained inflationary surges. In contrast, when inflation expectations are poorly anchored, as was the case in the 1970s, shocks of these types can destabilize inflation expectations, increasing the inflationary impact and leading to greater volatility in both inflation and output. In summary, one can hardly overstate the influence of Friedman's monetary framework on contemporary monetary theory and practice. He identified the key empirical facts and he provided us with broad policy recommendations, notably the emphasis on nominal stability, that have served us well. For these contributions, both policymakers and the public owe Milton Friedman an enormous debt.
No content found
Remarks by Chairman Alan Greenspan At the Federal Reserve Payments System Development Committee 2003 Conference, Washington, D.C. October 29, 2003 Introduction It is a pleasure to be with you this morning and to add my welcome to this very important conference on the payments system being hosted by the Federal Reserve's Payments System Development Committee. The name of the conference-- The Payments System in Transition --captures not only the state of the financial services that we all use to make payments, but also the expectations of many payments system participants as they look to the future. Decades of incremental change have had significant cumulative effects on both our check and electronic payments systems. Recent data show that the number of checks written in the United States began to decline in the mid-1990s. In contrast, electronic payments, particularly debit cards and automated clearinghouse (ACH ) transactions, have grown substantially and now total about 40 billion per year. Overall, the number of electronic payments has increased almost fivefold in two decades and, this year or next, may well exceed the number of checks written. During the past few weeks, after substantial work by Congress and the financial industry, the Check Clearing for the 21st Century Act, popularly known as the Check 21 Act, was passed. The new law was signed yesterday by the President and will become effective next October. Passage of this greatly anticipated statute is an important event for the financial industry. In preparing for the new law, the industry has begun to discuss the types of check products and services it will provide to the public as the infrastructure and rules for clearing and settling checks evolve. Against this background of a system in transition, the participants in this conference will be debating the future of the products, services, and infrastructure that support our broad and heterogeneous national payments system. I hope we will all be enriched by this discussion and take home ideas that will help shape the thinking of users of the payments system, suppliers of payments and financial services, and public authorities, as they all work to improve and modernize the payments system over the next few years. Historical Perspective For most of the post-World War II period, cash and checks have been the predominant instruments for making retail and commercial payments in the United States. Public confidence in these instruments and their usefulness for conducting transactions was built up over a long period of time, which spanned national debates about the proper instruments and institutions to support a sound national monetary system. A very large infrastructure for handling these paper instruments has been developed and maintained by the private and public sectors. In the case of checks, for example, this infrastructure includes offices, equipment, and staff for rapidly processing, shipping, and presenting checks throughout the country, literally overnight. The foundation of much of the current payments system infrastructure was laid in the 1960s. At that time, a paperwork crisis was overwhelming the financial markets, as the rapid growth in financial activity outpaced the system's ability to clear and settle financial transactions and payments using traditional, manual processes. The response of both the financial industry and government was twofold. Automation was applied to paper-based clearing activities. In addition, new electronic systems for creating transactions and making payments were established where this seemed practical. In the payments arena, the ACH and bank credit card systems and the beginnings of debit card systems date from this era. Reacting to these developments and new systems, commentators of the time predicted the advent of the "cashless" and "checkless" society. We know the history of these predictions. In reality, the 1970s saw a burst of creativity in the establishment of electronic payments systems but a relatively slow rate of adoption by consumers and businesses. More recently, the 1990s saw a new burst of creativity, including ideas for creating products called "electronic cash" and "electronic checks," adapting existing forms of payment to the Internet, and experimenting with entirely new payments systems. Work also began on projects to convert checks to ACH or other electronic payments, at the point of sale or at lockboxes, to reduce the costs of processing and to speed the collection of funds. Some of the results of these experiments are now gaining increasing acceptance in the marketplace. Many more have not succeeded. Many lessons have been learned from these experiences, and I am sure you will be discussing them during the next two days. It would be easy to dismiss the experiments and predictions from the 1990s about change in the payments system as hyperbole reminiscent of the 1970s. I believe, however, that the situations today and in the 1970s are very different. First, although data on the use of cash in transactions is notoriously poor, the nominal value of per capita holdings of small-denomination bank notes--those used heavily in domestic commerce--is now growing very slowly, and the inflation-adjusted per capita value has recently declined somewhat. More important for this audience, data from surveys conducted for the Federal Reserve show that the use of checks in our society has now begun to decline. Second, data on credit card, debit card, and ACH usage show very strong and sustained growth, to the extent that electronic payments now account for about half of the number of all noncash payments. Third, through the use of electronic payments, retailers and billers are continuing to seek productivity gains and cost reductions in their transactions with consumers. Fourth, data from the Federal Reserve's surveys of consumer finances show that over time households across most age and income categories have been adopting basic electronic payment instruments, although, as might be expected, younger households are in the forefront. Finally, the U.S. government is actively working on new technologies and services to increase the use of electronics in both its payments and collections. Taken together, these factors point to increasing use of electronic payments, when and where economic factors press this outcome. Public-policy perspective From the perspective of public policy, the key objectives for the payments system have always been economic efficiency and safety as well as confidence. Indeed, these broad objectives, which include the goals of integrity, security, reliability, and accessibility, have been endorsed by the G-10 central banks as international objectives for major payment systems in key reports published by the Bank for International Settlements and used widely by the International Monetary Fund and World Bank. The challenge is to bring these abstract ideas to bear on particular payments systems issues during this time of transition. Efficiency. Turning first to efficiency, in the area of check collection, both the financial industry and the Federal Reserve Banks face classic issues involving the adjustment of infrastructure to declining demand. Moreover, various programs to convert checks to electronic payments at the point of sale or the lockbox, or to truncate checks early in the collection stream, imply that the pace of decline in the volume of paper-check clearings could well accelerate. However, checks also remain a highly convenient payment instrument with a long and tested history; they are unlikely to be completely eliminated as a major payment instrument any time soon. As a result, the financial industry and the Federal Reserve Banks face the prospect of declining demand for paper-check processing, but also significant uncertainty about the extent and timing of the decline. Nevertheless, we know that over time the efficient use of resources will require reductions in excess production and processing capacity as the market demand for checks and check processing declines. In addition to managing resources to meet declining demand, the financial industry is also producing innovations in check clearing and storage centered on the development of digital imaging and the development of archives for these images. The passage of the Check 21 Act is likely to invigorate these efforts and also to encourage a range of new check-clearing techniques that are only now being envisioned. Overall, these changes suggest not only a reduction in the scale of overall check clearing, but also a shift to new technologies and services to meet remaining demand. In passing, I would like to encourage banking organizations to participate in a new survey on check usage that the Federal Reserve will be conducting in 2004--a follow-up to a similar survey conducted in 2001. Improved data on check usage are very important in helping the financial industry and the public adjust smoothly to the changes that are now in process. In the area of electronic payments, the industry faces the opposite challenge from that in check services. Continuing to meet the growth of electronic payment processing with highly reliable service is an obvious priority from a public-policy perspective. However, simply accommodating growth may not be sufficient, and consideration should also be given to meeting the changing needs of the users of these systems. One of the common misconceptions in the analysis of payments systems is that only production or processing costs--that is, conditions of supply--matter from a market or public-policy standpoint. A problem in early predictions of the growth of electronic payments was the lack of attention to the needs of users, including the fact that electronic payments could be quite inconvenient and costly for many purposes. In retrospect, it has taken years of investments in electronic infrastructure at homes and businesses to support the use of electronic payments as a convenient and relatively low-cost alternative to checks. The emphasis in this conference on bringing users of the payments system together with the providers of financial services demonstrates the importance of a balanced approach as we examine challenges for the payments system. A number of these topics illustrate the need to identify the attributes that users value and demand in payments systems--both in current systems and in the next generation of these systems--and to determine how that demand will be met. A particularly important topic is how electronic payments systems can better meet the needs of business users. Business people frequently report that, from their perspective, a payment is only one part of an overall transaction or relationship with a counterparty. Other parts include orders, confirmations, shipping documents, invoices, and a variety of accounting and other information that supports a transaction or relationship. The complexity of this situation has created challenges for businesses as they integrate corporate information systems with electronic payment capabilities, and this complexity has likely slowed the adoption of electronic payments for a wide range of business purposes. I hope this conference will help underscore the need for businesses, financial institutions, technology vendors, and payments system operators to find common approaches and standards for addressing this issue. Turning to questions of infrastructure, I particularly encourage you to discuss ideas for the future design of the core U.S. electronic payments systems, including those of the Federal Reserve. Some of the current designs date back several decades, and significant changes have taken place in both technologies and business needs since that time. The markets will undoubtedly shape the use of payments systems. However, there are only a handful of core systems and it is very important that these systems be well designed so that they do not block market innovation. I am particularly pleased that the Payments System Development Committee has over time focused on barriers to such innovation. The overall payments system is built on complex rules, business practices, and technologies. Change can often be difficult. In this situation, structures built up in the past can become barriers to the implementation of new ideas. Where barriers do exist, it is important to address them and, when appropriate, remove them, so that the market can provide us with new and useful payment and financial products and services. From a broad perspective, the Check 21 Act continues the work of our society to ensure that the marketplace can respond flexibly to fundamental shifts in our technologies. The act--appropriately--does not mandate that checks be truncated and turned into electronic payments, nor does it mandate that all payments be made electronically. Instead, it strengthens a market-based approach to innovation in the check-collection system. The act allows depository institutions to take digital images of checks and truncate the original check, provided that they, or a subsequent institution, are also willing to create a substitute paper check if one is demanded, and to bear the liability for doing so. The act essentially removes an important barrier to innovation and frees depository institutions to apply new technologies and market-based ideas to traditional check-clearing activities. Safety and confidence. Safety and confidence are the other basic public-policy objectives for payments systems. Sound designs, rules, and risk-management practices promote the safety of payments for users and their financial institutions. Central banks have an ongoing interest in the safety and integrity of payments systems, because they provide the infrastructure for transferring money in the economy. Public confidence in the payments system is a closely related concern. As I noted earlier, confidence in the integrity of our basic paper payment instruments and payments systems was built up over a very long period of time. It is not surprising that society has, at times, been cautious in adopting new payment ideas. Attitudes toward payments systems are often closely linked to attitudes about money, since such systems are the means of transferring money to meet a wide range of obligations. If payments systems do not work well, that can have serious consequences for the wealth, plans, and reputations of many individuals and businesses. In this context, it seems highly likely that prudent users will require new systems to earn confidence with strong evidence that these systems will meet their needs in both normal and exceptional circumstances. As we have seen, the process of building confidence can take years, and most suppliers realize that confidence is an asset to be guarded vigorously. Recently payments systems have faced a number of challenges in the area of risk and risk management. For example, as payments systems such as the ACH have been more widely used to make payments over the telephone and the Internet, fraudulent transactions have reportedly increased. Recent initiatives have apparently improved the situation, but the financial industry has continued to express concerns about fraud and the need to address it. I trust that all conference participants will focus on appropriate future risk designs and risk-management practices. While the risk designs of some large-value payments systems have changed significantly over the past few years, the risk designs for core retail payments systems have changed less. Indeed, some of these designs continue to be based on concepts dating back to the 1970s. Limited change may be the appropriate response. However, past designs and strategies should not themselves become barriers to the development of future payments systems that are more aligned with new forms of commerce and technology. Another important issue in the post-September 11 environment is the degree of resilience of not only our large-value payments systems but also our retail systems. Today, the mix of paper and electronic payment options helps mitigate the risk of disruptions to retail payments in the event of terrorist attacks, power blackouts, telecommunications disruptions, or similar infrastructure problems. As the United States increasingly relies on electronic payments for retail transactions, however, the financial system will increasingly need to ensure confidence in the resilience of these systems in a variety of adverse circumstances. As always, heightened resilience has costs. If, however, high resilience is built into new system designs and technologies as they are developed, it may be possible to mitigate these costs while strengthening our infrastructure. Conclusion Over the next two days, you doubtless will be having many very useful discussions. This conference provides an opportunity to address a range of significant topical issues, including the implementation of the Check 21 Act, the direction of the financial industry as it adjusts to lower volumes of checks, and the adoption of new technologies and business practices for electronic check collection. In the area of electronic payments, there will be a variety of views on the development of services, designs, and infrastructure for the next generation of systems. The challenge is both to have vision for the future and to be grounded in the realities of the marketplace. Your insights on these topics will help inform the ideas and actions of both the industry and public authorities during this historic period of the payments system in transition.
Joint Press Release Board of Governors of the Federal Reserve System Federal Deposit Insurance Corporation Office of the Comptroller of the Currency Office of Thrift Supervision For Immediate Release October 30, 2003 Proposed Treatment of Expected and Unexpected Losses Under the New Basel Capital Accord The federal bank and thrift agencies on Thursday issued the attached statement regarding the Basel Committee on Banking Supervision's request for comment on a modification to its proposed international capital standards. The modification deals with the treatment of expected and unexpected losses. The Basel Committee will accept comments from all interested parties until December 31, 2003. Federal Reserve Dave Skidmore (202) 452-2955 OCC Kevin Mukri (202) 874-5770 FDIC David Barr (202) 898-6992 OTS Chris Smith (202) 906-6677
Remarks by Governor Susan Schmidt Bies At the Conference on Market Discipline, Federal Reserve Bank of Chicago, Chicago, Illinois October 31, 2003 Effective Market Discipline: The Roles of Auditors, Companies, and Analysts I want to thank the Federal Reserve Bank of Chicago and the Bank for International Settlements for the opportunity to speak to you tonight. This conference addresses the important and timely topic of market discipline. Last evening Tom Jones spoke about the challenges in achieving high-quality, convergent, international accounting standards. My remarks, too, will touch on the importance of sound accounting and auditing, but I will also focus on the growing importance of transparent disclosures to the effective functioning of financial markets. Further, I will discuss how information users, particularly those who provide analysis to investors, can improve the quality and accessibility of their analysis. I will use some recent research on pension fund accounting to illustrate this issue. And, at the risk of setting a bleak tone for the remainder of the evening, I will also discuss the difficulties associated with achieving meaningful disclosure. Accounting standards and appropriate disclosure are complex issues, and achieving proper and useful disclosure is not an easy task. Disclosure is not just a greater volume of information but information provided in context. Companies need to make disclosures in a way that promotes and encourages transparency. In the long run, useful disclosures will benefit well-managed companies by allowing these firms to gain access to funds in the marketplace at rates that reflect their sound management. Of course, investors cannot be left completely off the hook--a point too often ignored. Analysts and stakeholders have an obligation to carefully analyze disclosed information and demand better disclosure if what is provided is not adequate or useful. Only then can they promote and achieve market discipline, in the classic sense of buying and selling securities, or taking a more activist approach in proxy voting and in initiating reforms. Audit Quality Most firms favor accounting standards and disclosure that help to faithfully portray the economics of a transaction, although a few companies in recent years have not been completely transparent about their practices. In these latter situations, financial reports have neither reflected nor been consistent with the way the business has actually been run or the risks to which the business has actually been exposed. Many, if not most, of these recent cases reflect fraud and breakdowns in auditing rather than inadequate accounting standards. Markets must be able to rely on the quality of work of outside auditors. In 2002, we were reminded that markets do in fact harshly discipline companies that have misled markets about their performance. But we also saw a general run-up in debt spreads in 2002, as more corporate accounting and fraud abuses became known and the market became concerned about the general quality of audits. Thus, weaknesses in a few companies and major audit firms significantly raised the cost of capital broadly across markets. That is why the work of the new Public Company Accounting Oversight Board is so important--to re-establish the trust that investors can rely on the quality of audits. International accounting standards are moving toward an approach based on principles rather than the approach based on prescriptive rules that is familiar in the United States. The more complex and dynamic the business world becomes, the more important it is that accounting be based on strong principles that provide the framework for proper accounting of new types of transactions. However, I strongly believe that accounting standards based on principles cannot achieve the high-quality, reliable information that markets demand unless the quality of audits improves and there is insistence on high professional standards and integrity for corporate financial officers and auditors. I hope the efforts of the newly formed Public Company Accounting Oversight Board to enforce a higher professional standard at accounting firms will help restore consistency in financial reporting in U.S. financial markets. The International Accounting Federation's recent proposal to create a Public Interest Oversight Board should be supported by regulators in all countries to ensure that this renewed emphasis on high-quality professionalism for accountants and auditors becomes the standard. Further, this summer, bank regulators in the United States adopted guidance that would permit the regulators to debar an accountant from auditing a financial institution. The bank regulators have had this statutory power since the1991 passage of the FDIC Improvement Act but, until 2001, had never seen weaknesses in audit quality that were widespread enough to require guidance. In the past two years, however, we have seen such significant breaches in audit quality at financial institutions that formal guidance was adopted to communicate clearly that we will not tolerate the poor quality of audits and attestations that we were seeing at several banking organizations. Meaningful Disclosures and Financial Innovations What constitutes meaningful disclosure can be discussed in the context of the rapid and dramatic pace of financial innovations and risk management practices in the past few decades. During this period, firms acquired effective new tools to manage financial risk, one of which was securitization. Securitization helps a firm manage the risk of a concentrated exposure by transferring some of that exposure outside the firm. By pooling a diverse set of assets and issuing marketable securities, firms obtain liquidity and reduce funding costs. Of course, moving assets off the balance sheet and into special purpose entities, with the attendant creation of servicing rights and high risk residual interests retained by firms, generates its own risks and reduces transparency unless the firm takes additional disclosure steps. Firms also use derivatives to manage their risk exposures. Firms face risks from price fluctuations in currency, commodity, energy, and interest rate markets. More recently, firms have used a relatively new type of derivative, credit derivatives, which allow firms to purchase protection against the risk of loss from the default of a given entity. By purchasing such protection, financial and nonfinancial firms alike can limit or reduce their exposures to given borrowers or counterparties. Credit derivatives also allow financial firms to achieve a more-diversified credit portfolio by acquiring credit exposure to borrowers with which they do not have a lending relationship. For example, European insurance companies reportedly have used credit derivatives to acquire exposure to European corporations that, because they rely primarily on bank lending, have little publicly traded debt outstanding. The improvements in technology, the quick pace of financial innovation, and the evolving risk-management techniques almost ensure that businesses will increasingly use almost limitless configurations of products and services and sophisticated financial structures. A byproduct of these developments will be that outsiders will have ever more difficulty understanding the risk positions of many large, complex organizations. These developments represent significant challenges to standard setters and to firms. For market discipline to function, accounting boards must innovate to accurately capture these developments. Company managers must also do their part, by ensuring that public disclosures clearly identify all significant risk exposures whether on or off the balance sheet and their effects on the firm's financial condition and performance, cash flow, and earnings potential. With regard to securitizations, derivatives, and other innovative risk transfer instruments, accounting measurement of a company's balance sheet at a point in time may be insufficient to convey the full effect of a company's financial risk profile. Therefore, disclosures about how risks are being managed and the underlying basis for values and other estimates must be included in financial reports. Unlike typical accounting reports, information generated by risk management tends to be oriented less to a particular time and more to a description of the risks. To take an example from the world of banking, where the discipline of risk management is relatively well developed, a fair value report might say that the value of a loan portfolio is $300 million and has dropped $10 million from the previous report. However, the bank's internal risk report would show much more extensive information, such as the interest rate and credit quality of the assets and the range of values the portfolio would take under alternative future scenarios. Thus, unlike a user of the fair value report, the user of a risk management report could determine whether changes in value were due to declining credit quality, rising interest rates, or sales or payoffs of loans. Corporate risk officers have developed other types of reports that provide information on the extent to which the total return in a particular line of business compensates for the line's comprehensive risk. A reader of such a report can determine whether the growing lines of business have risk exposures that tend to offset those in other business lines thereby resulting in lower volatility for the earnings of the corporation as a whole. Complex organizations should continue to improve their risk management and reporting functions. When they are comfortable with the reliability and consistency of the information in these reports, they should begin disclosing this information to the market, perhaps in summary form, paying due attention to the need for keeping proprietary business data confidential. The test for useful disclosure should be the question, Are we--the firm and its accountants--providing investors with what is needed to evaluate accurately the risk position of the firm? Disclosure that meets this test would not only provide more qualitative and quantitative information about the firm's current risk exposure to the market but also help the market assess the quality of the risk oversight and risk appetite of the organization. And, by reducing uncertainty, it would lower the cost of, and increase access to, market funding. I particularly want to emphasize that disclosure need not be in a standard framework nor exactly the same for all organizations. Rather, we should all be insisting that each entity disclose the information that its investors need to evaluate the entity's risk profile in the most convenient and useful way. And we should keep in mind that disclosure without context may not be meaningful. Transparency means that the information presented allows an accurate understanding of the transaction that has transpired and challenges firms to present information that fosters market discipline. That is why Pillar 3 in the Basel II accord is so important. The Importance of Reading Disclosures: Defined-benefit Pension Plans The complexities and difficulties of establishing sound accounting standards and meaningful disclosure are illustrated by the treatment of expenses associated with defined-benefit pension plans. Though the details of pension accounting will surely cause many eyes to glaze over and make you all wish you were out "trick or treating" tonight, I do want to use this accounting standard as an example. In recent years we have seen how the actuarial principles on which these standards are based can produce, quite frankly, some very misleading measures of corporate earnings and balance sheets. Moreover, "full disclosure" of the underlying details, by itself, does not appear to be a panacea. The most widely criticized aspect of the U.S. accounting standard for defined-benefit pensions is the treatment of investment returns. In effect, firms use expectations of the long-term return on assets in defined-benefit plans to calculate current-period pension cost (income) while disguising the volatility actually occurring in the portfolio. At the same time, they use a spot rate to discount the future liabilities. This accounting is reconciled with the economic reality by gradual amortization of the discrepancies between the assumed and the actual returns experienced on pension assets. As many of you are aware, this smoothing feature created very large distortions between economic reality and the pension-financing cost accruals embedded in the income statement. Of course, this begs the question of whether the market was actually hoodwinked by the accounting for pension expenses and liabilities, a question that became quite relevant after the stock market tumbled--along with interest rates--in the early 2000s. A study recently undertaken by staff at the Federal Reserve Board tackled that very question. The study adopts the premise that most of what investors need to know about the true pension-financing costs, not the smoothed costs, is most accurately reflected in two numbers disclosed in the pension footnote. These two numbers are the fair market value of pension assets and the present value of outstanding pension liabilities. The net financing cost in the income statement, a potentially misleading measure, can be thought of as a translation of these balance sheet figures into an expected flow. The study then asks two questions: First, how close is the correspondence between the fair market value of net pension assets--that is, the value of assets net of liabilities reported in the footnotes--and its wayward twin, the net financing cost accrual embedded in the income statement? Second, when these two measures provide conflicting information, which measure does the market employ to value the firm? The study finds that, in normal times--that is, when asset prices have not been subject to unusually large swings--these two measures of pension plan value usually give fairly consistent signals of the pension plan's condition. In such times, the valuation implications of using one measure over the other would be fairly modest for most firms. However, by late 2001, after stock prices and interest rates had tumbled for eighteen months, these two measures of pension finances gave very different, in many cases highly contradictory, signals of the pension plan finances and their valuation implications. What is more, the study finds that, when these measures do diverge, stock prices are much more prone to reflect the misleading net financing cost rather than the more direct measure of the pension assets and liabilities disclosed in the footnotes. For many firms, the implied valuation discrepancies were quite large in 2001, when the average discrepancy was 5 percent to 10 percent. In other words, the average firm with a defined-benefit plan in 2001 may have been 5 percent to 10 percent overvalued relative to an otherwise similar firm without a defined-benefit plan. What lessons should we take away? Well, to begin with, the findings suggest that wholesale disclosure, by itself, does not automatically create transparency. Arguably, there are two additional requirements. First, the disclosures must be provided in a way that facilitates their incorporation into a financial analysis of the firm. And second, transparency requires that investors, at least those having the wherewithal, assume the responsibility of doing their homework. For instance, key information from the pension balance sheet can be readily incorporated into a valuation model only if investors or analysts can easily identify its evil twin, the financing cost accrual, and exclude it from their preferred measure of earnings. But the information needed to make this adjustment has not been presented in a way that encourages or even facilitates such an approach. And though certainly some analysts and investors had their eye on the ball, too many apparently could not be bothered. FASB and the International Accounting Standards Board (IASB) are actively pursuing reforms to pension accounting and disclosure. The new annual disclosures proposed in the draft should give a clearer picture of plan assets and the risks they harbor. In particular, companies will be required to disclose in their annual financial reports the actual and targeted percentages of plan assets that fall into various investment categories --in particular, stocks, debt securities, and real estate. Sponsors will also be required to report the long-run rates of return that they are assuming for each category. Another piece of information that is not currently in the proposal, but probably ought to be, is the amount invested in the firm's own stock. Whereas investing pension assets in general equities increases the sponsor's risk, investing in a firm's own stock boosts this effect since it is equivalent to increasing the firm's balance sheet leverage. When the company gets into financial difficulties, the ability to contribute to the pension plan will be the weakest just when the declining value of those plan assets requires larger contributions. Disclosure regarding cash flow information--that is, actual contributions--is also to be enhanced. In particular, the proposal is that sponsors will be required to detail contributions intended for the fiscal year that was just begun, showing contributions required by funding regulations, additional discretionary contributions, and any noncash contributions. FASB is proposing that some of the annual pension disclosures be reported on a quarterly basis. Currently, none of this information is updated except when the annual report is released. The proposal has firms reporting quarterly net pension cost broken out into its various components as well as disclosing substantial unanticipated contributions to their plans. If implemented, these proposed disclosures will shed a fair amount of additional light on this murky area of pension accounting. Investor Information Finally, I turn to the role of users of financial information. The objective of accounting and disclosure is to make relevant, transparent information available to the market. But as we have just seen for pensions, the leading analysts did not focus on information available about the funding status of defined-benefit pension plans when they were added to earnings per share each quarter. Thus, the analysts did not communicate that these "pennies" were just timing differences and should not be included in long-term earnings forecasts. As organizations become more complex and financial innovations become more arcane to the average investor, the question arises as to how the typical investor can receive good analysis of the companies they are considering for investment. The serious nature of the conflicts of interest between sell-side analysts and investment bankers has lead to major restructuring of the research function at investment banks. We have seen a reduction in these staffs as firms begin to try to find a way to cover the expense of the research function by other means. As a result, investors are currently finding it hard to get any information about some companies because alternative sources of analysis that are accessible to the average investor have not yet replaced the sell-side information. Consider the companies among the S&P 500, presumably these large firms should have the most analyst coverage since they are more likely to be in investors' portfolios. However, if you go to sources of analyst reports, there are firms among the S&P 500 that have less than two earnings estimates by analysts posted! Further, as a result of Sarbanes-Oxley, Securities and Exchange Commission actions, and recent shareholder suits, some firms have decided not to give earnings guidance. Thus, for many investors, receiving good independent analysis about companies that they want to consider for investment has become a problem. I hope that more attention will shift to improving information that goes to investors. Without improved information, we will not see the full effect that better accounting, disclosure, audit quality, and corporate governance could have on strengthening financial markets. Conclusion In conclusion, timely, accurate, and credible financial information for market participants is critical to the successful functioning of market discipline. Because not all concepts can be easily measured and fit into a rigid accounting framework, investors need to rely also on enhanced disclosures to provide a faithful portrayal of economic transactions. However, disclosure by itself is not necessarily equivalent to transparency. Rather, firms need to provide information in ways that allow and encourage investors to process it, and as I emphasized earlier, the form of disclosure need not be the same across firms. And if analysts and shareholders do not use the information at hand, even the best disclosure is useless. For all these reasons, disclosures may need to be reevaluated as circumstances dictate. Success is in the interest of everyone, not least of all the companies themselves because disclosure reduces uncertainty and their financing costs. Footnotes Julia Coronado and Steve Sharpe, "Did Pension Accounting Contribute to a Stock Market Bubble?" Brookings Papers on Economic Activity , July 2003.