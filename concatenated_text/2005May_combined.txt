No content found
Remarks by Chairman Alan Greenspan Risk Transfer and Financial Stability To the Federal Reserve Bank of Chicago's Forty-first Annual Conference on Bank Structure, Chicago, Illinois (via satellite) May 5, 2005 Chicago is the birthplace of modern financial derivatives markets. So this conference seems an appropriate place in which to reflect once more on the prodigious growth of these risk-transfer instruments and their implications for financial stability. Two years ago at this conference I argued that the growing array of derivatives and the related application of more-sophisticated methods for measuring and managing risks had been key factors underlying the remarkable resilience of the banking system, which had recently shrugged off severe shocks to the economy and the financial system. At the same time, I indicated some concerns about the risks associated with derivatives, including the risks posed by concentration in certain derivatives markets, notably the over-the-counter (OTC) markets for U.S. dollar interest rate options. Today I will pursue those concerns about concentration in greater depth, drawing on discussions that Federal Reserve staff have had with market participants. I will also address concerns that some observers have expressed about the use of credit derivatives to transfer risk outside the banking system and about the growing role of hedge funds in bearing risk in derivatives markets and the financial system generally. Derivatives: Potential Benefits and Risk-Management Challenges Perhaps the clearest evidence of the perceived benefits that derivatives have provided is their continued spectacular growth. As a consequence of the increasing demand for these products, the size of the global OTC derivatives markets, according to the Bank for International Settlements (BIS), reached a notional principal value of $220 trillion in June 2004. Indeed, the growth rate of the OTC markets was more rapid in 2001-04 than over the previous three years. At the same time, the growth rate of exchange-traded derivatives exceeded the growth rate of OTC derivatives over 2001-04. Throughout the 1990s, the Chicago futures and options exchanges debated whether the growth of the OTC markets was good or bad for their markets. The data seem to have resolved that debate. In the United States, the Commodity Futures Modernization Act of 2000 has permitted healthy competition between the exchanges and the OTC markets, and both sets of markets are reaping the benefits. The benefits are not limited to those that use derivatives. The use of a growing array of derivatives and the related application of more-sophisticated approaches to measuring and managing risk are key factors underpinning the greater resilience of our largest financial institutions, which was so evident during the credit cycle of 2001-02 and which seems to have persisted. Derivatives have permitted the unbundling of financial risks. Because risks can be unbundled, individual financial instruments now can be analyzed in terms of their common underlying risk factors, and risks can be managed on a portfolio basis. Partly because of the proposed Basel II capital requirements, the sophisticated risk-management approaches that derivatives have facilitated are being employed more widely and systematically in the banking and financial services industries. To be sure, the benefits of derivatives, both to individual institutions and to the financial system and the economy as a whole, could be diminished, and financial instability could result, if the risks associated with their use are not managed effectively. Of particular importance is the management of counterparty credit risks. Risk transfer through derivatives is effective only if the parties to whom risk is transferred can perform their contractual obligations. These parties include both derivatives dealers that act as intermediaries in these markets and hedge funds and other nonbank financial entities that increasingly are the ultimate bearers of risk. Concentration in Derivatives Markets: The Case of U.S. Dollar Interest Rate Options Financial consolidation has reduced the number of firms that, by acting as dealers, provide liquidity to the OTC derivatives markets. Two years ago I expressed particular concern about the implications of dealer concentration for risks in derivatives markets. Among the markets identified as appearing to be especially concentrated were the markets for U.S. dollar interest rate options. Those markets have become increasingly large and important as the U.S. markets for fixed-rate mortgage-backed securities (MBS) have grown and as an increasing share of those securities have come to be held by investors that manage the prepayment risks associated with those instruments. The dealers intermediate between these mortgage hedgers, who seek to purchase swaptions and interest rate caps, and sellers of various types of interest rate options. The mortgage hedgers include Fannie Mae and Freddie Mac and mortgage servicers. The options sellers include issuers of traditional callable debt and issuers of structured notes with interest rate caps or other embedded options. Hedge funds at times have been important sellers of options and suppliers of options market liquidity, especially during periods when increases in interest rate volatility have caused spikes in options prices. Concentration in the OTC options markets raises at least three specific concerns. First, market illiquidity may result from a leading dealer's exit and that illiquidity has the potential to adversely affect Fannie and Freddie and other hedgers of mortgages and MBS. Second, meeting the demands for options by mortgage hedgers involves market risk to dealers, a concern that has been heightened by the fact that the notional value of options sold by dealers significantly exceeds the notional value purchased. Third, the failure of a leading dealer could result in counterparty credit losses for market participants. The extent to which these concerns are valid depends on how effectively market participants manage market risk and counterparty credit risk. To obtain information on participants' risk-management practices, members of the Federal Reserve staff last summer interviewed some of the leading market participants, including Fannie and Freddie and half a dozen leading derivatives dealers. The potential for a dealer's exit to adversely affect mortgage hedgers is dependent upon hedgers' diversification of counterparties, the way in which hedgers use options, and the underlying reason for such an exit. Fannie and Freddie have about twenty dealers as options counterparties, including investment banks and foreign banks as well as U.S. commercial banks. However, only about five or six of them have direct access to the supply of options from debt issuers; the others must depend on the interdealer market for a substantial portion of their supply. The exit of one of these five or six may or may not adversely affect market liquidity, depending on the reason for the exit and on the way in which other dealers react. If a dealer is forced to exit because of a credit problem unrelated to its options dealing, other dealers are likely to take its place quickly. If the exit is the result of losses from options dealing, possibly in difficult market conditions, other dealers with similar positions are likely to be pulling back as well, which could leave the options markets quite illiquid. In any event, Fannie and Freddie and other mortgage hedgers do not rely on continuous liquidity in the options markets. Rather, they purchase options periodically and opportunistically. Provided that options market illiquidity is not protracted (say more than a month), they could postpone transacting in those markets until liquidity returns, without exceeding their internal risk limits. By contrast, mortgage hedgers do rely on continuous liquidity in the swaps market because they currently use swaps to execute dynamic hedges of the prepayment risk that is not hedged up front with options, and those markets are less concentrated and easier to intermediate than the options markets. Still, as the hedgers are aware, the supply of liquidity available to them in the swaps market is not unlimited. Their periodic purchases of interest rate options are intended to limit their reliance on dynamic hedging and their potential demands on swaps market liquidity. Nonetheless, concerns about potential disruptions to swaps market liquidity will remain valid until the vast leveraged portfolios of mortgage assets held by Fannie and Freddie are reduced and the associated concentrations of market risk and risk-management responsibilities are correspondingly diminished. Articles that appeared in the financial press in early 2004 called attention to the fact that the BIS data showed that the notional value of U.S. dollar interest rate options sold by OTC derivatives dealers exceeded the notional value of options purchased. This difference had been growing as the options markets had grown and had reached more than $800 billion in notional value (an amount equal to 15 percent of the notional value of options purchased) as of year-end 2003. Not surprisingly, an analysis of risks based solely on notional amounts turns out to be misleading. The interviews that Federal Reserve staff members conducted last year indicate that dealers run fairly well-balanced books in terms of sensitivities to changes in interest rates and especially to changes in interest rate volatility. The options that dealers sell tend to have terms that create less sensitivity to changes in interest rate volatility than the options that they buy. Thus, in order to limit the overall sensitivity of their options portfolios to changes in interest rate volatility, dealers must sell a larger notional value of options than they buy. Because the terms of the options that they sell differ substantially from the terms of the options that they buy, dealers do assume significant basis risks, and their hedging strategies are dependent on options market liquidity when rates and volatilities are changing rapidly. In general, such risks are monitored and limited by various internal controls. If the options markets were to become illiquid, dealers could suffer significant losses; but their controls appear to be sufficiently tight that the losses seem quite unlikely to be large enough to jeopardize such large, diversified intermediaries. Participants in the OTC derivatives markets typically manage their counterparty credit risks to dealers by transacting only with counterparties that are perceived to be highly creditworthy, by entering into legal agreements that provide for closeout netting of gains and losses, and with the exception of most exposures to the few Aaa-rated dealers, by agreeing to collateralize net exposures above a threshold amount. All the major participants in the markets for U.S. dollar interest rate options markets that Federal Reserve staff interviewed follow these practices. The widespread use of collateral, in particular, usually is a powerful means of limiting counterparty credit losses. However, when counterparties hold very large net positions in illiquid markets, as the hedge fund Long-Term Capital Management (LTCM) did in 1998, the effectiveness of collateral as a risk mitigant may be reduced significantly. In such circumstances, when the nondefaulting counterparties seek to close out their positions with a defaulting counterparty, those actions can cause market prices to move rapidly in directions that may amplify losses to levels significantly exceeding even very conservative collateral requirements. In contrast to LTCM, however, dealers typically limit the size of their net open positions in markets, even though their gross positions often substantially exceed the size of LTCM's. Thus, the collateralization of exposures to dealers is likely to be quite effective in limiting the counterparty risks from dealer concentration. Nonetheless, participants in the interest rate options markets and other derivatives markets should carefully evaluate the potential market effects of the failure of a dealer or any other large market participant and the ways in which such effects could magnify both their counterparty credit risks and their market risks. In summary, as we have come to understand more clearly how participants in the OTC interest rate options markets use those markets and manage the risks associated with their use, wariness about concentration in these markets, though diminished, has not disappeared. The Federal Reserve remains concerned that the stress tests that some large participants are using to evaluate potential losses in the event of a large participant's default do not fully capture the potential interaction of counterparty credit risk and market risk, especially in concentrated markets. Use of Credit Derivatives to Transfer Risk outside the Banking System Perhaps the most significant development in financial markets over the past ten years has been the rapid development of credit derivatives. Although the first credit derivatives transactions occurred in the early 1990s, a liquid market did not emerge until the International Swaps and Derivatives Association succeeded in standardizing documentation of these transactions in 1999. According to the BIS, the notional value of credit derivatives outstanding increased sixfold between 2001 and 2004, reaching $4.5 trillion in June of last year. Moreover, this growth has been accompanied by significant product innovation, notably the development of synthetic collateralized debt obligations (CDOs), which allow the credit risk of a portfolio of underlying exposures to be divided or "tranched" into different segments, each with different risk and return characteristics. Recent growth of credit derivatives has been concentrated in these more-complex structured products. As is generally acknowledged, the development of credit derivatives has contributed to the stability of the banking system by allowing banks, especially the largest, systemically important banks, to measure and manage their credit risks more effectively. In particular, the largest banks have found single-name credit default swaps a highly attractive mechanism for reducing exposure concentrations in their loan books while allowing them to meet the needs of their largest corporate customers. But some observers argue that what is good for the banking system may not be good for the financial system as a whole. They are concerned that banks' efforts to lay off risk using credit derivatives may be creating concentrations of risk outside the banking system that could prove a threat to financial stability. A particular concern has been that, as credit spreads widen appreciably at some point from the extraordinarily low levels that have prevailed in recent years, losses to nonbank risk-takers could force them to liquidate their positions in credit markets and thereby magnify and accelerate the widening of credit spreads. A definitive evaluation of these concerns about nonbank risk-takers would require information on the extent of credit risk transfer outside the banking system and on the identities and risk-management capabilities of the entities to which the risk has been transferred. Unfortunately, available data do not provide this information. Data on the size of the credit derivatives markets are limited largely to the notional principal amounts of transactions. As discussed earlier, notional amounts often are misleading indicators of risk, and this problem is acute regarding credit derivatives. Critical to any evaluation of the CDO markets is an understanding that, per dollar of notional value, the risk (and risk transfer) associated with various CDO tranches varies enormously. The risk per dollar of notional amount of the "first loss," or equity, tranche can be thirty or forty times the risk per dollar of the senior tranche, which would be required to absorb losses only after the protection provided by the equity tranche and other more-junior tranches had been exhausted. While available data cannot resolve these issues, a study conducted last year by the Joint Forum, which was based on interviews with market participants, does shed some light. The study concluded that notional values had significantly overstated the amount of credit risk that had been transferred outside the banking system to that point and that the amount of risk transfer was quite modest relative to the total amount of credit risk that exists in the financial system. The study found no evidence of "hidden concentrations" of credit risk. Risk-takers outside the banking system included monoline insurers and other insurance companies; private asset managers acting on behalf of pension funds, mutual funds, and other institutional investors; and hedge funds. As to the risk-management capabilities of these nonbank entities, the study found that they seem largely aware of the risks associated with credit derivatives. The study did note that understanding the credit risk profile of CDO tranches poses challenges to even the most-sophisticated market participants. An especially difficult issue is the assessment of default correlation across different reference entities. In general, the valuation of CDO tranches is model dependent, and market participants need to carefully evaluate the models that they use and the model parameter assumptions that they make, notably the assumptions regarding default correlations. To limit legal and reputational risks, dealers should seek to foster a complete understanding of transactions among the investors to which they sell CDO products. The report cautioned investors in CDO tranches not to rely solely on rating-agency assessments of credit risk, in part because a CDO rating cannot possibly reflect all the dimensions of the risk of these complex products. The report also called attention to significant operational risks that have emerged as market participants' back offices have struggled to keep pace with growing transactions volumes and more complex products. Despite recent automation initiatives, the lack of timely documentation of new transactions and assignments of existing transactions remains a significant problem. Some other concerns about the transfer of credit risk outside the banking system seem to be based on questionable assumptions. Some observers believe that credit risks will be managed more effectively by banks because they generally are more heavily regulated than the entities to which they are transferring credit risk. But those unregulated and less heavily regulated entities generally are subject to more-effective market discipline than banks. Market participants usually have strong incentives to monitor and control the risks they assume in choosing to deal with particular counterparties. In essence, prudential regulation is supplied by the market through counterparty evaluation and monitoring rather than by authorities. Such private prudential regulation can be impaired--indeed, even displaced--if some counterparties assume that government regulations obviate private prudence. We regulators are often perceived as constraining excessive risk-taking more effectively than is demonstrably possible in practice. Except where market discipline is undermined by moral hazard, for example, because of federal guarantees of private debt, private regulation generally has proved far better at constraining excessive risk-taking than has government regulation. In fact, while many focus on the dangers of risk transfer to highly leveraged entities that might be vulnerable to a sharp widening of credit spreads, a significant portion of the risks that are being transferred outside the banking system are being transferred through private asset managers to institutional investors that have much lower leverage than banks. Indeed, the increasing transfer of systematic risks from banks to entities with lower leverage and longer time horizons may, other things equal, push credit spreads lower. Such investors may naturally have a greater tolerance for risk than banks. The Growing Role of Hedge Funds in Derivatives Markets and the Financial System Generally Of course, much of the unease about credit risk transfer outside the banking system reflects the growing role that hedge funds play in those markets and in the financial system generally. Although comprehensive data on the size of the hedge fund sector do not exist, total assets under management are estimated to be around $1 trillion. Inflows to hedge funds have been especially heavy since 2001, as investors have sought alternatives to long-only investment strategies in the wake of the bursting of the equity bubble. By some estimates, the size of the hedge fund sector doubled between 2001 and 2004. A substantial portion of the inflows to hedge funds in recent years reportedly has come from pension funds, endowments, and other institutional investors rather than from wealthy individuals. Hedge funds have become increasingly valuable in our financial markets. They actively pursue arbitrage opportunities across markets and in the process often reduce or eliminate mispricing of financial assets. Their willingness to take short positions can act as an antidote to the sometimes-excessive enthusiasm of long-only investors. Perhaps most important, they often provide valuable liquidity to financial markets, both in normal market conditions and especially during periods of stress. They can ordinarily perform these functions more effectively than other types of financial intermediaries because their investors often have a greater appetite for risk and because they are largely free from regulatory constraints on investment strategies. But some legitimate concerns have been expressed about the possible adverse effect of hedge funds' activities on market liquidity in some circumstances. One such concern is the potential for rapid outflows from the sector in the event that returns prove disappointing. Disappointments seem highly likely given the number of recent investors in this sector, all seeking arbitrage opportunities that of necessity will diminish as more capital is directed to exploiting them. Furthermore, some (perhaps many) hedge fund managers are likely to prove incapable of delivering the returns that investors apparently expect. Indeed, investors have already forced many hedge funds to fold after producing disappointing returns. Provided that investors do not force exiting funds to suddenly liquidate their assets, such exits contribute to the efficiency of the financial system and do not adversely affect market liquidity. Historically, investors have not been able to force the sudden liquidation of a hedge fund because investments have been subject to lengthy redemption or "lock up" requirements. However, there are reports that institutional investors have been able to negotiate much shorter redemption periods. If institutional money proves to be "hot money," hedge funds could become subject to funding pressures that would impair their ability to supply liquidity to markets and might cause them to add to demands on market liquidity. Another circumstance in which hedge funds could negatively affect market liquidity is if they became so leveraged that adverse market movements could lead to their failure and force their counterparties to close out their positions and liquidate their collateral. For example, the fear of the market effects of closeout and liquidation of LTCM's very large net positions motivated its counterparties to recapitalize the hedge fund in 1998. LTCM was able to become so large and so highly leveraged because its derivatives and repo market counterparties, perhaps awed by the reputations of its principals, failed to effectively manage their credit risk to LTCM. In the wake of the LTCM episode, the large banks and securities firms that were counterparties to hedge funds strengthened their management of hedge fund risk very significantly. Those improvements were motivated by their self-interest, which was reinforced by recommendations from their prudential supervisors and from the Counterparty Risk Management Policy Group (CRMPG), a group of twelve banks and securities firms that were among the most significant counterparties to hedge funds. However, recently there have been reports that competitive pressures have resulted in some weakening of risk-management practices. In light of these reports and of the rapid growth of the hedge fund industry, the Federal Reserve recently reviewed banks' management of hedge fund credit risk in relation to the recommendations that supervisors and the CRMPG made in 1999. The review indicated that, despite some recent slippage, banks have made considerable progress in implementing many of those recommendations and thereby in strengthening their risk-management practices. In particular, banks can now capture their aggregate credit exposures to individual funds, and their measures of the credit exposures now incorporate the risk-mitigating effects of collateral requirements. Furthermore, most banks now stress test the potential effects of volatile or illiquid markets on their exposures. Banks' due diligence procedures and hedge funds' disclosures have improved sufficiently that banks now can qualitatively assess the risk-management capabilities and overall risk profiles of the funds. Nonetheless, the review noted some remaining weaknesses. First, because many fund managers are reluctant to provide banks with complete information about their portfolios or with forward-looking measures of the risks that the funds are assuming, the banks often cannot fully evaluate a fund's risk profile. Banks sometimes tighten collateral requirements and other credit terms to compensate for this lack of transparency, but most banks' policies could be improved by the establishment of clearer and firmer links between credit terms and transparency. Second, banks do not always aggregate stress test results across hedge fund counterparties to assess concentrations of exposures in volatile and illiquid markets. Third, and perhaps of greatest concern, in certain highly liquid markets, especially OTC interest rate swaps and repos, there are signs that competitive pressures may be eroding the protection that banks achieve through collateral requirements by reducing the initial margins that they obtain from hedge funds. Thus, the review suggests that banks and their supervisors need to be alert to the possibility that further slippage of credit terms could result in material increases in credit risk to banks, a material loss of market discipline on hedge funds, and a material increase in the potential for hedge fund leverage to adversely affect market dynamics. Perhaps the recent widening of credit spreads will engender increased caution by managers of credit risk. Moreover, as in 1999, cooperative private-sector efforts to identify and implement sound risk-management practices have the potential to reinforce the efforts of individual firms and their prudential supervisors. In this regard, a very encouraging development is the recent formation, by leading banks, securities firms, insurance companies, asset managers, and hedge funds, of a new group (CRMPG II) to assess improvements in risk management since 1999 and to update the CRMPG recommendations to reflect subsequent changes in risk-management practices and in the financial, regulatory, and legal environment. Ensuring sound credit-risk management by hedge funds' counterparties remains the most promising approach to addressing concerns about hedge fund leverage. Some may believe that government regulation of hedge fund leverage would be more effective. But it would be very difficult to design a set of capital requirements for hedge funds that is appropriately sensitive to the diversity and flexibility of investment strategies that different funds employ and to the lack of diversification in the portfolios of individual funds. A regulatory capital regime that was not extraordinarily risk-sensitive would be ineffective at constraining hedge funds' risk-taking. At the same time, it would impair their capacity to pursue strategies that enhance the efficiency and liquidity of our financial markets and thereby to contribute to the productivity and resilience of our economy. Conclusion The rapid proliferation of derivatives products inevitably means that some will not have been adequately tested by market stress. Even with sound credit-risk management, a sudden widening of credit spreads could result in unanticipated losses to investors in some of the newer, more complex structured credit products, and those investors could include some leveraged hedge funds. Risk management involves judgment as well as science, and the science is based on the past behavior of markets, which is not an infallible guide to the future. Yet the history of the development of these products encourages confidence that many of the newer products will be successfully embraced by the markets. To be sure, for that favorable record to be extended, both market participants and policymakers must be aware of the risk-management challenges associated with the use of derivatives to transfer risk, both within the banking system and outside the banking system. And they must take steps to ensure that those challenges are addressed. Footnotes Concerns about the availability of high-quality collateral sufficient to meet the growing demands in these markets have abated as the use of cash as collateral has become widespread. The International Swaps and Derivatives Association estimates that, at the beginning of 2005, cash accounted for nearly three-quarters of the collateral held to support derivatives exposures. Recent spikes in some credit default swap spreads do not appear to have induced significant stress, although this experience has been too limited to be definitive. The Joint Forum (2004), Credit Risk Transfer (Basel: Bank for International Settlements). The Joint Forum was established in 1996 under the aegis of the Basel Committee on Banking Supervision (BCBS), the International Organization of Securities Commissions (IOSCO) and the International Association of Insurance Supervisors (IAIS) to deal with issues common to the banking, securities, and insurance sectors, including the regulation of financial conglomerates. Counterparty Risk Management Policy Group (1999). " ," report available through the House Committee on Financial Services.
Testimony of Governor Susan Schmidt Bies The Basel II Accord and H.R. 1226 Before the Subcommittee on Domestic and International Monetary Policy, Trade, and Technology and the Subcommittee on Financial Institutions and Consumer Credit, Committee on Financial Services, U.S. House of Representatives May 11, 2005 Chairman Bachus, Chairman Pryce, and members of the Subcommittee on Financial Institutions and Consumer Credit and the Subcommittee on Domestic and International Monetary Policy, Trade, and Technology: It is a pleasure to join my colleagues from the other banking agencies to discuss the current status of Basel II in this country, as well as the Federal Reserve's views on H.R. 1226. The continued discussion among the Congress and the regulators--and, of course, the banking industry and other members of the public--is critical to the final implementation of the new capital accord. The focus of recent attention has been the agencies' announcement that they will delay the notice of proposed rulemaking (NPR) for Basel II, originally scheduled for midyear 2005. In my remarks today, I will discuss the reasons for the delay and the Board's views regarding the timetable for implementation of Basel II in this country. But, first, I believe it may be useful to remind the members of the subcommittees why the agencies thought it wise to explore and then develop a modernization of the current capital accord; those factors have become, if anything, more important than they were when we began the process. Our banking system is becoming more concentrated, with a number of very large entities operating across multiple business lines and national boundaries, each entity with positions and exposures that are both complicated and difficult for third parties to understand. These entities have outgrown the current regulatory capital regime, which is still adequate for most banks. But the current rules simply cannot keep up with the complex business of the global banking organizations toward which Basel II and its infrastructure prerequisites are directed. These organizations represent significant risks to the financial system should they develop substantial problems in a period of stress. Basel II offers the opportunity to work with these large entities to develop quantitative risk-measurement and risk-management systems that can both measure their risk more accurately and become the basis for more risk-focused capital requirements and prudential supervision. We would also require, as part of the Basel II approach, more public disclosures to improve market discipline and supplement supervisory efforts. Many internationally active U.S. banks apparently agree that we should work toward Basel II. Indeed, we and the industry have already seen some benefits from work on Basel II implementation in that market participants and bank supervisors have developed a common language for inputs into risk-management processes. Earlier this year, twenty-six banking organizations provided us with internal measures of credit risk as part of the fourth quantitative impact study, or QIS4. The agencies have now reviewed the risk parameter estimates provided and are discussing with individual participants their approaches to developing the required inputs. These discussions, which are ongoing, have significantly changed some of the data provided, and some modifications are still coming in. Nonetheless, even with these revisions, two conclusions are already clear. First, the dispersion among the banks in their estimates of the key parameters that would be used to calculate Basel II capital requirements was quite wide--much wider than expected. Second, the implied reductions in minimum regulatory capital were often substantial--far more than previous quantitative impact studies, both here and abroad, had suggested. As responsible and prudent regulators, we believe it is appropriate to improve our understanding of these results and to see whether changes might be needed in our proposals. From the outset of our participation in the development of Basel II, the U.S. agencies have clearly and consistently stated that the final adoption of the new capital rules in the United States would occur only after (1) we had reviewed all public comments and incorporated any needed adjustments to address legitimate concerns, and (2) we were satisfied that Basel II was consistent with safe and sound banking in this country. Throughout this process we have stressed that, should we become concerned about the level of overall capital in the banking system or the capital results for individual portfolios, we would seek to modify the framework, including possibly recalibrating the regulatory capital formulas that translate an individual bank's risk parameters into required capital. The agencies' current review and study is consistent with our historical position at Basel. All of the agencies want to have a better understanding of QIS4 data and results. Does the dispersion reflect different risk profiles? Different model assumptions? Different estimates of risk for the same kind of asset? Different kinds of internal rating systems with some looking "through the cycle" and others being "point in time"? Different stages of institutions' implementation efforts? Limitations of current data bases? Some other factor? We hope that further analysis and discussion with respondents can provide some answers to such questions. All the agencies believed that the prudent approach was to delay the NPR to gain better understanding of the reasons for the unexpected results. Still, this decision presents the U.S. banking agencies with a dilemma. There is good reason to delay the NPR and related supervisory guidance, but those very documents are needed to provide more complete blueprints for what banks will need in terms of the databases and systems to implement Basel II. We are not saying that these large entities today have inadequate risk-management systems. Rather, they do not yet have the systems for producing Basel II inputs that meet the standards set forth in the Basel II proposal. Until the banking organizations have the NPR, many just will not be able to provide us the inputs we need to assess how banks would operate under Basel II. The dilemma can be solved only by first issuing the NPR. But, we must then have a prudent and flexible way to make adjustments should the resultant data produce results that we, as supervisors, are not comfortable with. The plans developed before the delay in the NPR offer just such an opportunity. Under those plans, institutions required or planning to move to Basel II would, after the adoption of a final rule, decide when to start their parallel run--with the first opportunity in January 2007. During the required parallel run, each bank would continue to calculate its required capital under the current capital regime and simultaneously calculate its Basel II capital statistics for review by its primary supervisor. When the supervisor believes that the bank has produced four quarters of credible Basel II estimates, the bank would be able to enter a minimum two-year transition run, the earliest in 2008 under these plans. During this transition run, the bank would be under Basel II capital rules, but it could not reduce its capital below 90 percent of what the current capital rules would require in the first year or below 80 percent in the second year. The length of either the parallel run or the transition run could be extended if the primary supervisor had doubts about the bank's Basel II system or the prudence of the resulting minimum regulatory capital level. Only after a minimum two-year transition run and only if its primary supervisor had no objection could a U.S. bank operate fully under Basel II capital rules. This phase-in plan has been designed to ensure that bank inputs are reasonable and consistent with sound risk-management practices and that supervisors are comfortable with the safety and soundness of Basel II before it goes fully "live" in the United States. Please note that only when we get into the parallel run period will the agencies be able to accurately assess the aggregate capital effects as well as the effects on individual institutions from the new accord. Only then will banks' systems provide risk-parameter inputs that comport with the operational requirements of Basel II, and only then can the U.S. authorities be confident that the resultant capital calculations are reliable estimates of what will happen when Basel II is fully implemented. Such data would be far superior to those obtained through the four QIS exercises that we have conducted to date, which, as I have noted, have been carried out by banks on a best-efforts basis using systems that do not yet meet the standards required under Basel II. Once we have data from the parallel run period, the agencies can then consider the need, if any, for a recalibration of the Basel II parameters or other actions to ensure more-accurate risk sensitivity and a prudent level of overall capital. This deliberate process provides multiple safeguards to help the agencies move to the final adoption of the new framework in the United States only when doing so is clearly appropriate. In other words, our implementation strategy has been designed to be both prudent and flexible enough to move banks from Basel I to Basel II as their own systems mature and they can provide reasonably accurate assessments of their credit and operational risks. The agencies' analysis of and reaction to QIS4 results show how those safeguards work: We saw results that gave us concern, and so we are investigating further before we go to the next stage. Additional, future safeguards--such as the NPR process and the minimum one-year parallel run and the minimum two-year transition period, with options to extend either--will also ensure ample opportunity to recalibrate or seek other adjustments if necessary. But, for now, we believe that after a certain point, further analysis of QIS4 is likely to reap little or no additional benefit. We should, of course, try to learn what we can from these data and particularly look for indications of the need to modify the Basel II proposal where necessary. However, as soon as we have learned what we can, we should promptly return to the development of the joint NPR and related supervisory guidance. These documents are essential so that core and opt-in banks can continue to develop the databases and systems that they would need to operate under the Basel II capital rules and that would provide more-accurate risk parameter estimates than those in QIS4. Recall that there have already been three rounds of U.S. public comments on the Basel II consultative papers between 1999 and 2004; an advance notice of proposed rulemaking (ANPR) in 2003; and numerous agency discussions with congressional committees, banking groups, and individual banks. All have resulted in significant modifications to the proposal. Once published, the NPR and the supervisory guidance will once more elicit comments that could result in further revisions. Particularly given its delayed issuance, the NPR must solicit feedback from core and potential opt-in banks as to whether the current timeline for implementing Basel II in this country needs to be delayed or can be retained. Looking forward, I agree with my colleagues at this table that it is prudent to delay the NPR in order to see what we can learn from further review of QIS4 data, recognizing at the outset that final answers will not be forthcoming because the requisite databases and risk-management systems are not yet in place. I hope that we can return to the NPR before midyear, present it to the Office of Management and Budget for its review, as our OCC and OTS colleagues must do, and release the NPR in the fall. With such a schedule, one might hope that the parallel running period, currently scheduled for 2007, need not be delayed. But, as I noted earlier, it is important for the agencies to get feedback on this issue during the NPR comment period. The views of banking organizations will provide critical insights into the feasibility of the scheduled 2008 start date for the transition run. Once we have the views of the banking organizations, the agencies will be in a better position to reach a consensus on the timeline. Basel II has the potential to be an important supervisory step forward. The Basel I framework is being arbitraged aggressively and provides us with less and less reliable measures on which to base a regulatory capital requirement for our largest and most complex banking organizations. Moreover, banks have spent tens of millions of dollars preparing for the U.S. implementation of Basel II and have contracts for further investment. They are awaiting the NPR, the guidance, and the final rules. Their global competitors are proceeding, and U.S. banks will be eager to avoid being placed at a competitive disadvantage. I might add that, as supervisors, we believe that the core risk-measurement and risk-management improvements contained in Basel II are appropriate, regardless of how the future accord is finally structured and calibrated. So it is, in our view, a good idea for banks to continue their current trajectory of making risk-management investments. While the regulatory capital requirements ultimately produced by Basel II would be, we believe, considerably more risk sensitive than the current capital regime, importantly this is not the only capital regulation under which U.S. institutions would operate. Over a decade ago, the Congress, as part of the Federal Deposit Insurance Corporation Improvement Act's Prompt Corrective Action (PCA) regime, defined a critically undercapitalized insured depository institution by reference to a minimum tangible-equity-to-asset requirement, a leverage ratio. The agencies have also used other leverage ratios to define other PCA capital categories because experience has suggested that there is no substitute for an adequate equity-to-asset ratio, especially for entities that face the moral hazard that accompanies the safety net. The Federal Deposit Insurance Corporation (FDIC), responsible to the Congress for the management of the critical deposit insurance portion of the safety net, has underlined the importance of that minimum leverage ratio and PCA as part of a prudent supervisory regime. The Federal Reserve concurs in the FDIC's view. We need, for reasons I have given, the risk-measurement and risk-management infrastructure and risk sensitivity of Basel II; but we also need the supplementary assurance of a minimum equity base. The market and the rating agencies will continue to require exactly that kind of base, and a regulatory minimum is prudentially desirable. Even though the market and the rating agencies, not to mention bank management, will still require banking organizations to carry capital considerably above regulatory minimums, many of the thousands of depository institutions that will remain under the current capital rules are concerned about the impact of Basel II on their businesses. This concern is often voiced as a general disquiet about broad competitive feedbacks but also about competitive implications in specific markets. The Federal Reserve has published a series of research papers investigating such concerns voiced in public comments on the previous ANPR on Basel II. These studies have indeed suggested that there are potential effects that should be addressed in the small business and residential mortgage markets. For this reason, as well as to continue to modernize the current capital regime, the agencies are developing, simultaneously with the Basel II proposal, a proposal to revise the current capital rules for non-Basel II banks to make those rules more risk sensitive and to blunt any unintended harm that Basel II might impose on non-adopters. Our intention is to keep these proposed changes simple to minimize any costs imposed on the many non-adopters. We plan to issue these proposals for public comment concurrently with or soon after the NPR on Basel II, to allow the banking community to comment on a combined package of proposed changes. However, these revised Basel I rules would not be an adequate substitute for the necessary capital reforms for the large, complex, global banks operating in this country because they would not provide the incentives for banks to adopt the more-sophisticated risk-measurement and risk-management techniques envisioned by the Basel II proposal. Chairman Bachus and Chairman Pryce, I would also like to present the Federal Reserve's views on H.R. 1226, a bill setting up a committee of the four banking and thrift regulators to reach a common U.S. position on Basel issues and authorizing the Secretary of the Treasury, as its chairman, to determine a common position on any issue about which the regulators could not agree. The Federal Reserve believes that the bill does not fully reflect the existing process used by the four agencies to develop and modify Basel II and we would counsel that Congress not enact it. Staff members of the four agencies have held frequent and comprehensive discussions about Basel II throughout the process. Certainly, the agencies have sometimes disagreed on specific issues, and we will sometimes disagree in the future. But we have in the past been able to find a common position that we can all support at Basel, and we will do so in the future. The salient fact is that any one of us has a veto over the entire proposal because we all realize that different rules cannot be applied to similarly situated insured depository institutions. That fact forces us to develop consensus positions on which all of us can agree. We have done so in the past because we understand that if the agencies cannot reach a collective agreement at Basel, the Basel II reforms will not be implemented in the United States while they go forward in the rest of the world. Communication, compromise, and comity are the prerequisites for agreement among the agencies. Further, the ability of the U.S. agencies to negotiate effectively at Basel would be severely constrained if our foreign counterparties knew that we had to return to a committee before we could agree. The formalized "decision by committee" approach of H.R. 1226 would not advance U.S. interests in the complex and dynamic Basel negotiation process. The U.S. banking agencies need to preserve our current flexibility to respond to Basel issues if we are to develop a set of capital rules that are useful and productive for U.S. banks and thrifts. Moreover, it is possible that the agencies are more likely to implement effectively an agreement that they helped shape than they would be one that was imposed on them and for which they did not understand fully the rationale. While we urge the Congress not to move forward on this bill, we look forward to keeping Congress fully informed as the Basel process continues. I will be pleased to answer your questions.
No content found
The Federal Reserve Board on Friday announced the issuance of a Final Decision and Order of Prohibition against Donald K. McKinney, a former Vice President of American National Bank, Wichita Falls, Texas. The Order, the result of an action brought by the Office of the Comptroller of the Currency, prohibits Mr. McKinney from participating in the conduct of the affairs of any financial institution or holding company. Attached are the Board's Final Decision and Order relating to this action.
Remarks by Chairman Alan Greenspan Commencement address At the Wharton School, University of Pennsylvania, Philadelphia, Pennsylvania May 15, 2005 Dean Harker, members of the faculty, Wharton alumni, friends and families and, especially, members of the 2005 graduating class. I have more in common with you graduates than people might think. After all, before long, after my term at the Federal Reserve comes to an end, I too will be looking for a job. I am delighted to join in celebrating your achievements and promise. You are being bequeathed the tools for creating a material existence that neither my generation nor any that preceded it could have even remotely imagined as we began our life's work. What you must fashion for yourselves are those values that will enable you to contribute and thrive in a world that is becoming increasingly competitive and frenetic. The creative abilities of this graduating class and those of your contemporaries will determine the magnitude and extent of American prosperity in this century. And the ideas and values that you employ in these creative endeavors will shape the future state of our cultural, legal, and economic institutions. You will doubtless foster advances in science, engineering, and business management. But scientific proficiency will not be enough. Technology is a tool that, unless guided by a set of ethical principles, is of qualified value. The principles governing business behavior are an essential support to voluntary exchange, the defining characteristic of free markets. Voluntary exchange, in turn, implies trust in the word of those with whom we do business. To be sure, all market economies require a rule of law to function--laws of contracts, rights to property, and a general protection of citizens from arbitrary actions of the state. Yet, if even a small fraction of legally binding transactions required adjudication, our court systems would be swamped into immobility, and a rule of law would be unenforceable. Of necessity, therefore, in virtually all our transactions, whether with customers or with colleagues, with friends or with strangers, we rely on the word of those with whom we do business. If we could not do so, goods and services could not be exchanged efficiently. Trillions of dollars of assets are priced and traded daily in our financial markets. Before recent technologies enabled transactions to clear and settle virtually in real time, most of the vast volumes of trades were not legally binding for days. Their validity rested on trust. Even today, much of business is transacted on parties' undocumented verbal agreements. We take this for granted and rarely pause to ponder how unusual this practice is. Moreover, even when followed to the letter, laws guide only a few of the day-to-day decisions required of business and financial managers. The rest are governed by whatever personal code of values market participants bring to the table. Trust as the necessary condition for commerce was particularly evident in freewheeling nineteenth-century America, where reputation became a valued asset. Throughout much of that century, laissez-faire reigned in the United States as elsewhere, and caveat emptor was the prevailing prescription for guarding against wide-open trading practices. In such an environment, a reputation for honest dealing, which many feared was in short supply, was particularly valued. Even those inclined to be less than scrupulous in their personal dealings had to adhere to a more ethical standard in their market transactions, or they risked being driven out of business. To be sure, the history of world business, then and now, is strewn with Fisks, Goulds, Ponzis and numerous others treading on, or over, the edge of legality. But, despite their prominence, they were a distinct minority. If the situation had been otherwise, late nineteenth- and early twentieth-century America would never have realized so high a standard of living. Indeed, we could not have achieved our current level of national productivity if ethical behavior had not been the norm or if corporate governance had been deeply flawed. * * * Over the past half-century, societies have chosen to embrace the protections of myriad government financial regulations and implied certifications of integrity as a supplement to, if not a substitute for, business reputation. Most observers believe that the world is better off as a consequence of these governmental protections. Accordingly, the market value of trust, so prominent in the 1800s, seemed by the 1990s to have become less necessary. But recent corporate scandals in the United States and elsewhere have clearly shown that the plethora of laws and regulations of the past century have not eliminated the less-savory side of human behavior. We should not be surprised then to see a re-emergence of the value placed by markets on trust and personal reputation in business practice. After the revelations of recent corporate malfeasance, the market punished the stock and bond prices of those corporations whose behaviors had cast doubt on the reliability of their reputations. There may be no better antidote for business and financial transgression. But in the wake of the scandals, the Congress clearly signaled that more was needed. The Sarbanes-Oxley Act of 2002 appropriately places the explicit responsibility for certification of the soundness of accounting and disclosure procedures on the chief executive officer, who holds most of the decisionmaking power in the modern corporation. Merely certifying that generally accepted accounting principles were being followed is no longer enough. Even full adherence to those principles, given some of the imaginative accounting of recent years, has proved inadequate. I am surprised that the Sarbanes-Oxley Act, so rapidly developed and enacted, has functioned as well as it has. It will doubtless be fine-tuned as experience with the act's details points the way. But the act importantly reinforced the principle that shareholders own our corporations and that corporate managers should be working on behalf of shareholders to allocate business resources to their optimum use. But as our economy has grown, and our business units have become ever larger, de facto shareholder control has diminished. Ownership has become more dispersed and few shareholders have sufficient stakes to individually influence the choice of boards of directors or chief executive officers. The vast majority of corporate share ownership is, of course, for investment, not to achieve operating control of a company. Thus, it has increasingly fallen to corporate officers, especially the chief executive officer, to guide the business, one hopes by what is perceived to be in the best interest of shareholders. To be sure, senior officers in today's corporations no longer have the dominance that they were able to achieve prior to the revolution in information technology. A decade ago, senior officers of a corporation could tightly control, if they chose, access to key information systems. Those senior officers could have far greater knowledge of the workings of their business than others and, as a consequence, were less subject to challenge when making day-by-day tactical and strategic decisions. Arguably, with information systems now accessible to broader ranges of managers and other employees, the monopoly power that proprietary information affords has been significantly reduced. Moreover, the availability of vital information now often extends beyond the borders of the company to suppliers and customers as well. A generation ago, for example, a purchasing manager rarely divulged to a supplier the state of the company's inventory position. It was presumed that such information in the hands of suppliers would undermine the bargaining position of the purchasing manager. Today such information is broadly and routinely shared to facilitate just-in-time supply systems. In general, technologies may be in the process of facilitating a much broader access to information, with the consequence that CEOs could increasingly face more-careful monitoring. It seems clear that, if the CEO chooses, he or she can, by example and through oversight, induce corporate colleagues and outside auditors to behave ethically. Companies run by people with high ethical standards arguably do not need detailed rules on how to act in the long-run interest of shareholders and, presumably, themselves. But, regrettably, human beings come as we are--some with enviable standards, and others who continually seek to cut corners. Rules exist to govern behavior, but rules cannot substitute for character. In the years going forward, it will be your reputation--for integrity, judgment, and other qualities of character--that will determine your success in life and in business. A generation from now, as you watch your children graduate, you will want to be able to say that whatever success you achieved was the result of honest and productive work, and that you dealt with people the way you would want them to deal with you. * * * I presume that I could offer all kinds of advice to today's graduates from my nearly six decades in private business and government. I could urge you all to work hard, save, and prosper. And I do. But transcending all else is being principled in how you go about doing those things. It is decidedly not true that "nice guys finish last," as that highly original American baseball philosopher Leo Durocher was once alleged to have said. I do not deny that many appear to have succeeded in a material way by cutting corners and manipulating associates, both in their professional and in their personal lives. But material success is possible in this world, and far more satisfying, when it comes without exploiting others. The true measure of a career is to be able to be content, even proud, that you succeeded through your own endeavors without leaving a trail of casualties in your wake. * * * Our system works fundamentally on trust and individual fair dealing. We need only look around today's world to realize how valuable these traits are and the consequences of their absence. While we have achieved much as a nation in this regard, more remains to be done. Prejudice of whatever stripe is unworthy of a society built on individual merit. A free market capitalist system cannot operate fully effectively unless all participants in the economy are given opportunities to achieve their best. If we succeed in opening up opportunities to everyone, our national affluence will almost surely become more widespread. Of even greater import is that all Americans recognize that they are part of a system that is fair and worthy of support. * * * Our forefathers bestowed upon us a system of government and a culture of enterprise that have propelled the United States to the greatest material prosperity the world has ever experienced. Today's Wharton graduates are being passed the standard to carry forward those traditions. I know you will improve upon this inheritance in ways that we have yet to imagine. I offer you all my congratulations and wish you success in your chosen careers.
Joint Press Release Board of Governors of the Federal Reserve System Federal Deposit Insurance Corporation National Credit Union Administration Office of the Comptroller of the Currency Office of Thrift Supervision For Immediate Release May 16, 2005 Media Contacts: Federal Reserve Susan Stawick (202) 452-2955 FDIC David Barr (202) 898-6992 OCC Kevin Mukri (202) 874-5770 OTS Erin Hickman (202) 906-6677 NCUA Cherie Umbel (703) 518-6330
Remarks by Governor Mark W. Olson At the Annual Washington Briefing Conference of the Financial Women's Association, Washington, D.C. May 16, 2005 Basel II Thank you for the opportunity to speak to you today about current status of the Basel II capital revisions. As most of you know, the U.S. banking agencies are working with their counterparts on the Basel Committee on Banking Supervision to develop and implement revisions to the original Basel Capital Accord--also known as Basel I--which was adopted in 1988. As we earnestly work to develop those revisions, it is instructive to recall the environment in which Basel I was created. If we look back at the state of the banking and financial markets in 1988, it is clear that Basel I was a major accomplishment. Many countries had distinct, and considerably different, capital adequacy requirements. Indeed, as banks conducted more and more business across borders, these differences raised significant competitive, if not safety-and-soundness, issues. It soon became clear that supervisors in the industrial countries would benefit from agreement on certain common definitions and minimum standards for regulatory capital. Thus was born the first Basel Capital Accord. Two factors made Basel I particularly noteworthy. First, it contained some common definitions of capital and risk-weighted assets that could be applied across countries. This was no small feat, since, as you know, different jurisdictions often apply a variety of definitions. By agreeing on common definitions, supervisors around the world were more readily able to depend on one another's assessment of capital adequacy without having to determine what the terms meant. Second, Basel I reflected agreement on what constituted a reasonable minimum ratio of capital to risk-weighted assets--the now-famous 8 percent. Although some empirical work went into the derivation of this 8 percent, we should not pretend that this ratio was determined with scientific precision. It did, however, reflect the combined experience and knowledge of all the supervisors around the table. The four risk-weighting brackets, while equally imprecise, were the first efforts to differentiate risk exposures among categories of loan and investment assets. Reasons for Developing Basel II In looking back, I believe Basel I, with its common definitions, common agreement on capital minimums, and the initial risk-weighting categories, has served the banking and financial community well. Indeed, there is no reason to replace Basel I for the vast majority of banks here in the United States. But our largest and most complex global banking organizations have, in a sense, outgrown Basel I. On the one hand, the need for Basel II reflects the increased sophistication of risk-management practices and the ways they can be applied to the measurement of capital. At the same time, it also reflects the increased complexity of banking in general, especially at larger institutions. Although many in this audience are very familiar with the provisions of Basel II, let me briefly review them for those who are not. In a general sense, Basel II represents an improved and broadly comparable way to look at risk taking across organizations and over time. It is composed of the now-familiar three pillars: Pillar 1, minimum capital requirements; Pillar 2, supervisory review; and Pillar 3, market discipline. The framework is structured to be much more risk-sensitive than its predecessor; for example, all commercial loans are not lumped into one risk bucket but are differentiated according to certain indicators of risk. Basel II is designed to address the concern that Basel I regulatory capital ratios are no longer good indicators of risk for our largest institutions. Indeed, at our largest institutions, calculating Basel I ratios is sometimes viewed as nothing more than a compliance exercise. The development of sophisticated secondary markets in recent years has allowed banks to make strategic decisions to either retain or sell virtually every category of loan and investment asset. This market advance has had significant implications for the initial Basel effort. Basel I presents an opportunity for banks to retain balance-sheet positions that are of higher risk than their regulatory capital charge and to shed those of lower risk. Using this type of capital arbitrage, banks can game the system in such a way that the resultant Basel I ratio does not have substantial meaning for the public, bank management, or the supervisor. Basel II is intended to close this gap by more directly linking riskiness of assets to their corresponding regulatory capital charge and to reduce, if not eliminate, the incentives to engage in capital arbitrage. Basel II also creates a link between regulatory capital and risk management, especially under the advanced approaches, which are the only ones expected to be applied in the United States. Under these approaches, banks will be required to adopt more-formal, quantitative risk-measurement and management procedures and processes. And, to implement this framework, both bank management and supervisors will need to focus on the integrity and soundness of these procedures and processes, including comprehensive assessments of capital adequacy in relation to the bank's overall risk taking. It is helpful to recall that the quantitative risk-measurement and management procedures and processes that will be required by Basel II are based on practices already in use at today's most sophisticated institutions. In other words, the practices in Basel II were not devised by regulators on their own. Admittedly, a certain degree of standardization in these practices was required because Basel II is a minimum regulatory capital framework intended to apply fairly consistently to a wide range of institutions. The new framework should improve supervisors' ability to understand and monitor the risk taking and capital adequacy of large complex banks, thereby allowing regulators to address emerging problems more proactively. The new framework should also enable supervisors to have much more informed and timely conversations with bank management about their risk profiles, based on the new information flows generated. Our hope is that conversations around this common analytical framework will create a common language for risk management. In the United States, we intend to use the framework to determine whether bankers are indeed able to monitor their own risk-taking and capital positions, and we will be placing the onus on bankers to show that they are able to measure, understand, and effectively manage their consolidated risks. It is important to remember that Pillar 1 is supposed to produce a minimum level of regulatory capital and that each institution's actual capital held will vary according to its own risk profile and business mix. Explicit assumptions are built into Pillar 1, such as the idea that portfolios are well diversified and do not contain geographic or sectoral concentrations. Supervisors must remind institutions that it is initially the bank's job to address any deviations from Pillar 1 assumptions, as well as any additional factors that affect the risk of the individual bank, and to adjust their minimum regulatory capital accordingly. Under Pillar 2, supervisory authorities, in turn, will review these adjustments by banks and could ask them to take additional steps to ensure that all risks have been addressed. We should also remember that beyond minimum regulatory capital requirements, Pillar 2 requires banks to develop a viable internal process for assessing capital adequacy that contributes to the determination of the amount of capital actually held. Banks should take this requirement seriously. Supervisors will carefully review banks' compliance with this requirement. Basel II also adds a new element of market discipline, as described in Pillar 3. In the case of our larger organizations, which have become so varied and complex, supervisors have the choice of either using more invasive procedures or relying on market discipline. And market discipline is impossible if counterparties (and rating agencies) do not have better information about banks' risk positions. Markets need accurate information to function effectively. The objective is not to supplant supervision of our larger organizations but to provide information that will enable market participants to serve as an effective complement to supervisors. I would also like to underscore that Basel II is indeed a seminal step in the development of regulatory capital requirements. It is not necessarily the endpoint, but it does represent a substantial step forward--one that we believe will remain in place for many years to come. Implementation Efforts in the United States I would now like to address some of the practical aspects of the implementation of Basel II in the United States. I assume that most of you either heard, or read about the testimony and discussion at last week's hearing before two subcommittees of the House Committee on Financial Services. I will not attempt to paraphrase my colleagues' statements, but I will point out that throughout this process the agencies have stressed the importance of comments from the Congress, the banking industry, and other interested parties. The agencies' reaction to the results of the fourth Quantitative Impact Study--known as QIS4--shows how seriously we are taking Basel II implementation. The interagency statement issued on April 29 indicated clearly, I believe, that results from QIS4 were more widely dispersed and showed a larger overall drop in capital than the agencies had expected. This was the impetus for deciding to delay the notice of proposed rulemaking for Basel II. We now have to determine whether these results arose from actual differences in risk among respondents, differences in stages of preparation (including data limitations) among respondents, limits of the QIS4 exercise, or a possible need for adjustments to the Basel framework itself. Staff at the agencies are working together to analyze the QIS4 results and in some cases are talking to banks about their submissions. Analyzing the data used in QIS4 is vitally important, because ultimately the success of Basel II will depend on the quantity and quality of data that banks have to use as inputs to the framework. From the Federal Reserve's perspective, delving into the results of QIS4 is an appropriate response by the agencies. But we should also acknowledge that this follow-up work has its limits. We can go only so far with the data given to us. We must recognize that banks--understandably--might not yet have their data systems ready to develop Basel II risk parameters and that it might take more time before we see Basel II parameters based on truly credible systems. But, in our view, that is not a reason to stop working. Indeed, one of the most important aspects of Basel II pertains to improved risk management, and having banks move forward in that area can only bring benefits. Of course, we support the established protocols and procedures for implementing domestic rules in the United States, which will include additional comment periods and opportunity for the banking industry, Congress, and others to express their views, and we are in favor of starting that part of the process as soon as possible. The information we now have points to the need to keep working; however, if new information indicates that changes need to be made or that additional pauses would be prudent, we will of course respond appropriately. Since the agencies remain committed to Basel II, we must give institutions as much information as possible to help them with their preparations. The agencies have sought to provide helpful information to institutions as soon as it becomes available--for example the draft supervisory guidance documents that are now under development. So far, the agencies have issued draft guidance for the advanced measurement approaches for operational risk and certain parts of the internal ratings-based approach for credit risk. Additional draft guidance is expected to be issued for public comment either along with or soon after the notice of proposed rulemaking is released. From the beginning, we intended this guidance to further clarify supervisory expectations for implementation of Basel II in the United States, and it is directed at bankers as well as at supervisors. We believe that by outlining what supervisors would expect, the proposed guidance gives banks a far better understanding of how to upgrade their systems, modify their procedures, and strengthen their controls in anticipation of eventual adoption of Basel II. Our hope is that, by clearly communicating expectations, we are giving both bankers and our own examiners sufficient time to prepare for the new framework. One vital element of our preparation for implementation has been our dialogue with the banking industry. At many stages along the way, banking organizations--both internationally and domestically--have expressed their concerns about certain aspects of Basel II. When credible evidence and compelling arguments have shown that those concerns are well founded, the agencies and the Basel Committee have modified the proposal. For example, global regulators heard the industry's call for addressing only unexpected loss in the framework, and the approach to securitization was substantially altered on the basis of comments received. Supervisors and the industry need to maintain an ongoing dialogue about Basel II, since it is such a complex and multifaceted project. Issues range from very technical questions about parameter values to broad concerns about how the framework will be implemented across countries. We hope it is clear that we are being attentive to the full range of these concerns and will continue to be as the industry raises additional concerns along the way. The continued emphasis on dialogue is even more important because a few important elements of the framework are still considered works in process. For example, the Basel framework issued last June did not define the exact manner in which institutions should calculate their estimates of loss given default (LGD) based on periods of economic downturn. The Basel member countries have commissioned a subgroup to study this issue, recommend a way to add clarity to the concept of downturn or "stress" LGDs, and provide guidance to the industry. The Basel Committee has also recently issued a consultative paper on certain trading-related exposures and double-default effects, stemming from the work of a joint group formed by the Basel Committee on Banking Supervision and the International Organization of Securities Commissions. We are acutely aware that our continuing work on these issues is complicating banks' preparations somewhat. But since these issues are so critical, we have to take the extra time to find the right solutions. And we need the help of the industry to do so. Competitive Effects of Basel II Before I close, I would like to say a few words about the potential competitive effects of Basel II. At the Federal Reserve, we are particularly interested in effects that Basel II could have on banking markets, particularly ones that could distort existing markets that work well. In that vein, we have published several white papers analyzing the potential impact on specific aspects of banking, such as small-business lending, mortgage lending, and mergers and acquisitions. A paper on credit cards is forthcoming. While the conclusions of the papers published so far do not point to broad disruptions in existing banking markets as a result of Basel II, we do acknowledge that certain participants could be affected, especially in the small-business and residential-mortgage credit markets. In part to address these concerns, but also to conduct overdue routine updates to existing regulatory capital rules, the agencies also plan to propose several modifications to the current rules that most banks in the United States will continue to follow, since Basel II is expected to apply to only a handful of institutions. The agencies understand that outside parties will likely want to see the notice of proposed rulemaking for Basel II and the relevant proposals for amending current rules alongside one another for comparison's sake. Conclusion Broadly speaking, in developing Basel II we are striving to establish higher standards for internal risk management at banking organizations, including capital adequacy, and to improve both the supervisors' and the public's understanding of banks' risk taking and risk management. Over the past two decades, major banking organizations have become ever larger and more complex, while some national financial systems have become more concentrated. Against this backdrop, assessing the overall risk and capital adequacy of the largest banks has become not only increasingly difficult for supervisors, the public, and bank management, but also critical for national authorities. If you believe, as I do, that Basel I was one of the most important advances in international bank supervision, then I hope you also accept that market changes and increased sophistication in risk-management techniques require that we now update that initial framework. A fundamental premise of Basel II is that, for these major banks, neither supervisory nor market discipline can be effective unless banks' own systems can be depended on to measure and manage risk taking and capital adequacy. Basel II is intended to provide both a framework and incentives for achieving these ends. It is useful to keep in mind that as supervisors we are seeking to apply a new set of rules to private, profit-seeking businesses and are trying to achieve the goals of Basel II in a manner consistent with achieving safety and soundness in a market economy. For example, we want to be relatively certain that the required capital levels for business lines (and, of course, for the bank as a whole) are in line with the underlying risks, and we need to have a certain degree of confidence in the capital numbers for each bank. We also want banks to target their investments where they can have the greatest positive impact on risk measurement and management. In other words, we are not looking for large expenditures in areas for which the benefits will not be material. Finally, we would like to see some comparability across banks, at least in their estimation of the inputs for Basel II, but at the same time offer a certain degree of flexibility to reduce burden on the banks and allow them to retain their own styles of risk management. Admittedly, navigating between consistency and flexibility is an art, but our experience tells us that it is achievable when all parties work together.
Testimony of Governor Edward M. Gramlich Regulation Z (Truth in Lending Act) Before the Committee on Banking, Housing, and Urban Affairs, U.S. Senate May 17, 2005 Chairman Shelby, Senator Sarbanes, and members of the Committee, I appreciate the opportunity to appear today to discuss consumer credit card accounts. The Board of Governors of the Federal Reserve System administers the Truth in Lending Act (TILA). Enacted in 1968, TILA is the primary federal law governing disclosures for consumer credit, including credit card accounts. It is implemented in the Board's Regulation Z. TILA has distinct rules for two categories of consumer credit: open-end (revolving) credit plans, such as credit card accounts and other lines of credit; and closed-end (installment) transactions, such as auto loans and home-purchase loans. Amendments targeting specific loan products or practices have been added over TILA's nearly forty-year history and the act was substantially revised by the Truth in Lending Simplification and Reform Act of 1980. TILA's purpose is to assure a meaningful disclosure of credit terms so that consumers can compare more readily the various credit terms available and avoid the uninformed use of credit. TILA fulfills this purpose by requiring the uniform disclosure of costs and other terms to consumers. TILA is also intended to protect consumers against inaccurate and unfair credit billing and credit card practices, which the act seeks to accomplish through procedural and substantive protections, including special rules for cardholders. Regulation Z review. Regulation Z and its staff commentary have been reviewed and updated almost continuously, but not comprehensively since 1980. In December 2004, the Board began a comprehensive review of Regulation Z, starting with the publication of an advance notice of proposed rulemaking (ANPR) on the rules for open-end credit that is not home-secured, such as general-purpose credit cards. The goal of the review is to improve the effectiveness and usefulness of open-end disclosures and substantive protections. The public comment period recently closed, and the Board's staff will be carefully reviewing the comment letters as they consider possible changes to the regulations. We also believe that consumer testing should be used to test the effectiveness of any proposed revisions, and anticipate publishing proposed revisions to Regulation Z in 2006. We recognize the hard work that is ahead. The landscape of credit card lending has changed since TILA's disclosure rules for credit card accounts were first put in place. Products and pricing are complex. Credit card accounts can be used for purchases, cash advances, and balance transfers, and each means of access may carry different rates. Promotional rates and deferred interest plans for limited time periods are commonly layered onto these basic features. However, under some credit card agreements, paying late or exceeding a credit limit may trigger significant fees and a penalty rate that is applied to the entire outstanding balance, and may trigger higher rates on other credit card accounts. Moreover, the amount of consumers' payments, how creditors allocate those payments to outstanding balances, and how the balances are calculated all affect consumers' overall cost of credit under open-end plans. The question is, of course, how might the Board revise its rules under TILA in a way that will enable consumers to more effectively use disclosures about the key financial elements of a particular credit card over the life of the account? Simplifying the content of disclosures may be one way; finding ways to enhance consumers' ability to notice and understand disclosures may be another. Reviewing the adequacy of TILA's substantive protections is a third, and the ANPR asks questions about each of these areas. As the Regulation Z review proceeds, the Board will be grappling with the challenge of issuing clear and simple rules for creditors that both provide consumers with key information about complicated products (while avoiding so-called "information overload") and provide consumers adequate substantive protections, consistent with TILA. For example, TILA contains procedures for resolving billing errors on open-end accounts, prohibits the unsolicited issuance of credit cards, and limits consumers' liability when a credit card is lost or stolen. To assist the Committee in its deliberations, I will provide an overview of TILA's rules affecting open-end credit plans, focusing on rules for credit card accounts. I will discuss some of the major issues raised in the ANPR, and commenters' views on these issues. I will also address compliance and enforcement issues, along with the role of consumer education in improving consumers' informed use of credit. Disclosures for Open-end Credit Plans TILA disclosures for open-end plans are provided to consumers: On or with credit card applications and solicitations, such as applications sent by direct mail. At account opening. Throughout the account relationship, such as on periodic statements of account activity and when the account terms change. Content. Generally, the disclosures provided with credit card applications, at account-opening and on periodic statements, address the same aspects of the plan; that is, in each case consumers receive information about rates, fees, and grace periods to pay balances without incurring finance charges. The level of detail differs, however. Disclosures received with a direct-mail credit card account application are intended to provide a snapshot to help the consumer decide whether or not to apply for the credit card account. For example, revolving open-end accounts involve calculating a balance against which a rate is applied. The method for calculating that balance may differ from creditor to creditor, however. Under TILA, identifying a balance calculation method by title, such as the "average daily balance method (including new purchases)," is sufficient at application. Account-opening disclosures are more detailed and complex, however, in part because the account-opening disclosures required under TILA are typically incorporated into the account agreement. The periodic statement discloses information specific to the statement cycle. In the case of balance calculation methods, the disclosure is typically identical to the account-opening disclosure. Creditors must also tell consumers about their rights and responsibilities under the Fair Credit Billing Act, a 1974 amendment to TILA that I will discuss later, which governs the process for resolving billing disputes. In addition to explaining these rights in the account-opening disclosures, creditors must send reminders throughout the account relationship. Under TILA, a detailed explanation must be sent about once a year; typically, however, creditors instead send an abbreviated reminder on the reverse side of each periodic statement, as permitted by Regulation Z. Format. Generally, disclosures must be in writing and presented in a "clear and conspicuous" manner. For credit card application disclosures, the "clear and conspicuous" standard is interpreted to mean that application disclosures must be "readily noticeable." Disclosures that are printed in a twelve-point type size have a safe harbor in the regulation under this standard. Disclosures for direct-mail credit card account applications have the most regimented format requirements. The disclosures must be presented in a table with headings substantially similar to those published in the Board's model forms. Regulation Z's sole type-size requirement also applies to direct-mail application disclosures; the annual percentage rate for purchases must be in at least eighteen-point type size. Format requirements for credit card account applications available to the general public ("take-one's") are quite flexible. At the card issuer's option, take-one disclosures may be in the form required for direct-mail applications, an abbreviated narrative, or a simple statement that costs are involved that provides information about where details can be obtained. Compared to application disclosures, account-opening and periodic statement disclosures are governed by few specific format requirements. Except in the context of recently enacted amendments to TILA contained in the Bankruptcy Abuse Prevention and Consumer Protection Act of 2005 ("2005 Bankruptcy Act"), disclosures need not be presented in any particular order, nor is there any detailed guidance on the "clear and conspicuous" standard other than a requirement that the terms "finance charge" and "annual percentage rate" must be more conspicuous than any other term. The 2005 Bankruptcy Act contains several amendments to TILA, three of which are particularly relevant here. The act generally requires creditors to provide on the front page of periodic statements a warning about the effects of making minimum payments and a standardized example of the time it would take to pay off an assumed balance if the consumer makes only the minimum payment, along with a toll-free telephone number that consumers can use to obtain estimates of how long it would take to pay off their actual account balance. In addition, the act provides that if a temporary rate is offered on solicitations and applications, or promotional materials that accompany them, the term "introductory" must be "immediately proximate" to each listing of the temporary rate. The expiration date and the rate that will apply when the introductory rate expires must be "closely proximate" to the first listing of the introductory rate in promotional materials. Under the act, the Board must issue guidance regarding a "clear and conspicuous" standard applicable only to these minimum payment and introductory rate disclosures, including model disclosures. The Board has published model forms and clauses to ease compliance for many of TILA's disclosure requirements. Creditors are not required to use these forms or clauses, but creditors that use them properly are deemed to be in compliance with the regulation regarding these disclosures. The Board has published model forms for direct-mail credit card account application disclosures, but there are no model forms illustrating account-opening or periodic statement disclosures. Regulation Z review. Considering how consumers' use of open-end credit, and credit cards in particular, has grown, and the increased diversity in credit products and pricing, the Board's ANPR asked a number of detailed questions about how to improve the effectiveness and usefulness of TILA's open-end disclosures, including how to address concerns about "information overload." The Board also invited comment on how the format of disclosures might be improved, and whether additional model disclosures would be helpful. The Board announced its intent to use focus groups and other research to test the effectiveness of any new disclosures. In general, commenters representing both consumers and industry believe that the regimented format requirements for TILA's credit card account application disclosures have proven useful to consumers, although a variety of suggestions were made to add or delete specific disclosure requirements. Many, however, noted that typical account-opening disclosures are lengthy and complex, and suggested that the effectiveness of account-opening disclosures could be improved if key terms were summarized in a standardized format, perhaps in the same format as TILA's direct-mail application disclosure. These suggestions are consistent with the views of some members of the Board's Consumer Advisory Council, who advise the Board on consumer financial services matters. Industry commenters support the Board's intention to use focus groups or other consumer research tools to test the effectiveness of any proposed revisions. To combat "information overload," many commenters asked the Board to emphasize only the most important information that consumers need at the time the disclosure is given. They asked the Board to avoid rules that require the repetitive delivery of complex information, not all of which is essential to comparison shopping for credit cards, such as a lengthy explanation of the creditor's method of calculating balances that is now required at account-opening and on periodic statements. Commenters suggested that the Board would more effectively promote comparison shopping by focusing on essential terms in a simplified way. They believe some information could also be provided to consumers through educational, non-regulatory methods. Taken together, this approach could lead to simpler disclosures that consumers might be more inclined to read and understand. Truth in Lending's Cost Disclosures for Open-end Credit Plans As I have indicated, TILA is designed to provide consumers with information about the costs and terms of a particular form of credit, to enable consumers to make comparisons among creditors or different credit programs, or to determine whether they should obtain credit at all. Finance charges and other charges. Creditors offering open-end credit must disclose fees that are "finance charges" and "other charges" that are part of the credit plan. A "finance charge" is broadly defined as any charge payable directly or indirectly by the consumer and imposed directly or indirectly by the creditor, as an incident to or a condition of the extension of credit. Interest, cash advance fees, and balance transfer fees are examples of finance charges. Fees that are not incident to the extension of credit, but are significant charges imposed as part of an open-end plan must also be disclosed as "other charges." Late payment fees, application fees, and recurring periodic membership fees that are payable whether or not the consumer uses the credit plan ("annual fees") are examples of other charges. Annual percentage rate. Under TILA, the finance charge is also expressed as an annualized rate, called the Annual Percentage Rate, or APR. Interestingly, within the Truth in Lending structure, the term represents three distinct calculations, one under TILA's rules for closed-end credit and two under the rules for open-end credit. For closed-end (installment) credit, the APR includes interest and finance charges other than interest, such as points or origination fees on mortgage loans. Thus, the APR on closed-end transactions can be somewhat higher than the interest rate identified in the loan agreement, whenever other fees are present in the finance charge. APRs for open-end credit are calculated differently. Interest is the only component of the APR that can be disclosed on credit card solicitations and applications, account-opening disclosures, and advertisements for open-end plans. This is because the actual cost of credit to the consumer is unknown when these disclosures are provided, since the amount and timing of advances and the imposition of fees generally are in the consumer's control. Periodic statements must also disclose an "effective" or "historic" APR that reflects interest as well as finance charges other than interest, such as a cash advance fee, that were imposed during the past billing cycle. Because non-interest finance charges are amortized over one billing cycle for purposes of disclosing the effective APR, such fees can result in a high, double-digit (or sometimes triple-digit) effective APR on periodic statements. To avoid a skewed APR that could possibly mislead consumers, non-recurring loan fees, points, or similar finance charges related to the opening, renewing, or continuing of an open-end account are currently excluded from the effective APR that is disclosed for a particular billing cycle, under Regulation Z and the Board's official staff commentary. Regulation Z Review. A major focus of the Board's Regulation Z review is how to disclose more effectively the cost of open-end credit. For the industry as a whole, the types of fees charged on open-end consumer credit accounts have grown in number and variety. To the extent these fees are not specifically addressed in TILA or Regulation Z, creditors are sometimes unsure whether the fee should be disclosed under TILA as a "finance charge" or an "other charge," or not disclosed under TILA at all. The Board asked for comment in the ANPR on how to provide more certainty in classifying fees, and whether consumers would benefit from other disclosures that address the cost of credit, such as how creditors allocate payments. Commenters provided a variety of views. Some suggested that creditors should disclose only interest as the "finance charge" and simply identify all other fees and charges. Others suggested that all fees associated with an open-end plan should be disclosed as the "finance charge." Above all, to mitigate the risks and potential liability for non-compliance, creditors seek clear rules that allow them to classify, with confidence, fees as a "finance charge" or an "other charge" under TILA, or as fees that are disclosed pursuant to the credit agreement or state law. Under the statute, a creditor's failure to comply with TILA could trigger a private right of action by consumers and administrative sanctions by the federal agency designated in TILA to enforce its provisions with regard to that creditor. One of the Board's most difficult challenges in the Regulation Z review is to address the adequacy of periodic statement APRs. TILA mandates the disclosure of the effective APR on periodic statements, but its utility has been controversial. Consumer advocates believe it is a key disclosure that is helpful, and provides "shock value" to consumers when fees cause the APR to spike for the billing cycle. Commenters representing industry argued that the effective APR is not meaningful, confuses consumers, and is difficult to explain. They said the disclosure distorts the true cost of credit because fees are amortized over one billing cycle--typically thirty days--when the credit may be repaid over several months. Several commenters urged the Board to include in the effective APR calculation only charges that are based on the amount and duration of credit (interest). In response to the Board's ANPR, some commenters believe the effective APR might be more effectively understood if a disclosure on the periodic statement provided additional context. Comments received on the merits of requiring creditors to disclose payment allocation methods illustrate the competing interests in improving the overall effectiveness of cost disclosures. Some commenters believe any additional disclosure about payment allocation methods would be excessive and that many card issuers already make such disclosures. Others believe such a disclosure could be helpful to consumers but worry that descriptions might be overly detailed; some asked the Board to publish model disclosures to ensure clarity and uniformity. Rate increases. Credit card account agreements typically allow card issuers to change interest rates and other fees during the life of the account. Agreements spell out with specificity some potential changes, such as that the rate will increase if the consumer pays late. Credit card agreements also more generally reserve the right to increase rates, fees, or other terms. The statute does not address changes in terms to open-end plans. Regulation Z, however, requires additional disclosures for some changes. The general rule is that fifteen days' advance notice is required to increase the interest rate (or other finance charge) or an annual fee. However, advance notice is not required in all cases. A notice is required, but not in advance, if the interest rate increases due to a consumer's default or delinquency. And if the creditor specifies in advance the circumstances under which an increase to the finance charge or an annual fee will occur, no change-in-terms notice is required when those circumstances are met before the change is made. This is the case, for example, when the agreement specifies that the interest rate will increase if the consumer pays late. Under Regulation Z, because the card issuer has specified when rates will increase in the account agreement, the creditor need not provide advance notice of the rate increase; the new rate will appear on the periodic statement for the cycle in which the increase occurs. Regulation Z review. The ANPR asked how consumers were informed about rate increases or other changed terms to credit card accounts, and whether the current rules were adequate to allow consumers to make timely decisions about how to manage their accounts. Comments were sharply divided on this issue. Some consumers believe there is not enough advance notice for changes in terms, and believe a much longer time period is needed to find alternative credit sources. Creditors generally believe the current rules are adequate. The fifteen days' advance notice is sufficient, they stated, because change-in-term notices are typically sent with periodic statements, which means as a practical matter consumers receive about a month's notice before the new term becomes effective. Creditors noted that many states require at least thirty days' advance notice and allow consumers to "opt-out" of the new terms by closing the account and paying the outstanding balance under the former terms. For rate (and other) changes not involving a consumer's default, a number of creditors support a thirty-day notice rule and a few support a consumer "opt-out" right under Regulation Z. Where triggering events are set forth in the account agreement, creditors believe there is no need to provide additional notice when the event occurs; they are not changing a term, they stated, but merely enforcing the agreement. Some suggested this is a case where consumer education is the best solution, and that perhaps Board-published model forms would result in uniformity and greater consumer understanding. Consumers and consumer groups agreed that change-in-term policies should be more prominently displayed, including in the credit card application disclosures. Procedures and Substantive Protections TILA and Regulation Z provide protections to consumers when a lost or stolen credit card is used ("unauthorized use"), when the consumer believes a charge on a billing statement is in error ("billing error"), and when a purchase is made with a credit card and the consumer cannot resolve with the merchant honoring the card a dispute about the quality of goods or services ("claim or defense"). The Fair Credit Billing Act was enacted, in part, to provide a procedure for resolving disputes between cardholders and merchants who honor credit cards, and to allocate to card issuers some responsibility for providing relief to the consumer if the merchant fails to accommodate the cardholder. In general, these protections allow the consumer to avoid paying the disputed amount while the card issuer investigates the matter. The card issuer cannot assess any finance charge on the disputed amount or report the amount as delinquent until the investigation is completed. Depending on the facts, a dispute could trigger one or more of the protections discussed below. The applicability of a protection can hinge on timing (when the cardholder notifies the card issuer about the problem), the outstanding balance (how much of the sale price remains unpaid at the time the cardholder notifies the card issuer), and receipt of the good or services (nothing was delivered, or something was delivered but didn't meet the cardholder's expectations). Unauthorized use of a credit card. A cardholder cannot be held liable for more than $50 for the unauthorized use of a card. State law or other applicable law determines whether the cardholder "authorized" the use of the card. There are no specific timing or procedural requirements to trigger this protection (other than notifying the card issuer). An unauthorized charge may also be raised as a billing error or a claim or defense. Billing error. The billing error provisions contain the strictest timing and procedural requirements of TILA's substantive protections for open-end plans. For example, the consumer's claim must be in writing and sent to the address specifically designated for this purpose. The consumer triggers the billing error rules by notifying the creditor about the dispute. The notice must be received, and creditors must respond, within a set time period. If asserted in a timely manner, a billing error can be asserted even if the consumer previously paid the charge in full. Claim or defense for a credit card purchase. Cardholders may assert against the card issuer any claim or defense they could assert against the merchant. Cardholders trigger the rule by notifying the card issuer that they have been unable to resolve a dispute with a merchant about a sales transaction where a credit card was used. There is no specific time period within which the cardholder must give notice or the card issuer must respond. However, the cardholder must try to resolve the matter with the merchant before involving the card issuer. Unlike the billing error provision, this remedy is available only if the cardholder has an unpaid balance on the disputed purchase at the time notice is given. Under TILA, the claim or defense remedy cannot be used to assert tort claims (for example, product liability) against the card issuer. Also, the remedy is available only for sales exceeding $50 and for sales that occur in the state the cardholder has designated as his address or within 100 miles of that address. Unsolicited issuance. Credit cards may be issued to consumers only upon request. Nevertheless, credit cards may be issued to cardholders in renewal of, or substitution for, a previously accepted card (including supplemental cards for the existing account). Regulation Z Review . The Board's ANPR asked whether there was a need to revise the regulations' provisions implementing TILA's substantive protections, for example, whether the rules need to be updated to address particular types of accounts or practices or to address technological changes. To illustrate, TILA requires creditors to credit payments on open-end plans on the day the payment is received. Regulation Z permits creditors to set reasonable cut-off hours, which must be disclosed to consumers. The ANPR solicited comment on payment process systems, where mail delivery and electronic payments may be continuous twenty-four hours a day, seven days a week, and whether further guidance was needed on what constitutes a "reasonable" cut-off hour. Most industry commenters stated that cut-off hours vary among creditors due to a number of internal and external factors, and asked that creditors' flexibility in processing payments be maintained. The Board also received suggestions for standardizing cut-off hours in ranges, such as between 3 p.m. and 5 p.m. for mail delivery and 6 p.m. and 8 p.m. for electronic payments. Consumers and some consumer groups suggested that payments be credited as of the date payments are received regardless of the time. They asked the Board to consider rules that would provide greater certainty to consumers with regard to determining when the payment is received, because creditors more frequently than in the past exercise their right under the account agreement to impose late fees when a payment is not received by the due date. Moreover, consumer groups stated, many credit card agreements allow creditors to increase rates when the creditor learns the cardholder was late on another account even if the cardholder makes timely payments to the creditor. Supervision and Enforcement As part of the bank supervision process, the Federal Reserve enforces safe and sound banking practices and compliance with federal banking laws, including the Truth in Lending rules, with respect to the approximately 915 state-chartered banks that are members of the Federal Reserve System. Other regulators enforce these rules with respect to other institutions. For the vast majority of state member banks, credit card lending is not a significant activity. In fact, of the banks supervised by the Federal Reserve, the issuance of credit cards is the principal business activity of only two of these banks. In January 2003, the Federal Reserve, along with the Office of the Comptroller of the Currency, the Federal Deposit Insurance Corporation, and the Office of Thrift Supervision, issued interagency guidance on credit card account management practices. Federal Reserve supervisory staff have applied the principles of this guidance through constructive discussions with bank management about individual institutions' portfolio management practices. In the limited instances where formal or informal enforcement actions have proven necessary to ensure sound management of an institution's credit card portfolio, the Federal Reserve has appropriately exercised this authority. The Board also investigates consumer complaints against state member banks and forwards complaints it receives involving other creditors to the appropriate enforcement agencies. In 2004, the Board received approximately 5,100 consumer complaints. Of this number, approximately 2,300, or 45 percent, were against state member banks, while about 2,800, or 55 percent, were against other creditors not under the Board's supervisory authority and were forwarded to the appropriate agencies. About 39 percent of the 2,300 complaints against state member banks processed by the Board were complaints about credit cards. The data show that complainants' main concerns were about interest rates and terms, penalty charges and fees such as late fees, over-the-limit fees, and annual fees. In addition, consumers were concerned that their credit information was incorrectly reported to consumer reporting agencies. By way of comparison, industry estimates suggest there are more than 600 million credit cards in consumers' hands and annual domestic transactions involving credit cards exceed $1 trillion. Role for Consumer Financial Education This detailed description of the issues of concern in our review of Regulation Z is illustrative of both the complexity of and the growth in today's consumer credit markets. Technology has significantly changed consumers' payment options, with the credit card becoming an accepted payment medium for virtually any consumer good or service. In addition, credit scoring models, the mathematical formulations lenders use to predict credit risk, have enabled creditors to price credit more efficiently, and charge rates of interest commensurate with a consumer's repayment risk. This technology has contributed to the expansion of the subprime market, which has significantly increased access to credit for consumers who, more than likely, would have been denied credit in the past. As a result, concerns surrounding consumer protection relate as much to issues of fair pricing practices as they do to fair access to credit. In addition, as the industry has become more competitive on interest rate pricing, it has adopted more complex fee structures that, if triggered, affect a consumer's overall cost associated with the credit card. The use of disclosure rules as a consumer protection strategy is predicated on the assumption that consumers have an understanding of consumer credit and personal financial management principles. By dictating disclosure requirements, regulators and lawmakers rely on consumers to be familiar with basic financial principles and to be able to evaluate personal financial scenarios and options, once they have access to pertinent financial information. Indeed, this is the fundamental premise of our free market system, in which information increases market efficiency. In recent years, however, there has been an increase in concern that consumers' level of financial literacy has not kept pace with the increasingly complex consumer financial marketplace and the expansion of financial service providers and products. Lenders, regulators, and consumer and community advocacy groups have agreed that there is an increased need for consumer financial education, and have pointed to a variety of factors, including record personal bankruptcy filings, high consumer debt levels, and low personal savings rates, to support this assertion. Financial education could encourage consumers to focus on their credit contracts in addition to the TILA disclosures, which highlight the key terms of the contract. Toward this end, many public and private initiatives have been undertaken at both the local and national level to highlight the importance of financial education. As you know, Congress has established the Financial Literacy and Education Commission and the Financial and Economic Literacy Caucus--further demonstration of the degree of interest and concern in helping consumers obtain the knowledge they need to effectively manage their personal finances. The Federal Reserve System has also been active in promoting consumer financial education, and is an active participant in initiatives to further policy, research, and collaboration in this area. In closing, I would like to note that disclosure and financial education work in tandem in the interest of consumer protection, and I believe that it is important to continue to focus our collective attention on both fronts. Footnotes The Board's of proposed rulemaking for Regulation Z.
Remarks by Governor Susan Schmidt Bies At the Central Bank of the Republic of Turkeys International Conference on Financial Stability and Implications of Basel II, Istanbul, Turkey May 17, 2005 Financial Stability Benefits and Implementation Challenges of Basel II I want to thank the Governor and the Central Bank of Turkey for the invitation to speak at this prestigious conference. The sharing of ideas among policymakers, academics, and bankers at venues such as this benefits all involved and, I believe, helps us assess important issues relating to the strength and stability of banking and financial markets. I hope that my remarks today will contribute to that overall objective. This conference on financial stability and implications of Basel II is certainly timely. As you know, members of the Basel Committee on Banking Supervision are working diligently to implement the framework issued last June. At the same time, we are all dedicated to maintaining financial stability in our respective jurisdictions, and in global banking and financial markets as a whole. In this light, Basel II should not be seen as an end in itself, but a means to promote broad stability and enhance safety and soundness of financial institutions. Today I want to address three issues. First, I will describe the challenges facing bank regulators as they strive to improve financial stability. Then I will briefly describe some of the Basel II issues in the United States that were covered in the recent interagency press release and in last week's congressional hearing. Finally, I want to describe the challenges bank supervisors face in effectively implementing Basel II. Financial Stability As a central banker, I realize how vital it is to have a strong, stable financial system to support effective monetary policy. Excessive volatility in financial markets can significantly raise the cost of capital for business investment and adversely affect real economic expansion. History has demonstrated that a weak financial sector can significantly impede the monetary transmission mechanism when the central bank is trying to stimulate the economy. Since banks are the core of the financial system, efforts to improve their risk management can help mitigate the impact of shocks on financial markets and real economic performance. With effective risk management, banks are better able to plan alternatives to mitigate risks when they exceed predetermined risk exposure levels. It is important to emphasize that the normal fluctuations in asset prices that result from dynamic demand and supply conditions, and even some increase in uncertainty, do not usually generate financial instability. Put differently, financial stability implies that key institutions in the financial system are operating without significant difficulty and markets are generally functioning well. Bankers implicitly accept risk as a consequence of providing services to customers and also take explicit risk positions that offer profitable returns relative to their risk appetites. The job of bank supervisors is to ensure that bank capital represents an adequate cushion against losses, especially during times of financial instability or stress. Basel II is yet another step to minimize the negative consequences of risk-taking by financial institutions, particularly those institutions that could contribute to financial instability. This is reflected in the use of unexpected loss to calibrate capital. The assumption is that normal volatility should be covered by normal operating earnings. For losses beyond the normal range of expectations, capital should be in place to absorb the loss and leave the financial institution stable and able to continue operating effectively. Thus, financial institutions with weaker profit margins, or with customers with more varied ability to meet their obligations, should have more capital. It is important here to distinguish between higher expected losses, for which bankers raise prices to cover risk, and greater volatility of results, which requires additional capital. Greater sensitivity of regulatory capital to risk has taken on increased significance as virtually all banking markets have become considerably more concentrated, with some companies--by their very size alone--posing the potential for systemic risk. Also, the advanced approaches of Basel II better align regulatory capital to the risks presented by sophisticated financial instruments and to the complexity of large, internationally active financial institutions. The current Basel I framework is more focused on credit risk for balance sheet assets. But sophisticated financial institutions carry fewer of their potential exposures on their books. Rather, after credit- and market-risk mitigation, it is often the process of managing risks or laying off exposures that has created earnings surprises in recent years. Basel II is intended to mitigate potential disruptions in banking markets by improving risk measurement and management; establishing a better link between risk and minimum capital ratios; and providing more information to bankers, supervisors, and other market participants. But we should also remember that the increased sensitivity to risk in Basel II carries with it the possibility that minimum capital ratios could actually be more volatile than they are today. As my colleague Bill Rutledge pointed out yesterday, that is what we expect, since those ratios will be more responsive to changes in risk. The Basel Committee has attempted to reduce procyclicality effects in the new framework, incorporating factors such as estimates of loss severities that focus on downturns. These are wise decisions intended to obviate the need for institutions to raise large amounts of capital at the trough of a downturn--something that can be quite difficult and add to financial market instability. But I think we could all agree that Basel II should not be unresponsive to changes in risk, for example when the obligor rating distribution at an institution shifts to poorer-quality borrowers. In my view, we want these signals of changes in risk reflected in regulatory capital levels. But by being careful about the extent that capital levels respond to cyclicality, we are trying to make sure that risk signals do not on their own generate added instability. This requires some balancing. Greater responsiveness of regulatory capital ratios to risk is something that institutions will have to learn to manage under Basel II. Given the potential for increased volatility in their capital ratios, I expect that institutions operating under Basel II will maintain a certain cushion above their minimum ratios since they must have the capital in place before the date of measurement of risk. Indeed, Pillar 2 of the Basel framework (supervisory review) requires banks to develop a viable internal process for assessing capital adequacy that helps determine the amount of capital actually needed for their particular business mixes and risk profiles. Explicit assumptions are built into Pillar 1 (minimum capital requirements), such as the idea that portfolios are well-diversified and do not contain geographic or sectoral concentrations--assumptions that are not true in the case of many institutions. Supervisors must remind institutions that it is initially the banks' job to address any deviations from Pillar 1 assumptions, as well as any additional factors that affect the risk of the individual bank, and adjust their capital accordingly. Under Pillar 2, supervisory authorities, in turn, will review these adjustments by banks and could ask them to take additional steps to ensure that all risks have been addressed. There are additional reasons why I expect that well-run financial institutions will maintain capital ratios above the regulatory minimums, as they have under the existing Basel I framework. Some markets and customers will require their banks to have a stronger credit rating than that implied by the Basel I or II minimum capital frameworks. Banks will also continue to be opportunistic in pursuing mergers and new business expansion, and this requires capital above the regulatory minimum to be able to respond promptly to new initiatives. Finally, bankers who are using economic capital models such as RAROC (risk-adjusted return on capital) recognize that Basel II does not take into consideration some forms of unexpected losses, for example, higher charge-offs that occur when new products are introduced, information technology systems change, merger integrations occur, and internal control processes occasionally prove ineffective. Implementation Efforts in the United States The U.S. banking agencies' reaction to the results of the fourth Quantitative Impact Study--known as QIS4--shows how seriously we are taking Basel II implementation. In a statement issued on April 29, the U.S. banking agencies indicated that the minimum regulatory capital changes resulting from QIS4 were more variable across institutions and capital dropped more in the aggregate than the agencies had expected. This was the impetus for deciding to delay issuance of our next round of proposals for Basel II. These unexpected results show the continued benefit of conducting periodic quantitative impact studies. They serve as a milestone to help us calibrate the progress of the framework and the bankers as we move to Basel II. We now must determine the reasons for the unexpected results from QIS4. Do they reflect actual differences in risk among respondents when prior supervisory information suggested more similarity in credit quality? None of the participating banks has completed their databases and models for all of their risk areas. In some cases, this created results that would not be reliable for implementing Basel II. For example, for some portfolios, expected losses reflected only the last year or two of results. Thus, the strong credit performance of recent experience was not balanced by higher losses at other points of the credit cycle. Were there limits of the QIS4 exercise itself? Is there a possible need for adjustments to the Basel framework itself? Analyzing the data used in QIS4 is vitally important, because ultimately the success of Basel II will depend on the quantity and quality of data that banks have to use as inputs to the framework. I am sure that those of you working on Basel II--particularly the advanced approaches--are facing the same types of issues in your own countries. For those of you who will be conducting QIS5 or similar exercises, I strongly suggest that you include qualitative responses from the participants as well as quantitative data. We are finding this very useful as we review the results and have follow-on discussions with bankers. U.S. regulators expect to provide additional information on the lessons we learn from the QIS4 review in the near future. The notice of proposed rulemaking for Basel II will incorporate what we learn from this exercise. But we really are caught in a process dilemma. Bankers cannot complete their models and collect the necessary data until they know what the specific requirements will be. Regulators, on the other hand, will have to develop these requirements before seeing the actual results of these models and robust databases. The process we have for vetting Basel II in the United States is probably similar to those followed in many other countries. We are putting forward proposals and seeking comment from the industry, our legislature, and other interested parties. Given what a vast undertaking Basel II is, this seems entirely appropriate and beneficial. In addition to what we learn from the work on QIS4 results, we will also assess the trading and banking book comments of the Basel Committee on Banking Supervision and the International Organization of Securities Commissions. We will incorporate the latest proposal into the notice of proposed rulemaking and hope to complete our efforts in a timely manner. Challenges for Supervisors In preparing for Basel II, supervisors realize that they must address their own capital needs--that is, human capital. Throughout Basel II implementation in the United States, it has become strikingly apparent that supervisors will need a higher degree of knowledge, skill, and experience. Even just our preliminary work on Basel II, which includes writing regulations, drafting guidance, and evaluating preliminary estimates from banks, has consumed substantial resources within the Federal Reserve System. We are in the process of training existing staff members and recruiting new ones, and that itself takes time and resources. We are aware that to implement a framework of the complexity and scope of the advanced approaches of Basel II, we need highly qualified supervisors. As we have learned over the past few years, many aspects of Basel II will require a considerable amount of judgment and experience. That is, as supervisors engage in the qualification of institutions for Basel II and then conduct ongoing monitoring, they will need to become intimately familiar with many technical aspects of the framework and have the ability to assess each institution in context. We want to ensure that in all Basel II discussions, bankers will sit across the table from supervisory staff who understand the framework and how it applies to individual institutions. This does not pertain just to Basel II, specifically, but also to supervision of evolving risk-measurement and -management practices more generally. As they have in the past, supervisors must keep pace with the latest developments in the industry and be able to differentiate among them in terms of appropriateness. One of the many attractive characteristics of the Basel II framework is its flexibility for incorporating new best practices without having to be fundamentally restructured. It provides a useful and credible basis for improving bank practice today and allowing for future improvements--which could include actual modifications to the framework. We consider this vitally important because banking will remain a highly dynamic industry. Supervisors will have to be especially attentive to changing best practices and ensure that Basel II does not inhibit adoption of new banking practices and financial instruments. Conclusion Maintaining financial stability in global banking and financial markets continues to be an important objective of regulators, bankers, and other market participants, particularly because of the negative impact that financial instability has on economies as a whole. Basel II, in my view, will help improve financial stability. The new framework will enable bank regulatory capital ratios to be more responsive to changes in risk and will foster additional disclosures by banks about their risk-measurement and -management systems. And even though minimum regulatory capital ratios are likely to be more volatile under Basel II, this reflects greater risk sensitivity. Perhaps most important, Basel II will encourage banks to develop their systems to measure and manage risk as part of the investment needed to support strategic initiatives. The greater volatility in measured risk, coupled with strategic capital planning, should encourage bankers to continue to maintain actual capital levels above regulatory minimums. In the United States, we are working very hard on Basel II implementation and are taking the appropriate, measured steps to ensure that we get it right. I expect that those in other Basel member countries are doing the same, and facing similar challenges. Of course, certain non-Group of Ten countries are looking to see if adapting Basel II is the best choice for them in the near term. For all of us engaged in Basel II work, it is helpful to remember that certain prerequisites have to be met--particularly for the advanced approaches--including the development of qualified and experienced staff to oversee banks' adoption of the new framework.
No content found
No content found
Remarks by Chairman Alan Greenspan Government-sponsored enterprises To the Conference on Housing, Mortgage Finance, and the Macroeconomy, Federal Reserve Bank of Atlanta, Atlanta, Georgia (via satellite) May 19, 2005
Remarks by Governor Donald L. Kohn To the International Research Forum on Monetary Policy Conference, Frankfurt am Main, Germany May 20, 2005 Modeling Inflation: A Policymakers Perspective Nothing is more important to the conduct of monetary policy than understanding and predicting inflation. Price stability is our responsibility as central banks--it is how, in the long run, we contribute to society's welfare. Achieving and maintaining price stability will be more efficient and effective the better we understand the causes of inflation and the dynamics of how it evolves. I think central bankers are asking more of inflation analysis these days. In the United States, our attention was focused for many years on containing and then reducing inflation. The risks and rewards were one-sided, and policymakers were mostly interested in whether inflation would rise. Now that we are in the neighborhood of price stability, we can be faced with looking at the possibility that inflation will fall too low as well as rise too high. Moreover, so long as inflation expectations are well anchored, we can tolerate limited changes in inflation, but we need to know that a rise or fall is not the beginning of a more extended trend. Consequently, we focus closely on the reasons for any changes in inflation and their implications for the outlook. Of course, I have always known how important the analysis and forecasting of inflation was for monetary policy, but I must admit that as someone who now has to go on record with a vote on the basis of some notion of the future course of inflation, the exercise has taken on added meaning. I thought I might take advantage of this captive audience of researchers on central bank policies to ruminate a bit on the evolution of inflation modeling and suggest areas for further research. I know that European central banks have been in the forefront of recent efforts to improve our understanding of some key issues in this area, but I will focus on our practices in the United States. The Stability of the Basic Framework I find it remarkable how fundamentally stable our basic framework for analyzing inflation has remained over the past thirty-five years or so: That basic framework is essentially the expectations-augmented Phillips curve introduced by Milton Friedman and Edmund Phelps in the late 1960s. One of the key assumptions underlying this basic framework is the temporary rigidity of wages and prices. It is because of these nominal rigidities that monetary shocks have real effects: In the well-known litany, wages and prices do not change immediately in response to a positive monetary surprise, so real interest rates fall, and spending is stimulated. But higher demand cannot be met without pushing firms up their marginal cost curves as they compete for scarce labor and other resources. As opportunities to raise prices present themselves, firms take them to better align prices with costs. That process may be gradual, because firms' competitors may not be raising their prices at the same time. It is easy to see in this tale the central mechanism of the Phillips curve. What is missing from the story, though, is that seminal feature of Friedman and Phelps's framework, namely, expectations. Expectations are a key part of the framework because wages and prices will be set for some time, and so it is important for workers and firms to consider the economic conditions expected to prevail during the period that the wages and prices are fixed. If inflation is anticipated over the period ahead, wages and prices will be set commensurately higher as workers and firms strive to protect themselves against the erosion of their purchasing power. As Friedman and Phelps emphasized, these efforts to protect against the erosion of purchasing power by inflation will mean that an ongoing and fully anticipated inflation will, to a first approximation, have no effect on the level of resource utilization; the outcome of the economy will be whatever the real forces at work dictate. Friedman called the unemployment rate determined by such real factors the natural rate of unemployment. An important implication of the expectations-augmented Phillips curve is that any attempt to use monetary policy to lower the unemployment rate below the natural rate on a sustained basis will end in failure. Initially, expansionary monetary policy would lower unemployment as well as raise inflation. As the stimulus continued, however, firms and workers would increasingly protect themselves against the higher inflation, giving an additional boost to inflation. Eventually, there would be no additional employment; only a (self-reinforcing) higher rate of inflation. In 1970, the Federal Reserve held a conference that addressed this then-new framework; the conference encompassed both theoretical extensions, including Lucas' first exposition of rational expectations, and empirical implementation. In its essentials, the way we forecast inflation today is not all that different from what came out of that conference. That is, inflation is importantly a function of an output or employment gap relative to a natural rate, plus some measure of inflation expectations. Advances within the Basic Framework One of the first challenges that the new framework had to face was the supply shocks of the early 1970s. The framework was extended to allow for the effects of shifts in relative prices, such as crude oil and import prices. Such shifts can feed through fairly directly to the measures of core inflation through their effect on business costs, though their influence on inflation should be temporary unless they get built into labor costs or inflation expectations. We include these types of price terms today in our forecasting equations, and they are important to forming our views of the inflation outlook and thus to the policy process. Another early development within the framework was the buttressing of its microeconomic foundations, in particular by paying more careful attention to the modeling of nominal rigidities. John Taylor's staggered-contracts framework remains a touchstone because of its intuitive appeal--annual wage reviews are a familiar experience for most people who work. Much subsequent work--including, recently, among economists at the European Central Bank and the euro-area national central banks--has confirmed the key assumption underlying this model, which is that wages and prices are changed infrequently. A key objective of Taylor's staggered-contracts model was to show that, in an economy with nominal rigidities, monetary policy can have important effects even when expectations are perfectly rational. However, about a decade ago, Jeff Fuhrer and George Moore pointed out that inflation was more persistent than was predicted by the model with sticky prices and rational expectations. Since their work, a number of researchers have suggested that "sticky information" or rules of thumb can account for this excess persistence. Such departures of expectations from perfect rationality can be an important source of observed inflation dynamics. At the Fed, the staff takes a number of different approaches to the modeling of expectations. The staff's large, formal model (FRB/US) assumes rational expectations--but with a twist. In particular, the model addresses the Fuhrer-Moore critique by making inflation itself, as well as the levels of wages and prices, costly to adjust. The implications of these additional frictions are very similar to those of the departures of expectations from perfect rationality used by other modelers. An advantage of a model with expectations that are, at least in part, rational is that we can address questions related to how the behavior of the economy may change when the systematic implementation of monetary policy changes. We also look at models that assume that inflation expectations are well modeled by lagged inflation--the original proposal of Friedman and of Phelps. Such models may not be as useful in addressing policy questions. However, they have a good forecasting track record. The Performance of the Board Staff's Inflation Forecast The Board staff forecasts distributed to the Federal Open Market Committee (FOMC) are judgmental: Although the staff consults a variety of models in coming up with its forecasts, no one model can be said to summarize the staff view. Also, the staff forecasts are not necessarily its best guess on how inflation will evolve; the forecasts are conditioned on an assumed path for monetary policy and, during some periods, at the behest of policymakers, the staff did not assume what it would have viewed as the most likely policy path. I have distributed a that shows the Board staff's four-quarter-ahead forecasts for inflation as measured by the core Consumer Price Index along with the actual outcomes. The period shown is 1984 to 2000; I chose those years because the current definition of the core CPI did not come into use until 1983, and the staff's forecasts remain confidential for five years. As shown in the inset box, the root-mean-squared error of the staff projections has been smaller than that of a nave benchmark model, in which inflation is assumed to continue at its pace over the preceding four quarters. Nonetheless, the one-year-ahead root-mean-squared error of the staff forecast is about 1/2 percentage point. That is to say, almost one-third of the time, inflation has been either more than 1/2 percentage point higher, or more than 1/2 percentage point lower, than the staff has predicted. Moreover, over the period shown, there was, on average, some bias in the staff's inflation forecasts; inflation has tended to come in lower than the staff anticipated, by about 0.2 percentage point per year. No single explanation suggests itself for either the extent of the misses or the bias. Rather, a variety of factors has caused inflation to deviate from expectations. At times, demand was not as robust as anticipated, and unexpected but persistent changes in the foreign exchange value of the dollar and oil prices fed through to core CPI inflation on several occasions. But I will concentrate on one general phenomenon and two episodes that help illustrate how our understanding has evolved and some of the more general challenges for inflation forecasting over the past twenty years. One factor that may account for some of the upward bias over this entire period was a gradual reduction in the natural rate of unemployment. With hindsight, I believe we can point to a number of developments in labor markets that are consistent with such a reduction. For example, disability insurance rolls rose steadily over this period, which allowed many people who likely would have had above-average unemployment rates to withdraw from the labor force. Also, in the early 1990s, many public opinion surveys indicated a sharp increase in worker insecurity--and workers who are anxious about losing their jobs will be less willing to risk unemployment. These examples illustrate the need to be alert to the possibility of the natural rate shifting. As Friedman emphasized, the natural rate is not a rigid data point, but rather the reflection of many developments in the economy. In the 1988-90 period, the behavior of crude oil prices, unemployment, and the exchange rate were not especially surprising or anomalous. However, the models the staff was consulting in preparing its forecasts may have been miscalibrated. In particular, inflation expectations perhaps were becoming better anchored, so that an unemployment rate below the natural rate was putting less pressure on inflation than it would have over the preceding twenty years. Likewise, the staff may have overestimated the ongoing effects of the dollar decline on inflation, as the models in use at that time inevitably gave considerable weight to the experience of the 1970s. Empirical estimates unavoidably lag these sorts of endogenous changes in inflation dynamics. For the period from 1996 to 1998, inflation also came in consistently lower than the staff forecast. Here, the pick up in structural productivity growth was the likely cause: The historical record suggests that a sustained acceleration in productivity affects prices before it affects wages. Thus, the pickup in productivity growth has a direct, depressing effect on costs--and thus ultimately on prices. It took Fed forecasters--and others--a while to discern the acceleration in productivity and its implications for inflation. Interestingly, by the time of the forecasts made in 1998 and 1999--the 1999 and 2000 observations on the chart--the string of forecasting errors had ended. This improved performance likely reflected the eventual recognition that productivity growth had increased on a sustained basis. An Agenda for Further Research As I noted at the beginning of my remarks, research aimed at improving our understanding of and ability to predict inflation is essential to the central banker's mission. The better the forecasts, the better the odds that policy choices will contribute to economic stability and efficient resource allocation. Needless to say, more work remains to be done--and always will. My chart stops at 2000, but it is no secret that forecasters everywhere did not anticipate the extent of disinflation in the U.S. economy in 2003 and, even after the fact, have had trouble explaining what happened. Moreover, the degree to which core inflation picked up in 2004 and 2005 also caught many economists, including this one on the FOMC, by surprise. Surprises are inevitable; aggregate supply and demand curves shift for reasons that cannot be anticipated. But improvement should be possible in several dimensions. We could identify shocks sooner and get a better understanding of their likely effects on inflation. And we could attempt to narrow the definition of "shock." I suspect that much of what we consider to be exogenous is the working out of endogenous events that we do not understand very well. Better predictions inevitably begin with improved understanding--both theoretical and empirical. In reviewing some of the advances of the past thirty-five years for this talk, I was struck by the degree to which so much of the work on rigidities and expectations seemed to be trying to find an elegant rationale at the level of the firm and the worker for the observed dynamic properties of aggregate price measures. This work, while illuminating in many respects, does not seem to have greatly advanced the empirical forecasting of inflation. And, the microeconomic behaviors we describe to justify the empirical specifications of our macroeconomic models often do not coincide very well with what we find when we directly observe the decisionmaking of workers and firms. I think we need to push forward along these microeconomic lines. I have a lengthy list of macroeconomic inflation puzzles whose answers would make me a better policymaker, but, for the most part, the solutions to the puzzles rest on a better understanding of how workers and firms set wages and prices. Researchers at the ECB and at the euro-area national central banks have made an important contribution in their recent work. I agree with one of the conclusions I understand many of them came to--that is, we especially need to improve our understanding of the determinants of labor compensation. The reduced-form price equations we so often use for inflation prediction bypass direct contact with labor compensation issues. But the labor market is at the foundation of the Friedman-Phelps analysis. Labor is the major element of business cost and as such often occupies a prominent role in policy discussions of inflation prospects, and the unemployment rate often proxies for resource slack more generally. As I have already discussed, unanticipated changes in the natural rate have contributed to forecasting errors over the past two decades. In the past few years, we have had some experience with wage setting under conditions of price stability, and nominal compensation showed greater flexibility than some observers had anticipated. Too often, discussions of wage and compensation determination rely on descriptions of worker demands and expectations that seem drawn from an era of strong unions rather than from the more atomistic labor markets that dominate the U.S. economy these days. A better understanding of the motivation and dynamics of how compensation is determined between firms and individuals or small groups of workers would help unravel a number of the inflation puzzles I think we face, including those involving productivity growth, globalization, markups, and expectations formation. Changes in Productivity Growth An important aspect of this story has been that productivity affects prices before it affects wages--that is why we were able to experience low and stable inflation in the latter part of the 1990s with the unemployment rate well below any estimates of its natural level. But is it really true that prices are more responsive to productivity than wages? Why? Should the effects be symmetrical when productivity growth slows? How can we better estimate structural productivity and determine changes in its pace of growth more promptly? Globalization and the Inflation Process Several observers have argued that increased trade has been an important factor in the downtrend in inflation over the past two decades. One channel is said to be through greater competitive pressures and another through increased support for price stability engendered by the competitive environment. Globalization might restrain prices and wages in those sectors in which imports play an increasing role, but how does it hold back the average wage and price level? And, how do we reconcile the sense of greater competitive pressures with record levels of profits--and capital income more generally--in the United States? The Behavior of Profit Margins or Markups The Federal Reserve's 1970 conference and much of the work since then has approached the determination of inflation as a two-step process: model both labor costs and the price markup over labor costs. Yet, we find that this approach does not work very well in practice. Years of experience suggest that although profit margins tend to return to their mean, deviations can increase for a time and the eventual return can be slow and very difficult to predict. In the United States, markups have remained unusually elevated of late, absorbing little of the rise in the cost of energy, import, and materials. Does it matter whether the shock to margins comes from a change in potential supply or aggregate demand? What type of pricing behavior reconciles these outcomes? How are they consistent with the expectation that margins return to means? Inflation Expectations Measures of inflation expectations are among the variables I watch most closely as I formulate my policy recommendations because I recognize that changing expectations are a principal avenue by which short-term perturbations in price levels are propagated into more persistent changes in inflation rates. Yet our knowledge of the expectations that businesses and workers bring to the process of setting wages and prices is extremely limited. We use proxies--most often surveys of economists, whose projections may be influenced by their knowledge of other economists' projections, and of households, who may or may not understand the question or have a realistic view of what to expect. Readings from the financial markets are helpful, but they are also muddied by changing premiums for inflation risk and liquidity, and they are not necessarily representative of the attitudes of households or businesses. Moreover, how expectations are formed remains an area that would benefit from further research. How much do people rely on the immediate past in forming expectations about the future? To what extent are projections from the past modified by what they know about the goals of the central bank or the stage and characteristics of the current economic cycle? How often do expectations get updated, and what types of information are used in the process? * * * This is a daunting research agenda, but it should be given a high priority. I appreciate the opportunity to spell it out before an audience that has the skills and the opportunity to address some of these pressing questions. Footnotes The views I am expressing today are my own and not necessarily those of my colleagues on the Federal Open Market Committee. , of the Board's staff, helped with the preparation of these remarks. Milton Friedman (1968), "The Role of Monetary Policy," A merican Economic Review , vol. 58 (March), pp. 1-17; Edmund S. Phelps (1968), "Money-Wage Dynamics and Labor-Market Equilibrium," Journal of Political Economy , vol. 76 (July-August, part 2: Issues in Monetary Research), pp. 678-711. The Econometrics of Price Determination (1972), proceedings of a conference sponsored by the Board of Governors of the Federal Reserve System and the Social Science Research Council, October 30-31, 1970 (Washington: Board of Governors of the Federal Reserve System). Jeff Fuhrer and George Moore (1995), "Inflation Persistence," Quarterly Journal of Economics , vol. 110 (February), pp. 127- 59. The discussion of the forecasting record that follows is largely a matter of conjecture and guesswork. It is hard to pinpoint the cause of any particular forecast error. I am drawing on my own memories of events, as well as those of current members of the Board's staff. Kenneth S. Rogoff (2003), "Globalization and Global Disinflation," in Monetary Policy and Uncertainty: Adapting to a Changing Economy, proceedings of a symposium sponsored by the Federal Reserve Bank of Kansas City, August 28-30 (Kansas City: Federal Reserve Bank of Kansas City), pp. 77-112; Alan Greenspan (2005), "Globalization," speech presented to the Council on Foreign Relations, March 10.
No content found
Financial Services Policy Committee A Committee of the Conference of Presidents Federal Reserve System David Fettig FSPC Spokesman (612) 204-5274 Federal Reserve Banks Announce Changes to Increase Efficiency in Check Services Minneapolis, Minn., May 25, 2005--As part of their ongoing effort to respond to the significant shift away from the use of paper checks and toward the much greater use of electronic payments, the Federal Reserve Banks will discontinue check processing at the Federal Reserve Bank of New York's East Rutherford Operations Center. That volume will be processed at the Federal Reserve Bank of Philadelphia. No firm date for the transition has yet been determined, but it is expected to take place in the second half of 2006. The change is aimed at increasing the efficiency of the Reserve Banks' check-processing operations, while continuing to provide high-quality services to depository institutions throughout the country. "The step announced today will help the Reserve Banks reduce our check service operating costs in line with the continuing shift in consumer and business preferences for electronic payments," said Gary Stern, chairman of the Reserve Banks' Financial Services Policy Committee and president of the Federal Reserve Bank of Minneapolis. "Today's announcement marks the third annual review of our check infrastructure, which has resulted in a reduction in the number of locations processing checks. We will continue to evaluate our check processing infrastructure annually to ensure that we are well positioned to meet the needs of the nation's payment system." Since 2003, the Reserve Banks have reduced the locations where they process checks from 45 to 29 as of today. An additional six locations, previously announced, will no longer process checks by early 2006, further reducing the number to 23. After the step announced today is completed, the Reserve Banks will process checks from 22 locations nationwide. "The changes that we have implemented over the past three years have been good for the nation's payments system but difficult for our organization as we have been required to reduce our staff," said Stern. To assist affected staff, the Reserve Banks will offer a variety of programs, including separation packages, extended medical coverage and career transition assistance. As a result of the action announced today, the Reserve Banks will reduce their overall check staff by approximately 80 positions, representing about 2 percent of the Reserve Banks' current check employees. At the East Rutherford location, about 140 positions will be affected. Some staff reductions may occur through attrition and there may be some opportunities for reassignment. The Reserve Banks estimate that they will add approximately 60 positions in Philadelphia to help process the additional volume. In 2004, Reserve Banks' check volume declined at about a 12 percent rate. During 2005, check volumes have continued to decline; further decline is anticipated in the coming years. A 2004 study revealed that about 37 billion checks were paid in the United States in 2003, down from 42 billion in 2001 and 50 billion in 1995. Electronic payments, including those made by credit cards, debit cards, and through the automated clearinghouse system increased from about 30 billion transactions in 2001 to more than 44 billion transactions in 2003. The Federal Reserve Banks' long-term check processing strategy will allow them to better meet the expectations of the 1980 Monetary Control Act. That act requires the Federal Reserve to set prices to recover, over the long run, its total operating costs of providing payment services to financial institutions, as well as the imputed costs it would have incurred and the profits it would have expected to earn had the services been provided by a private business firm. # # # Federal Reserve System 2005 Check Restructuring Fact Sheet Federal Reserve check processing locations: By early 2006, checks will be processed at 23 Federal Reserve locations nationwide (see a listing of specific locations below). Based on the change announced today, checks will be processed in 22 locations when the office moves are completed in the second half of 2006. Staff levels: The Federal Reserve System, including the Board of Governors, employs approximately 22,000 staff nationwide; approximately 3,800 of these employees work in the check function. National check volumes: By Federal Reserve estimates, roughly 37 billion checks were paid in the United States in 2003, down from about 42 billion in 2001 and 50 billion in 1995. The Reserve Banks handled over 12 billion checks in 2004. Listing of specific Federal Reserve locations (note that check processing sites current as of the end of the first quarter of 2006) District 1: Boston, Mass. (head office, no check processing); Windsor Locks, Conn. District 2: New York City, N.Y. (head office; the check processing center is located in East Rutherford, N.J.); Buffalo, N.Y. (branch, no check processing); Utica, N.Y. District 3: Philadelphia, Pa. (head office) District 4: Cleveland, Ohio (head office); Cincinnati, Ohio (branch); Pittsburgh, Pa. (branch, no check processing) District 5: Richmond, Va. (head office, no check processing); Baltimore, Md. (branch); Charlotte, N.C. (branch) District 6: Atlanta, Ga. (head office); Birmingham, Ala. (branch, no check processing); Jacksonville, Fla. (branch); Miami, Fla. (branch, no check processing); Nashville, Tenn. (branch, no check processing); New Orleans, La. (branch) District 7: Chicago, Ill. (head office; the payments center is located at Midway Airport); Detroit, Mich. (branch, no check processing); Des Moines, Iowa District 8: St. Louis, Mo. (head office); Little Rock, Ark. (branch, no check processing); Louisville, Ky. (branch, no check processing); Memphis, Tenn. (branch) District 9: Minneapolis, Minn. (head office); Helena, Mont. (branch) District 10: Kansas City, Mo. (head office); Denver, Colo. (branch); Oklahoma City, Okla. (branch, no check processing); Omaha, Neb. (branch, no check processing) District 11: Dallas, Texas (head office); El Paso, Texas (branch, no check processing); Houston, Texas (branch, no check processing); San Antonio, Texas (branch, no check processing) District 12: San Francisco, Calif. (head office); Los Angeles, Calif. (branch); Phoenix, Ariz. (cash processing facility); Portland, Ore. (branch, no check processing); Salt Lake City, Utah (branch, no check processing); Seattle, Wash. (branch)
Remarks by Governor Edward M. Gramlich At the Euromoney Inflation Conference, Paris, France May 26, 2005 The Politics of Inflation Targeting
Remarks by Vice Chairman Roger W. Ferguson, Jr. To the Seventh Deutsche Bundesbank Spring Conference, Berlin, Germany May 27, 2005 Asset Prices and Monetary Liquidity Over many decades, the Federal Reserve and the Bundesbank have enjoyed a very productive working relationship that has included sharing our concerns and insights about good policy. The sharing has been not only between our two institutions but with the policy and research communities more broadly. This conference is one more such occasion, and I am grateful to the Bundesbank organizers for this opportunity to speak this evening. Several of my recent assignments, both at the Board and in international groups, concerned the continuing effects of financial innovation and liberalization on the scope and scale of financial activity around the globe. In fact, in terms of sheer volume, the expansion of financial activity has greatly outstripped economic growth in recent years. With wider and faster access to information, markets have become more precise, on balance, in pricing value and risk. Innovation and liberalization have also brought more diverse opportunities for investors and borrowers, a wider selection of financial instruments, and generally improved risk management. The overall result has been more-efficient use of financial and real resources and, ultimately, better economic performance. Many observers believe that this result has been a key factor in the strong productivity and growth that the United States has realized in recent years. Many economies have become more resilient and more capable of withstanding shocks, but financial markets themselves seem to have become more sensitive to real-time data. The past decade has been marked by episodes of financial volatility that have had the potential for trouble at a systemic level. Linkages between financial markets and real economic outcomes have become more complex, periodically presenting policymakers with surprises and puzzles. And, paralleling efforts by central banks at improving transparency of the policy process, heightened scrutiny of policy by markets has added both new benefits and new complexities. A particular phenomenon that touches on all these issues is the movement of asset prices, especially the prices of equities and residential real estate. Because these assets are the most widely held by the general public, price changes, even when not exceptional, can significantly affect the macroeconomy. Rising asset prices support household consumption, whereas falling asset prices damp consumption. In a scenario of collapse, the damage to balance sheets and private wealth could go as far as undermining the soundness of the financial system and threatening stability of the real economy. Apart from such outcomes, policymakers might also take special interest in asset price movements because it has been alleged that badly designed or poorly implemented policy (even if well intended) sometimes has helped feed unsustainable movements in asset prices. Accordingly, I would like to highlight some aspects of the link between monetary conditions and asset prices and point to areas in which the policy community could use further insight from researchers, including perhaps from this distinguished group. I should note, my comments this evening reflect only my views and should not be taken to represent the views of my colleagues on the Board of Governors or in the Federal Reserve System. Interaction of Asset Prices and Policy Asset price movements that are discontinuous or extreme can affect the policy process in at least two important ways. First, because they are interest-sensitive, asset prices are primary components of the channels by which monetary policy is transmitted to the real economy. If these transmission channels are disrupted, the reliability and the effectiveness of policy are degraded. In the worst case, policy's room for maneuver may be narrowed or even severely compromised, and risks of a policy blunder are heightened. Second, because asset prices are forward-looking, they should contain information of value for the policy-setting process. If those signals of market participants' expectations are blurred by extreme movements, important information may be misread or lost. Of course, some uncertainty is inevitable and quite manageable in the course of normal policymaking. By virtue of its mandate, the Federal Reserve focuses on price stability, interpreted to mean stability of consumer prices. Monetary policy does not aim at any particular relative price, nor is the mandate thought to refer to prices beyond consumer prices, such as those for assets. Ordinarily, however, asset prices are among the many factors that central bankers assess when we evaluate present economic and financial conditions and the outlook. And when asset prices exhibit large, systematic, and persistent deviations from fundamentals, the implications of those deviations inevitably get more prominence. Clearly central bankers would benefit from a better understanding of asset price movements--particularly more extreme movements--so that we do not mistakenly facilitate in some way potentially harmful outcomes. Liquidity and Asset Prices Overly rapid monetary expansion, or excessive liquidity, has been named as a leading suspect in some episodes of unsustainable movements in asset prices. Liquidity is not a precise concept, however. Liquidity could be measured narrowly as central bank money, for example, or more broadly to reflect the multiplier effects of the financial system; sometimes it is measured instead by the level of policy interest rates. All these definitions and others have been in play in the economics profession's analysis of the link between monetary conditions and asset prices. What is meant by "excessive" is even less well defined. Some commentators have taken a circular approach: If monetary conditions were comparatively easy when asset prices experienced a significant swing, then liquidity must have been "excessive." A good place to start to unravel some of these issues is to look at the data. The results of research at the Board and that of many others seeking to document a link between liquidity and swings in asset prices has focused on growth rates of broad monetary aggregates and measures of equity and house prices deflated by the price index for personal consumption expenditures. Using these measures, the results differ significantly according to the type of asset being studied. For equity prices, under various alternative specifications, the link between the growth rate of liquidity and changes in real equity prices at frequencies beyond very short term appears to be tenuous at best. To illustrate, the shows a scatter plot of contemporaneous four-quarter growth rates of M3 and changes in broad measures of equity prices for sixteen industrial countries during roughly the past twenty-five years. As you can see, no obvious sign of a meaningful correlation at this frequency is evident in this pattern. Indeed, the actual correlation is slightly negative. Most other studies--such as the study by Bordo and Wheelock--using a variety of definitions and more-rigorous techniques, support the same conclusion. The lack of a strong finding of any positive medium- to longer-run relationship, of course, could be because equity prices are quite volatile, making unconditional correlation difficult to identify. Perhaps, too, more-refined measures of liquidity than those used in studies so far are needed to capture possible effects on equity prices. So the effect of money growth on real equity prices is by no means a closed question, and there certainly would be interest in whatever might come from further research on this topic. In contrast, the link between monetary growth, as measured by M3, and changes in real house prices appears to be more definite. The shows a similar scatter plot for the same group of countries and periods as in the upper panel, but for changes in real house prices. The two series exhibit a small positive correlation that is statistically significant. Moreover, various tests have shown that the correlation is not just a recent phenomenon or confined to a few countries; it is evident in varying degrees both over time and across our sample of sixteen countries. Again, this finding is consistent with findings from a number of other academic researchers. (I might add that the correlation with real house prices also holds even if M3 is normalized by prices or gross domestic product. Negative correlation between interest rates and house prices is, of course, not unexpected.) To explore this relationship further, provides some background on longer-term movements in house prices in the United States, the United Kingdom, and Japan--three countries where house price movements have been prominent. As the three panels show, each country's movements in real house prices have experienced two or three cycles during the past thirty-five years, usually with relatively long periods of gains followed by long periods of decline. This suggests that one potentially useful way to explore further the relationship between liquidity and asset prices is to examine movements in those two variables in ten-year windows surrounding house price peaks. The solid line in shows the average behavior of the growth rate of M3 from twenty quarters before a peak in real house prices until twenty quarters after the peak, where the average is taken across all the cyclical episodes in our broad sample. The broken line in the chart is the log level of real house prices indexed to zero in the peak year. As can be seen from the chart, on average the growth rate of money increases fairly steadily until around two quarters before the peak in real house prices and then drops fairly steadily for ten quarters afterward before recovering somewhat. Although there are some variations, this pattern tends to occur in most countries' episodes. Importantly, the connection between money growth and house prices does not seem to vary much with the rate of macroeconomic expansion. This statement is underlined by , which shows as an example actual house prices for the United Kingdom (the broken line) and their forecasted values from an equation based on the growth rate of real broad money and real long-term interest rates over more than twenty years (the solid line). The close correspondence is apparent. While these results are suggestive, correlation is by no means causality. Confirming that variations in growth rates of liquidity (especially unusual changes in liquidity growth that could be called excessive) systematically lead to wide swings in real asset prices requires further theoretical investigation and more empirical support. That asset prices rise during a period of monetary easing is not surprising, of course. But why growth of money sometimes might cause housing prices to move more than other asset prices or, for that matter, than goods prices is not clear. Indeed, some elements of causality run in the opposite direction, clouding the underlying relationships. For instance, as the value of collateral rises during a house price boom, the associated expansion of credit typically leads to increases in broad money. The effect can be accentuated by the temporary parking in liquid accounts of the proceeds from the more-frequent turnover and refinancings that often accompany a house price boom. Complicating the analysis further, a substantial list of third factors could drive both housing prices and liquidity measures, producing co-movements that look like causality. A productivity shock or a sharp decrease in energy prices, for example, could lead to general economic expansion and rising asset prices while goods prices are not rising. This situation might allow more liquid monetary conditions than would otherwise be the case. Distinguishing the various forces in play obviously requires tools that are finer than simple observations that both liquidity measures and assets prices are moving together. Identification of Unsustainable Movements The example cited above also should remind us that not all situations in which asset prices are rising rapidly under seemingly easy monetary conditions are worrisome. Some are quite benign and even signal a healthy economy. Accordingly, for policymakers who have to confront these situations in real time, a fundamental challenge is identification. When asset prices are moving in an unusual manner, is the movement unsustainable; does the pattern arise from excessive liquidity in the market; and will policy be challenged by that combination at some future date? Or are we observing a more complex process, perhaps less aberrant and less prone to a troubling reversal? The answers to these questions depend in part on the link between an asset's price and some estimate of its "fundamental" value. Unfortunately, from the point of view of both the analyst and the policymaker, the link between an asset's price and the structure of its return is hard to pin down, as it typically embodies complex factors that are inherently difficult to measure, such as expected future earnings, riskiness, and risk aversion. Consequently, for evaluating in real time whether prices may be "off track" and, if so, by how much, we have to fall back on more readily observable measures that, in principle at least, should bear some systematic relationship to ideal measures and that can be assessed against historical experience. For equities, a stock's price-earnings ratio is a standard benchmark for assessing valuation. In the late 1990s, the price-earnings ratio for U.S. equities--especially in the high-tech area--soared to record levels, and the so-called equity risk premium narrowed to a historical low. There was a strong sense at the time that such elevated price levels were unusual, but there was no uniform consensus regarding whether or not they were sustainable. All agreed, however, that the pricing of assets, given profit expectations, was difficult. And even after we have seen a major correction, our understanding of what drove that process and what the proper level should have been--and should be now--still is unsettled. For housing, rent-to-price ratios and income-to-price ratios are commonly used measures to assess valuation. Over the past several years, both measures have decreased sharply in many countries, and they currently are well outside historical ranges in some countries. In 2004, U.S. home prices increased 11.2 percent, their fastest pace since 1979, and right now, housing prices in many markets in the United States are relatively high when judged by conventional valuation measures. To know if housing is fairly valued requires assessing whether today's valuations are consistent with unobservable future rents, interest rates, and returns--concepts for which we have only rough proxies. However, in some markets the most prudent judgment is that the growth of house prices will slow from the rapid pace experienced most recently. Interpretation of Correlation: Other Fundamental Factors The issue of identification becomes much more difficult in a changing environment. Some patterns of asset price changes that are attributed to excess liquidity may arise, for example, from financial innovation and other structural changes. But mapping such changes into asset price movements to control for such factors is difficult. For example, the rise in house prices in the United Kingdom in the late 1980s was thought at the time to relate importantly to the liberalization of U.K. banking laws several years earlier and, therefore, likely to be sustainable. As it turned out, the run-up ended fairly soon and was followed by a steep decline. The inability to more accurately gauge the effects on asset prices conferred (or not) by earlier structural changes likely contributed to the inaccurate expectation that they would last longer. In Sweden, the large and swift increases in property prices in the 1980s also related in part to the earlier liberalization of domestic banking laws and consequent easier lending practices. The tax reform of 1990, which among other things made the tax treatment of mortgage interest rates in Sweden considerably less favorable, contributed to a subsequent drop in property prices that ultimately severely stressed banks' balance sheets, an experience that again highlighted the potential impact of such structural factors on asset price movements. Controlling for cyclical effects on both liquidity and real asset prices is basic, of course, in any empirical assessment of how they may be linked. But some observers have conjectured that, beyond conventional cyclical patterns in a benign environment with low-inflation and strong real growth such as we have had in recent years, investors' preferences and behaviors may change qualitatively in ways that are not captured well by models based on historical patterns. Faced with cumulating wealth and lower nominal returns, investors may develop a distinctly greater tolerance for risk that results in aggressive "search for yield" behavior. This behavior may engender, it is argued, a tendency for unusual run-ups in some asset prices. If so, a concern is that changes in the underlying conditions that fostered this pattern or a policy misstep could cause a quick reversion to the historical norm. The policy community has been on the lookout for such patterns of "search for yield" during the latest cycle of low rates and tightening. Others have argued that in certain circumstances, when special factors may be preventing inflationary pressures from showing through to consumer prices, increases in asset prices might serve as an early, more visible warning that liquidity is excessive. It has been asserted, for example, that the strength of the yen in the late 1980s stifled Japanese consumer price inflation and contributed to an easier policy stance by the Bank of Japan. The fact that resulting liquidity was greater than would have been ideal was most apparent (unfortunately, more so in retrospect) in Japan's booming asset prices. A sharp (but temporary) boost in productivity or disruption of exchange rate pass-through behavior could have similar masking effects. But whether, in general, asset prices could provide reliable signals of suppressed but impending general inflation is a complex matter, and elevating asset prices to a more prominent role in the policy process is not without a number of potential pitfalls. Among other complications is the possibility that financial globalization may be changing the links between liquidity and asset prices. Movements in asset prices across countries now appear to be more synchronized. This synchronization could arise in a number of ways. National business cycles and policy responses may be moving more in tandem just because national economies have become more closely integrated through trade and investment, producing in turn a greater synchronization in asset markets. Global shocks (for example, from oil prices and geopolitical risk factors) that produce broadly similar effects on most economies may have become more prevalent and may tend to dominate idiosyncratic national shocks. But it also is quite possible that greater international diversification of portfolios now allows developments affecting assets in one country to spill over into markets of others--both at the level of particular industries and more broadly. If synchronization of asset price movements comes about mainly in this way, the suggestion is that excess liquidity in one country could move asset prices in another, perhaps significantly, even if liquidity was well contained in the latter. In the somewhat longer run, patterns of aggregate saving--influenced, for example, by demographic developments--may also affect the path of real interest rates and, in turn, prices of assets, again complicating interpretation of co-movements. For instance, an increase in savings as a growing share of the population nears retirement may reduce real rates and raise asset prices. Patterns of immigration that affect the population's demographic profile, too, can affect asset prices. In the United States, we have seen some of these effects in certain segments of the housing market. Conclusions In summary, although excess liquidity has been cited as a source of general asset price instability, the support for this conclusion is mixed, at best. We do find a positive correlation between growth rates of real house prices and M3, but the correlation does not seem to hold for real asset prices more generally--including, in particular, equities. In this talk, I have suggested that we are only at the beginning of an understanding of some of these relationships. Even if an underlying causal link exists between some real asset prices and liquidity, many additional factors may be influencing what we see, with potential for misleading us in interpreting simple associations or correlations. The problem obviously is complex. As a start, however, we need a better understanding of how asset prices are determined that can be translated into guidance for a policy process that uses real-time variables. To borrow a turn of phrase cited often in the physical sciences, theory tends to "explain a complex visible, with a simple invisible." In the realm of policy, of course, we need to work with simple, but reliable visibles--a requirement that makes this task all the more challenging. For that reason, I have been pleased to join this group of distinguished economists this evening. Appendix Countries in the dataset and the date for the beginning of the broad money series used for each country are as follows: Country First data point for broad money Australia 1970 Belgium 1996 Canada 1970 Denmark 1993 Finland 1990 France 1980 Germany 1980 Japan 1980 Netherlands 1982 New Zealand 1988 Norway 1970 Spain 1997 Sweden 1985 Switzerland 1985 United Kingdom 1982 United States 1970 Residential real estate and equity index data cover the period 1970-2004 for all countries except Spain, for which the coverage begins in 1971. Figures Footnotes I wish to thank Robert Martin, Alain Chaboud, Jon Faust, and Brian Doyle of the Board staff for research support. Various studies have found that stock prices do exhibit an immediate reaction to monetary policy surprises, not unlike their responses to other macroeconomic surprises, for example, Ben S. Bernanke and Kenneth Kuttner (2005), "What Explains the Stock Market's Reaction to Federal Reserve Policy?," Journal of Finance , vol. 60, 1221-57. Data on residential real estate and equity prices are from the Bank for International Settlements and, for most countries, are quarterly. M3 data are derived from national sources; the length of the series on M3 varies across countries. The list of countries covered and the relevant periods are provided in the appendix. Michael D. Bordo and David C. Wheelock (2004), "Monetary Policy and Asset Prices: A Look Back at Past U.S. Stock Market Booms," NBER Working Paper 10704. House prices in other industrial countries have shown similar, but generally more modulated, swings. The correlation between real house prices and money growth in Japan was close to zero during the deflationary 1990s. For further details on the difficulty of identifying unsustainable movements in asset prices, see Refet S. Gurkaynak (2005), "Econometric Tests of Asset Price Bubbles: Taking Stock," (Finance and Economic Discussion Series) working paper 2005-04. The source of the data is the Office of Federal Housing Enterprise Oversight. The OFHEO index is based on repeat sales prices derived from mortgage acquisitions on conforming loans. See, for example, Franklin Allen and Douglas Gale (2000), "Bubbles and Crises," Economic Journal , vol. 110, pp. 236-56. For example, see my as Chairman of the Financial Stability Forum to the International Monetary and Financial Committee, April 16, 2005, Washington, D.C., and . See, for example, Takatoshi Ito and Frederic S. Mishkin (2004), "Two Decades of Japanese Monetary Policy and the Deflation Problem," NBER working paper 10878. This argument obviously applies best to markets such as those for equities where cross-holdings and international diversification are comparatively well developed. An IMF research paper has found evidence of liquidity spillovers and that an increase in aggregate G-7 liquidity is "consistent with" an increase in G-7 real stock returns (Klaas Baks and Charles Kramer (1999), "Global Liquidity and Asset Prices: Measurement, Implications, and Spillovers," IMF working paper 99/168). The origin of the remark is generally attributed to French scientist Jean Baptiste Perrin in his Nobel Lecture, given when accepting the 1926 Nobel Prize in physics. Australia 1970 Belgium 1996 Canada 1970 Denmark 1993 Finland 1990 France 1980 Germany 1980 Japan 1980 Netherlands 1982 New Zealand 1988 Norway 1970 Spain 1997 Sweden 1985 Switzerland 1985 United Kingdom 1982 United States 1970
No content found