For immediate release The Federal Reserve Board today announced its approval of the proposal of Firstar Corporation, Milwaukee, Wisconsin, to merge with Mercantile Bancorporation, Inc. ("Mercantile"), and thereby acquire Mercantile's wholly owned registered bank holding company, Ameribanc, Inc. ("Ameribanc"), its lead subsidiary bank, Mercantile Bank National Association, all in St. Louis, Missouri, and the other subsidiary banks and the nonbanking subsidiaries of Mercantile.
For immediate release The Federal Reserve Board today requested comment on revised proposals to permit electronic delivery of federally mandated disclosures under five consumer protection regulations: B (Equal Credit Opportunity), E (Electronic Fund Transfers), M (Consumer Leasing), Z (Truth in Lending), and DD (Truth in Savings). Comment is requested by October 29 . In March 1998, the Board published an and proposed changes to Regulations B, M, Z, and DD to permit financial institutions and others to send electronic disclosures, if the consumer agrees, with few other requirements. Disclosures may be sent to the consumer's e-mail address or posted on a web site. In response to comments, the Board on August 18 approved the publication of revised and more comprehensive proposals, together with proposed commentary that would provide guidance on electronic communication. The interim rule to Regulation E remains in effect. The Board today also published an interim rule to Regulation DD regarding the electronic delivery of periodic statements.
For immediate release The Federal Reserve Board today announced its approval of the proposal of Fleet Financial Group, Inc., to acquire BankBoston Corporation, both of Boston, Massachusetts, and its banking and nonbanking subsidiaries. The combined organization will be the largest banking institution in the northeastern United States. To address competitive concerns arising from the proposal, Fleet is required to divest more than 300 branches, controlling total deposits of more than $13 billion, located in Massachusetts, Connecticut, New Hampshire, and Rhode Island. This is the largest divestiture ever to take place in connection with a banking combination. Attached is the Board's Order relating to this action.
For immediate release The Federal Reserve Board today announced its approval of the application of United Bank of Philadelphia, Philadelphia, Pennsylvania, to acquire four branches of First Union National Bank, Charlotte, North Carolina, located in Philadelphia and to establish branches at the acquired locations. Attached is the Board's Order relating to this action. FEDERAL RESERVE SYSTEM United Bank of Philadelphia Philadelphia, Pennsylvania Order Approving Acquisition and Establishment of Branches United Bank of Philadelphia ("United"), a state member bank, has applied under section 18(c) of the Federal Deposit Insurance Act (12 U.S.C. ï¿½ 1828(c)) (the "Bank Merger Act") to acquire four branches of First Union National Bank, Charlotte, North Carolina ("First Union") Philadelphia, Pennsylvania. United also has applied under section 9 of the Federal Reserve Act (12 U.S.C. ï¿½ 321) to establish branches at the locations of the branches to be acquired, as described in the Appendix. Notice of the applications, affording interested persons an opportunity to submit comments, has been given in accordance with the Bank Merger Act and the Board's Rules of Procedure (12 C.F.R. 262.3(b)). As required by the Bank Merger Act, reports on the competitive effects of the acquisitions were requested from the United States Attorney General and the other federal banking agencies. The time for filing comments has expired, and the Board has considered the application and all facts of record in light of the factors set forth in the Bank Merger Act and section 9 of the Federal Reserve Act. The Bank Merger Act prohibits the Board from approving an application if the proposal would result in a monopoly or would be in furtherance of an attempt to monopolize the business of banking. The Bank Merger Act also prohibits the Board from approving a proposal that would substantially lessen competition or tend to create a monopoly in any relevant market, unless the Board finds that the anticompetitive effects of the proposed transaction are clearly outweighed in the public interest by the probable effects of the transaction in meeting the convenience and needs of the community to be served. United is the 58th largest depository institution in the Philadelphia market, controlling deposits of $107.5 million, representing less than 1 percent of total deposits controlled by depository institutions in the market ("market deposits"). First Union is the largest depository institution in the market, controlling deposits of $25.6 billion, representing approximately 37.6 percent of market deposits. The four branches that United proposes to acquire control approximately $61.1 million of deposits, representing less than 1 percent of market deposits. On consummation of the proposal, United would become the 40th largest depository institution in the Philadelphia banking market, controlling deposits of $168.6 million, representing less than 1 percent of market deposits. The Herfindahl-Hirschman Index ("HHI") would not increase and would be 1767 after consummation of the proposal. The proposal would be consistent with the Department of Justice Merger Guidelines ("DOJ Guidelines"), and the Department has advised the Board that consummation of the proposal would not likely have a significant adverse effect on competition in any relevant banking market. The other federal banking agencies also have been afforded an opportunity to comment and have not objected to consummation of the proposal. After carefully reviewing these and all other facts of record, the Board concludes that consummation of the proposed transaction would not be likely to result in a significantly adverse effect on competition or on the concentration of banking resources in the Philadelphia banking market or any other relevant banking market. Accordingly, the Board has determined that competitive factors are consistent with approval. In reviewing this proposal under the Bank Merger Act and section 9 of the Federal Reserve Act, the Board also has considered the financial and managerial resources and future prospects of United. The Board has reviewed these factors in light of all the facts of record, including supervisory reports of examination assessing the financial and managerial resources of the bank. The Board has stated and continues to believe that capital adequacy is an especially important factor in analyzing the expansion proposals of banking organizations. The Board notes that United has raised additional capital in anticipation of and as part of the proposal and would remain well capitalized on consummation of the proposal. The Board also has carefully considered projections provided by United of the financial benefits that are expected to result from the proposal, including projected operating revenues and expenses. Based on all the facts of record, including the commitments provided by United in connection with this case, the Board concludes that considerations relating to the financial and managerial resources and future prospects of United are consistent with approval. The Bank Merger Act also requires the Board to consider the convenience and needs of the communities to be served. In considering this factor, the Board has reviewed United's record of performance under the Community Reinvestment Act, 12 U.S.C. ï¿½ 2901 et seq. ("CRA"). United received an "outstanding" rating at its most recent examination for CRA performance by the Federal Reserve Bank of Philadelphia, as of March 1997. The Board also notes United's role in serving the banking and credit needs of low-income individuals and minority individuals in Philadelphia. Sixty-six percent of the census tracts in United's service area are designated as low-income. United offers products and services tailored to meet the needs of the community it serves, including checking accounts with no monthly service charges for customers who maintain a $100 minimum balance. United and its affiliates also provide traditionally underserved individuals with credit counseling and financial workshops that address such subjects as personal finances, small business financial management, and the fundamentals of maintaining a banking relationship. Based on its review, and for the reasons discussed above, the Board concludes that convenience and needs considerations, including the CRA performance records of the institutions involved, are consistent with approval. As part of its review of the proposal under section 9 of the Federal Reserve Act, the Board also has considered the location of each of the branches to be established by United. As a state member bank, United is authorized to establish a branch at the location of each branch it proposes to acquire. The proposal also is consistent with all other provisions of section 9 of the Federal Reserve Act and the Board's regulations thereunder. Based on the foregoing and all the facts of record, the Board has determined that the proposal should be, and hereby is, approved. The Board's approval is specifically conditioned on compliance by United with all the commitments made in connection with the applications. For purposes of this action, the commitments and conditions relied on in reaching this decision are conditions imposed in writing by the Board and, as such, may be enforced in proceedings under applicable law. The acquisition of the branches may not be consummated before the fifteenth calendar day after the effective date of this order, or later than three months after the effective date of this order, unless such period is extended by the Board or by the Federal Reserve Bank of Philadelphia, acting pursuant to delegated authority. By order of the Board of Governors, effective September 7, 1999. (signed) Robert deV. Frierson Associate Secretary of the Board Footnotes United is a wholly owned subsidiary of United Bancshares, Inc., Philadelphia, Pennsylvania. 12 U.S.C. ï¿½ 1828(c)(5)(A). 12 U.S.C. ï¿½ 1828(c)(5)(B). The Philadelphia banking market comprises Philadelphia, Bucks, Chester, Delaware, and Montgomery Counties, in Pennsylvania; and Burlington, Camden, Gloucester, and Salem Counties and the southwestern portion of Mercer County, in New Jersey. In this context, depository institutions include community banks, savings banks, and savings associations. All deposit data are as of June 30, 1998, and are adjusted for structural changes since that date. Market share data are based on calculations that include the deposits of thrift institutions at 50 percent. The Board previously has indicated that thrift institutions have become, or have the potential to become, significant competitors of commercial banks. See, e.g., Midwest Financial Group, 75 Federal Reserve Bulletin 386 (1989); National City Corporation, 70 Federal Reserve Bulletin 743 (1984). Thus, the Board has regularly included thrift deposits in the calculation of market share on a 50-percent weighted basis. See e.g., First Hawaiian, Inc., 77 Federal Reserve Bulletin 52 (1991). Under the DOJ Guidelines, 49 Federal Register 26,823 (June 29, 1984), a market in which the post-merger HHI is between 1000 and 1800 is considered to be moderately concentrated. The Department of Justice has informed the Board that a bank merger or acquisition generally will not be challenged (in the absence of other factors indicating anticompetitive effects) unless the post-merger HHI is at least 1800 and the merger increases the HHI by more than 200 points. The Department of Justice has stated that the higher than normal thresholds for an increase in the HHI when screening bank mergers and acquisitions for anticompetitive effects implicitly recognize the competitive effects of limited-purpose and other nondepository financial entities. See, e.g., Deutsche Bank AG, 85 Federal Reserve Bulletin 509, 511 (1999); Chemical Banking Corporation, 82 Federal Reserve Bulletin 239, 243 (1996); BankAmerica Corporation, 78 Federal Reserve Bulletin 338, 343 (1992). See 12 C.F.R. 208.6. Voting for this action: Chairman Greenspan and Governors Kelley, Meyer, Ferguson, and Gramlich. Appendix Branch locations to be acquired by United Bank of Philadelphia, all in Philadelphia, Pennsylvania. 1. 1620 Wadsworth Avenue. 2. 2836 West Girard Avenue. 3. 3945-49 Chestnut Street. 4. 1501 North Broad Street.
For immediate release The Federal Reserve Board today announced the execution of a Written Agreement by and among First Security Bancshares, Inc., Lake Park, Iowa; the Security State Bank, Milford, Iowa; and the Federal Reserve Bank of Chicago. A copy of the Written Agreement is attached.
Remarks by Governor Laurence H. Meyer Before the Philadelphia Council for Business Economics, Federal Reserve Bank of Philadelphia, Philadelphia, Pennsylvania September 8, 1999 Q&A on the Economic Outlook and the Challenges Facing Monetary Policy Often the most interesting part of a presentation on a challenging subject--such as the economic outlook and the implications for monetary policy--is the Q&A after the formal remarks. This is especially the case when the audience is well informed and has strong views about the subject--as is always the case with a NABE group. This suggested an innovative approach: dispensing altogether with the formal remarks and going directly to the Q&A. My initial reaction to this insight was: Wow--no paper to write, what a relief! My second response was, on the other hand--and isnï¿½t there always an ï¿½other handï¿½ when economists start talking--I always remind my audiences at the end of my talks that I am now prepared to ï¿½entertainï¿½ their questions. They soon learn that can be rather different from answering them. This distinction is important because there are some questions to which I am not prepared to respond. Today I will try to resolve this tension by proceeding directly to the Q&A, but, at the same time, orchestrating the Q&A by first identifying the questions that I am prepared to answer and then answering them. When I have completed this exercise, you are welcome to offer your own answers to my questions or, of course, raise additional questions for me to entertain. I will ask and answer two sets of questions--the first related to the economic outlook and the second to the strategy for monetary policy. In each case, I will focus on the implications for the conduct of current monetary policy. Let me also emphasize that both the questions and the answers reflect my own views and should not be interpreted as the position of the FOMC or the Board of Governors. I. Outlook Q&A 1. Has there been an upturn in productivity growth? What we can confidently say is that, since late 1995, actual productivity growth has increased and, indeed, moved still higher at the end of 1998 and through early 1999. In addition, this improvement cannot be fully accounted for as a normal cyclical improvement because productivity growth increased further after GDP growth stabilized at about 4 percent. This supports the view that there has been an increase in trend productivity growth. In addition, the recent improved productivity performance is even more remarkable, given that the decline in the unemployment rate to almost a 30-year low might have been expected to result in employment of lower-skilled and hence less-productive workers. Before discussing possible sources and implications of this development, let me clarify what I mean by trend productivity growth. First, I use it to refer to the maximum sustainable rate of growth in productivity, once the economy has reached capacity. Looking backward, it can be measured by de-cycling actual productivity growth or by computing actual productivity growth over periods long enough to wash out the effect of the business cycle. Extrapolating the trend rate forward, it is the rate of actual productivity growth that would be expected if the economy remained at full employment. Trend productivity growth has varied widely over the postwar period, from more than 3 percent in the 1960s to close to 1 percent in the decade preceding this expansion. Because actual productivity is highly pro-cyclical and because there is considerable noise in quarterly observations, it is not easy to discern immediately changes in the underlying trend. This analytical problem has been further complicated in the current episode by unexpectedly weak productivity growth earlier in the expansion. Recent data are consistent with an increase in the productivity trend, but given that the evidence for an upturn in the productivity trend is so recent, considerable uncertainty remains about what the underlying trend will be going forward. Many forecasters, myself included, have been revising upward their estimate of trend productivity growth in response to recent unexpectedly strong productivity growth. Many of these estimates are now about 2 percent or slightly higher. Part of the increase--about ï¿½ percentage point--from the earlier trend of just above 1 percent reflects improved measurement and is not in fact an acceleration of true productivity. Beyond that, an estimate of a 2 percent trend implies the trend rate of productivity growth has increased more than ï¿½ percentage point. I do not, however, want to put too much emphasis on that point estimate, given the uncertainty that I believe surrounds the calculation. But an incremental change of even ï¿½ percentage point is really enormous if it is sustained over several decades. The source of such an improvement is some combination of capital deepening and a faster rate of technical advance. It appears that the ratio of capital services to labor has been rising appreciably in the past few years as a result of the prevailing high level of net investment. This likely accounts for some portion of the recent improvement in labor productivity. In addition, some portion of this improvement may also reflect a higher rate of technical advance, related to the rapid pace of innovation in information and communication technology and other high-tech contributions to production. The implications of such an acceleration in productivity are profound, at least if the increase is sustained going forward for a considerable period. Productivity is, of course, a close relative of real income per capita, a widely used measure of living standards. Higher productivity growth therefore means a faster rate of improvement in living standards. It also means increased tax revenue to the government and enhanced ability of the country to meet its longer-run spending obligations, including Social Security. Of course, at the Federal Reserve, the key question is, What does higher productivity growth mean for monetary policy? First, monetary policy never should target a specific rate of growth in output but rather should adjust to changes in resource utilization rates and inflation. This is because growth itself does not cause inflation; it is excessive utilization rates that are a proximate source of inflation pressures. We sometimes try to capture this by saying that monetary policy should foster the maximum sustainable growth that the economy is capable of achieving. More precisely, once at full employment, monetary policy should accommodate the maximum growth that does not push the economy beyond that point. A monetary policy focused on maintaining price stability has to be careful to avoid stifling unexpected increases in trend growth, specifically by confusing higher trend growth with above-trend growth. In addition, monetary policy cannot accept responsibility for raising trend growth. The only contribution monetary policy can make in this regard is through promoting price stability and thereby reducing the allocative distortions and possible biases against saving and investment associated with inflation. Another reason for avoiding growth as a target for monetary policy is that the preferred growth rate, at any time, depends on prevailing utilization rates. If the economy is at full employment and at the preferred rate of inflation, trend growth will indeed be the preferred outcome. However, if the economy is initially at utilization rates high enough to result in rising inflation over time, the preferred growth outcome will be below trend, allowing some unwinding of the initial excess demand. Therefore, it is always more precise to characterize the monetary policy response in terms of adjustments to changes in prevailing and projected utilization rates rather than in terms of a response to prevailing and projected growth. Unfortunately, it is sometimes difficult to gauge the degree of excess demand associated with a given utilization rate, a subject I will return to later. What are the implications for monetary policy of a step-up in trend productivity growth and, hence, trend real GDP growth? First, such a development would ultimately call for an upward revision to the targets for money growth, so that the money growth targets would remain consistent with an unchanged target for the inflation rate. As a practical matter, however, this is not a serious policy issue because the monetary aggregate targets play only a minor role in the conduct of monetary policy today. At any rate, such an adjustment might be premature today, given the uncertainty about the underlying trend. However, if the apparent higher trend productivity and GDP growth persists, at some point the money growth ranges should be appropriately adjusted. Under the prevailing operational regime of setting a target for the federal funds rate, money growth would automatically adjust to accommodate the higher rate of trend growth, at an unchanged nominal federal funds rate target. Over the longer run, the challenge under an interest-rate regime is to align the real federal funds rate with its new equilibrium value, which is likely to increase with a higher trend rate of productivity growth (due to a higher return on capital that underlies the new equilibrium real interest rate in the economy). The principal challenge to a monetary policy focused on utilization rates is that an unexpected shift in productivity growth, in effect lowers the unemployment rate consistent with stable inflation (the NAIRU) for awhile. This allows the economy to operate at a higher utilization rate without inflationary consequences, at least until the higher productivity is fully anticipated in wage bargaining or until productivity growth stops accelerating. Let me explain the source of the decline in the effective NAIRU. Assume that the increase in productivity is not anticipated and therefore does not immediately raise workersï¿½ real wage demands. With unchanged nominal wage demands and higher productivity, firms will experience a decline in unit labor costs. This will initially boost profits. But competition will quickly force the lower costs to be passed through to consumers in lower prices, lowering price inflation relative to nominal wage change. This decline in inflation, in turn, will put some downward pressure on nominal wage gains. The net result is that an unanticipated increase in the rate of growth of productivity is another example of a favorable supply shock, temporarily lowering inflation. It is useful, nevertheless, to distinguish price shocks--such as declines in energy or non-oil import prices--from productivity growth shocks that have their initial effects on costs rather than on prices. During a transitional period following an unexpected increase in the productivity trend, until productivity growth stabilizes and the higher rate becomes anticipated it will be possible to operate at resource utilization rates beyond what is sustainable over the longer run without inflationary consequences. It is perfectly reasonable to take advantage of this opportunity, as long as care is taken to return to more sustainable utilization rates as the disinflationary force of the upward shift in productivity growth dissipates. Of course, policymakers must also weigh the option of ï¿½opportunistic disinflationï¿½ in such a circumstance--the possibility of reducing inflation toward their long-run target without depressing, even temporarily, resource utilization rates. However, if inflation is already at its target, the option of permitting temporarily higher output and employment clearly dominates. The apparent increase in the productivity trend probably has been an important disinflationary force over the last few years. Some of the benefits in this case have been taken in the form of lower inflation and some in the form of temporarily higher resource utilization rates. An important issue therefore is whether current utilization rates are sustainable, once productivity growth stabilizes. This issue motivated my next question and answer. 2. Is the economy overheated, or is there a threat of overheating? An overheated economy is one operating beyond the point of sustainable capacity and therefore experiencing excess demand in labor and product markets. The importance of the concept follows from the reasonable expectation that excess demand in labor and product markets is a proximate source of higher inflation. So, is the economy overheated? There are two ways to identify such a condition. The first is to find some proxy for excess demand in labor and product markets. We have to satisfy ourselves with proxies because excess demand is not directly measurable. The second is to look for the consequences of overheating, in the form of acceleration in wage gains or prices. Proxies for excess demand are utilization rates in the labor and product markets--such as the unemployment rate in the labor market and capacity utilization in the product market. Capacity utilization is measured only in the ï¿½industrialï¿½ sector, so it is a narrow measure of generalized excess demand in the product market. Nevertheless, it remains an important proxy for the balance of supply and demand in the product market. At any given time, a lower unemployment rate or higher capacity utilization rate implies greater demand relative to supply in the respective market. But we are also searching for an absolute concept, the point of balance between supply and demand in the respective markets that divides excess demand from excess supply--in effect, the origin in a diagram relating inflation to excess demand and supply. Thatï¿½s the difficult part, because we observe only absolute unemployment and capacity utilization rates and have to estimate their respective ï¿½naturalï¿½ rates, the levels consistent with balance between supply and demand in the respective markets. Worse still, the balance point we want to identify is not fixed but shifts over time. Obviously, the more stable the natural rates are, the more useful is the concept in forecasting and policy analysis. The less stable they are, the more uncertainty we will have at any point about the underlying degree of excess demand. We have no choice but to estimate the natural rates from equations that try to capture inflation dynamics. Thatï¿½s all the Phillips Curve is--an equation that relates nominal wage change to expected inflation and excess demand in the labor market, proxied by the unemployment rate relative to an implicit estimate of the NAIRU, derived directly from the estimation of the equation. We have encountered several difficulties in applying this framework in the recent period. First, based on equations for wage dynamics, the evidence suggests some decline in the NAIRU and more uncertainty about its current value. Second, the signals about excess demand coming from labor and product markets--that is from unemployment and capacity utilization rates--have diverged to an unusual degree, making an assessment of the overall degree of excess demand in the economy still more difficult. Third, the economy has been subject to powerful price shocks in this episode--including significant swings in oil prices and exchange rates. Such shocks are a second proximate source of movements in inflation, and their presence further complicates the identification of the signal from excess demand as well as the assessment of what utilization rates will trigger rising inflation in the near term. Fourth, it appears that there has been an upturn in the productivity trend, which, as noted above, acts as a disinflationary force for a period of time, further masking underlying excess demand and further complicating the assessment of sustainable utilization rates. So, where does all this put us in terms of proxies for excess demand? The answer is that it leaves us with considerable uncertainty. Weï¿½re in an environment where reasonable people can disagree about whether or not there is currently excess demand in the economy and, given that uncertainty, whether or not we ought to use evidence based on estimates of excess demand directly in the conduct of monetary policy. It also puts a priority on developing a strategy that takes account of possible shifts in and uncertainty about both productivity growth and the NAIRU. Although it appears that there is excess demand in the labor market, its effect has been diminished by the combination of the absence of corresponding excess demand in the product markets, the residue of the long period of reinforcing favorable price shocks, and the force of the unexpected acceleration in trend productivity. As the favorable price shocks dissipate or reverse and once trend productivity growth stabilizes, there is a risk that excess demand in the labor market will put the economy on a path of rising inflation, unless growth slows enough to unwind the excess demand before inflation begins to move upward. Given the momentum in sales and expectations for a stronger pace of inventory building in the second half, the consensus is that growth will rebound in the second half to trend or above, though we have not yet seen the effects on spending of the rise in bond rates and the flattening of equity prices since the spring. This should help to slow the growth of domestic demand. Although there is some risk that growth could remain above trend and therefore aggravate any initial excess demand, a major concern remains that the prevailing balance of supply and demand in the labor market might put upward pressure on inflation, even if growth slows to trend ahead. Let me briefly comment on the second indicator of excess demand. Instead of trying to measure the state of balance or imbalance between supply and demand, we could focus on observing the consequence of excess demand--specifically, increases in prices--or, for a given initial inflation rate, increases in inflation. Unfortunately, because of supply shocks, we cannot always make this identification so easily. I read the recent inflation data as at least suggesting that the underlying inflation rate is stabilizing, after a period of decline, without any evidence of a broad-based upturn in inflation. Nominal wage increases have moderated since the middle of 1998, likely reflecting the decline in inflation associated with a combination of favorable supply shocks, including the unexpected increase in the productivity trend. Some of the most recent data suggest that the growth of nominal compensation is no longer declining, and there are hints in the data and in anecdotes that wage pressures may be building. How should monetary policy respond to increasing utilization rates? Should real interest rates be held constant until utilization rates increase above some threshold, for example, or should real interest rates be more smoothly pro-cyclical, gradually increasing in response to rising utilization rates? That is simply a question of what systematic policy response works best to promote the dual objectives of monetary policy: promoting price stability and damping fluctuations around full employment. My judgment is that a regime in which there is a gradual, systematic pro-cyclical response of real interest rates is the one that produces the best trade-off between inflation and output variability. This is the kind of response embodied in the Taylor Rule, for example, though in practice, as we have seen, implementation of this approach is complicated by uncertainties about the level of the NAIRU or its cousin, the output gap. I will return to this problem when I take up the question of how preemptive monetary policy can and should be. 3. Are equities overvalued, so that the economy is threatened with an asset market bubble? This is perhaps the most-asked question I get, so I thought I would preempt you and answer this one directly--though you may decide that I chose to entertain this question! Equity prices have increased enormously over the past four years, to levels that challenge previous valuation standards. Let me make clear at the outset that I honestly do not know whether or not equities are fairly valued or overvalued. I have nothing to share with you about this question. What I do want to share with you is how the equity market fits into my thinking about monetary policy. Those of us fortunate enough to attend this yearï¿½s Jackson Hole Conference, sponsored by the Federal Reserve Bank of Kansas City, had plenty of opportunity to think about and discuss this issue. Most important, policymakers should reflect the higher value of equities in their forecasts for aggregate spending and adjust monetary policy as necessary to remain consistent with the broad objectives of monetary policy. The key here is to remain focused on broad macroeconomic performance, responding indirectly to the movements in equity prices--whether the higher value of equities appears driven by fundamentals or otherwise--rather than to use policy directly to influence the value of equities. If policy is disciplined in pursuit of its broad macroeconomic objectives, this will reduce (though not eliminate) the prospect that equities will become significantly overvalued. There are, nevertheless, several steps that policymakers might consider if they have suspicions that equities might be overvalued. They could build some assumption about a market correction into their forecast. That would seem reasonable but could be a mistake. Specifically, it could discourage them from tightening in response to robust demand driven in part by past increases in market values, counting instead on an autonomous correction in equity values, the degree and timing of which has to be extremely uncertain. On the other hand, given such suspicions, policymakers should be alert to the potential that a tightening of policy could have a disproportionate effect on demand, if it induces a reassessment of market fundamentals. This does not mean, however, that policymakers are trapped and cannot respond to robust demand and rising inflation risks. It does suggest that they should appreciate that there will be more uncertainty about the magnitude of the effect of a given policy tightening and that the effect could be disproportionately large. While the stock market should not be a target for monetary policy, policymakers should pay attention to the signals from the market. An aggressive rise in equity prices can be a sign of highly favorable financial conditions in general and a highly accommodative monetary policy in particular. If this occurs when the economy is already near potential, policymakers should re-evaluate the appropriateness of their policy setting in terms of promoting price stability and damping fluctuations around full employment. Finally, policymakers should be alert to the need to respond appropriately to a significant market correction. It is important to note that policymakers should not target the level of equity prices on the way down any more than on the way up but, in both directions, should take the movement in equity prices into account in their forecast and hence in the setting of monetary policy. II. Monetary Policy Q&A 1. Does prevailing uncertainty about the structure of the economy and the recent forecast errors diminish the ability of monetary policy to be preemptive? Without doubt. But that does not mean that there cannot be a preemptive element in monetary policy. It means only that policy is likely to be less preemptive--and hence more reactive--than it otherwise would be. The critical questions are just how preemptive can and should monetary policy be today? Let me begin to answer this question by defining what I mean by reactive and preemptive policy approaches. First, policy is reactive if it responds only to the incoming data and preemptive if it also responds to a forecast. This is the distinction between backward-looking and forward-looking policy. Second, with respect to inflation, policy can still be preemptive if it responds to incoming data on utilization rates, given the link between current utilization rates and future inflation. Of course, the degree of confidence that policymakers have in this link (and specifically in their measure of excess demand) will determine how preemptive they are prepared to be. The greater uncertainty about the level of excess demand should, I believe, diminish the aggressiveness with which monetary policy responds to changes in utilization rates. The difficulty in forecasting should, in addition, encourage more emphasis on responding to incoming data and diminish (though not eliminate) the role of the forecast in the policy decision. This is sensible and prudent. To sever this relationship of monetary policy either to incoming data on utilization rates or to the forecast altogether, however, would remove the key elements of preemptive monetary policy with respect to containing inflation and leave policy entirely reactive. If the uncertainty were great enough, this would be a reasonable response. But I do not believe that such an extreme position is warranted. There are, however, a couple of constructive policy responses in light of prevailing uncertainties about the level of excess demand and the forecast. First, policymakers could update their estimates of the NAIRU and the output gap (assuming, in the first place, that they find these concepts useful, as I do) in light of realizations of unemployment, output, inflation, and other variables. This has, in fact, been one response that many, including myself, have taken in response to recent developments. In following this approach, policymakers would become less responsive to declines in the unemployment rate, to the extent that estimates of the NAIRU are revised downward as unemployment and inflation decline together. Second, policymakers could attenuate the response of the real federal funds rate to declines in the unemployment rate in a region around their estimate of the NAIRU. But once the unemployment rate gets far enough below (or above) the estimated NAIRU so that confidence returns that the labor market is experiencing excess demand (or supply), then the more normal response of real interest rates to incremental declines in the unemployment rate would again become appropriate. But you have to be careful about overdoing caution as well as overdoing aggressiveness. If you take care to adjust your estimate of the NAIRU and the output gap in response to incoming data, you would be unwise to ignore these revised measures of the unemployment and output gaps in setting policy. 2. What is the meaning of symmetric and asymmetric directives? I think that it has become clear that symmetry/asymmetry is a subtle concept. I believe that there are two interrelated dimensions of the so-called tilt or bias in monetary policy. The first dimension relates to the balance of risks going forward. The second relates to the probability of a near-term policy change. The tilt provides the market with an indication of the Committeeï¿½s balancing of the risks related to emerging excess demand or supply and inflation going forward and hence in what direction policy is more likely to move. In a symmetric directive, the risks are viewed as evenly balanced, so that the next rate increase could as easily be up or down. In an asymmetric directive, the risks are viewed as tilted in one direction or the other, so there is, for example, a greater likelihood of an increase than a decrease in rates. A symmetric directive also indicates little prospect that a near-term move will be required if the economic outlook evolves roughly as expected. An asymmetric directive, in contrast, alerts the market to the greater possibility, though not the certainty, of a move in a particular direction over some near-term policy horizon. At the December 1998 meeting, the FOMC decided, going forward, to announce the tilt and explain the reason for such a policy change in those cases where the change in the Committeeï¿½s views of the balance of risks was ï¿½significantï¿½ and when announcing this change to the public was viewed as ï¿½important.ï¿½ This made the announcement of the tilt, in effect, another policy tool because an announced change in the tilt would move market rates, though to a lesser degree than a change in the rate itself. This decision should be understood as an effort to increase the transparency of monetary policy and to allow the Committee to communicate more clearly its views of the balance of risks and the prospects of further policy actions going forward. Underlying this effort is the view that financial markets operate more efficiently when they have more complete information and the preference for signaling markets about prospective policy actions rather than surprising markets. The early experience with its use suggests that announced adjustments in the tilt sometimes have unexpectedly large effects on financial markets and on the reaction of markets to subsequent data or statements by FOMC members. It has also become clear that it is not easy to communicate some of the subtleties and complexities of monetary policy intentions in a single word. When the signaling is, as a result, imperfect and the Committeeï¿½s intentions are misperceived, market rates may move inconsistently or faster than is justified by the balance of risks and the likely course of policy. The recent experience suggests that the use and announcement of tilts should be viewed as a ï¿½work in progress,ï¿½ rather than a well-tuned and final product. 3. Does the response of the bond market to evolving economic developments reduce or eliminate the importance of activist monetary policy? A theme we sometimes hear is that the FOMC can take a permanent vacation, leaving the conduct of monetary policy to the bond market. I have heard this referred to as the ï¿½gyroscopeï¿½ theory, in that it portrays the bond market as the gyroscope of the economy, sensitively responding to developments so as to stabilize the economy. Were it only so simple! It is sometimes said at the Federal Reserve that when we look at the bond market we are really looking at ourselves in the mirror. This means that market participants are responding to the data, their changing forecast, and their understanding of our policy reaction function. Long-term rates are, after all, based on current and expected future short-term rates. Expected future short-term rates, in turn, are very much a function of the Fedï¿½s policy response. What this means is that when bond rates rise in the expectation of future monetary policy tightening (that is, in the expectation of higher short-term rates), we have a choice. We can confirm the expectations by tightening, preserving the higher bond rates. Or we can contradict those expectations by leaving policy unchanged, likely resulting in some reversal of the initial movement in bond rates (as the bond market comes to better understand our policy reaction function). There is, however, also the possibility that the failure to tighten when such a move is widely expected may leave in place higher long-term rates (or raise them further), if our failure to act is viewed as inconsistent with our commitment to price stability. Such preemptive pricing in the bond market is the private sector analogue to preemptive moves in the federal funds rate. When the bond market is correct, its preemptive pricing should be rewarded by the expected movement in the federal funds rate. To fail to so reward it would be to undermine the ability and willingness of the bond market to engage in such preemptive pricing. On the other hand, when the bond market is viewed as having inappropriately built in expectations of higher rates, monetary policy ought to provide an anchor, in the form of an unchanged funds rate, to which the bond market can return as incorrect expectations are unwound. The bottom line is--no vacation for the Fed. But preemptive pricing in the bond market can make monetary policy more effective by speeding the response of long-term rates to changing economic conditions. There is a potential synergy here between monetary policy and the bond market. The more transparent is monetary policy, the more effective will be preemptive pricing in the bond market. The more effective is preemptive pricing in the bond market, the shorter the lag from a change in monetary policy to the effect on aggregate demand, and hence the more effective is monetary policy. III. Conclusion Sum it all up! First, clarify the logic of the recent Fed tightenings and then provide some insight into the prospects for near-term monetary policy in light of the comments above on the outlook and the strategy of monetary policy. In my view, the current monetary policy problem has two dimensions. The first dimension is to adjust the federal funds rate so that it is appropriate in terms of prevailing utilization and inflation rates, with appropriate regard to the uncertainty about the measurement of utilization rates. I have called this the reassessment issue because it involves a reassessment of the desirability of the full amount of easings implemented last fall. The second dimension involves the adjustment in the funds rate going forward, in response to incoming data and changes in the forecast. The easings last fall were implemented at a time of sharp dislocations in financial markets and sharp downward revisions to the forecasts of both global and U.S. growth this year. I view the recent tightenings as the partial reversal of the earlier easings, in response to the reversal of the factors that motivated the easings. Financial markets have clearly improved. The global economy looks stronger and the United States is now projected to expand this year at a multiple of the rate projected last fall. In determining how much of the easings to reverse, one also has to take into account the decline in core inflation since last fall, as well as uncertainties about translating the current utilization rates into measures of excess demand. What can and will I say about monetary policy going forward? I always like to point out that I am prepared to be quite explicit about the course of policy going forward. The answer to the question, what policy changes do I expect going forward, is simple: ï¿½It depends.ï¿½ Specifically, it depends on the incoming data and the evolving forecast.
Remarks by Governor Roger W. Ferguson, Jr. Before the The National Economists Club, Washington, D.C. September 9, 1999 Transparency and Responsibility in Monetary Policy It is a pleasure to address the National Economists Club. I understand that the Club is an educational organization with the goal of encouraging and sponsoring the public discussion of significant economic issues. This is a worthy goal, and I am pleased to support it. Today I would like to spend a few minutes discussing the issue of transparency and responsibility in monetary policy. I am sure you will all agree with me that the first task of the monetary authority is to get the right policy setting to achieve national economic objectives. However, that task is complemented by and intertwined with another -- telling the public, both the general public and active market participants -- what the central bank is trying to accomplish, what forces it sees impinging on meeting its goals, and how it might cope with those forces. The latter job of communication has received increasing attention in recent years as central banks worldwide have moved toward greater "transparency" in their operations, objectives, and economic assessments. In my view, this trend is healthy and should be extended when it would be useful. But we must be mindful that this new openness can have an impact on how effectively central banks pursue their primary mission -- making the best possible monetary policy. I want to take a few minutes to review the actions taken at home and abroad toward greater transparency, and assess some of the issues that arise as central banks contemplate further steps in this regard. Of course, the views expressed here are my own and should not be interpreted as the position of the FOMC or the Board of Governors. The Recent Trend Towards Openness and Transparency Let me start this discussion by focusing on the United States. The Federal Open Market Committee in recent years has increased in several ways the transparency with which policy decisions are made and implemented. In 1994, the FOMC began formally announcing, immediately after any meeting in which a policy action had taken place, the change in the targeted federal funds rate and a brief rationale for the decision. Until that time, changes were "signaled" through open market operations. It sometimes took several days before market participants and the broader public were certain that a new policy setting was in effect. In that earlier regime, the public rarely received a prompt explanation for the change. This year, the FOMC went a step further and began a policy of communicating major shifts in its views about future policy even when the current policy setting has not changed. The idea is that providing more information about the Committee's views of the economic outlook may allow financial market prices to reflect more accurately the likely future stance of monetary policy. The Committee made its first such announcement after the May meeting. Both the markets and the Committee are learning to live with the new disclosure policy, and our experience will help to guide any subsequent modification of these practices. As technology continues to reduce the costs of disseminating information, and as Americans participate in financial markets in greater numbers, the Federal Reserve has endeavored more broadly to improve communication with the public about its views and actions. A major component of that effort is our website. The texts of policy announcements, testimony, speeches, and minutes of meetings are available through this medium simultaneously with their initial release or presentation. As a consequence, the public and market participants can form their own views on the implications of these texts, and do not rely solely on press reports for interpretation. These changes have been part of a broader trend among central banks in the industrialized world. For example, in 1997 the Bank of England took a number of steps to increase the openness and transparency of its policy making. It now announces monetary policy changes immediately after meetings, provides press releases or conferences at pre-announced times following monetary policy meetings, and has shortened considerably the lag time in releasing its meeting minutes. The press conference announcing its quarterly inflation report is also now televised. Others have become more transparent as well. In 1998, the Bank of Japan's governing law was revised. The Bank now announces policy changes immediately after meetings, publishes minutes of the meetings, and produces two formal reports each year on monetary policy, which it explains to the Diet. The new European Central Bank incorporates a number of these aspects of openness. It too announces policy changes immediately after meetings, along with a detailed rationale in a press release or conference. Furthermore, it intends to sometimes provide occasional post-meeting announcements that give a sense of the direction of future policy when the current policy setting has been left unchanged. The ECB also publishes a Monthly Bulletin, which provides an assessment of monetary, financial, and economic developments, and explains monetary policy decisions of the Bank. Openness and the Effectiveness of Policy I believe that there are two main forces driving this move towards greater openness and accountability. The first is that openness may improve the effectiveness of monetary policy. The Federal Reserve controls a very short-term interest rate -- the federal funds rate -- but it is the longer-term interest rates at which businesses and households borrow to finance spending on capital goods, homes, and durable goods which matter most for the economy. Those longer-term rates reflect expectations of future short-term rates, as well as a premium for uncertainty. If the monetary authority can be clearer about what it is doing now and plans to do -- not in the sense of setting future moves in stone, but rather in terms of explaining risks that might influence future policy -- then market participants can improve their expectations of future short rates, and possibly reduce the premium for uncertainty. Both of these changes ought to bring the rates that matter most for the macroeconomy into closer alignment with the intentions of monetary policymakers. Openness on the part of the central bank does not guarantee that markets will accurately forecast interest rates, however. Markets can err because the economy changes or because, despite greater openness, they don't fully understand the central bank's intentions. One aspect of this seems to be a tendency to extrapolate the recent trend of rates into the future. For example, after a series of tightenings in 1994, the federal funds rate topped out at 6 percent, and short-term Treasuries remained below that level for virtually the entire subsequent five-year period. During this period of tightening five-year Treasury yields soared to nearly 8 percent, a rate never fully justified by the extent of firming in short-term rates, suggesting that greater openness by policymakers does not preclude the natural uncertainty involved in interest rate forecasting. I might note that the decline in the usefulness of the monetary aggregates may have contributed to the type of openness and transparency in use today. The regime targeting money growth was, in a sense, a transparent one -- the targets were contained in the biannual Humphrey Hawkins report and widely known, and money numbers were published weekly. As the monetary aggregates became less reliable and consequently receded in importance, maintaining openness called for increased communication about the rationales for discretionary interest rate policy. I do not wish to leave the impression that market participants accurately anticipating policy actions is sufficient for central bankers to play their assigned role. While a tighter link between monetary policy and long-term interest rates may lessen the need for the central bank to move its policy instrument as much or as quickly, it should be clear that markets are not a substitute for monetary policy action. Market movements in response to anticipated central bank actions must be validated when they are, in the view of the policymakers, correct, in order for the market to continue to reflect the intentions of policymakers. Openness, Independence, and Democracy A second force behind the trend towards openness and transparency is the increased importance attached to, and increased prevalence of, independent central banks. The notion that central banks need to be insulated from political pressures, especially regarding the financing of national budgets, has been embraced and implemented throughout much of the developed world. And, in many cases, openness and transparency on the part of the central bank have been something of a quid-pro-quo for that independence. Here I am referring to independence of instrument rather than independence of goal. It is appropriate that in a democracy voters' representatives should decide the central bank's ultimate goals, as they have in the U.S., for example, by providing the Federal Reserve with a mandate to pursue stable prices, maximum employment, and moderate long-term interest rates. The main reason that central banks need to be free of political pressure in using their instruments is that effective monetary policy requires a distant time horizon. Because changes in monetary policy generally affect output well before inflation, policymakers with nearer time horizons could be tempted to favor output in excess of capacity, at the expense of higher inflation in the future, and the well-known and inevitable disruptions that accompany it. Most countries have come to believe that the solution to this problem lies in providing the central bank with independence in choosing settings for its policy instruments, and with an institutional structure protective of that independence as well. For example, Federal Reserve governors' terms are long and staggered, the Federal Reserve's budgets are not subject to the congressional appropriations process, and it has no obligation to purchase or support the prices of Treasury debt. Along with their independence, central banks are required to account for their decisions in various ways. This too is appropriate in a democracy: the public has a right to know what an unelected body as important as the central bank is doing, and why. To some extent, accountability is fostered by openness and transparency because letting people know what you are doing gives them the tools to hold you accountable. The Federal Reserve's accountability is collective, and it is often centered in the office of the Chairman. For example, while the minutes of FOMC meetings generally refer to views of "members of the Committee," the Chairman represents the Committee in testimony and answers questions about the semi-annual Monetary Policy Report to Congress under the Humphrey-Hawkins law. When a committee makes policy, a process involving consensus building and compromise, collective rather than individual accountability seems preferable because policies are not simply weighted averages of members' wishes. Having outlined some of the advantages of transparency and accountability in central banking, let me note that there are limits to how open central banks can practically be. The Federal Reserve, like many but not all central banks, releases minutes after its next regularly scheduled meeting. The ECB, representing a potentially more diverse set of national constituencies, has decided neither to release minutes nor to publish the voting behavior of its individual members. In this way, it hopes to avoid pressures to act in what may be perceived to be a national interest rather than in the interest of the euro area as a whole. Similarly, most central banks omit committee members' names in the discussion portion of the meeting minutes, for fear that attributing remarks during the discussion would undermine the free ranging nature of meetings and encourage members to arrive with prepared positions. In fact, many members of the FOMC now do rely on prepared remarks for certain segments of the meetings, in part due to the greater transparency that they face, given that in the mid-1990s transcripts of meetings started to be released after five years. To the degree that policy decisions benefit from the give and take of opinions and evidence at meetings, something less than full openness to public scrutiny is required to assure the quality of monetary policymaking. Price Stability, Inflation Targeting, and Credibility While central banks' mandated goals often include many things, price stability has emerged as the preeminent one, either formally -- as in inflation targeting countries or the Maastrict Treaty establishing the European Central Bank -- or informally, through a greater understanding of how economies work. That is, policies that foster low inflation are more likely to deliver high employment and maximum sustainable output growth as a by-product, while policies which aim directly to keep employment and output high are more likely to result in poor inflation performance. To paraphrase Chairman Greenspan, the economy works best when inflation is so low that businesses and households do not have to take it into account when making everyday decisions. In the short run, however, difficult choices may be required about the timing and magnitude of policy actions consistent with achieving price stability. One approach to such difficult choices, as I have mentioned, is to mandate a quantitative inflation target for the central bank, or to have the central bank select an inflation target that it must endeavor to achieve. A number of other countries, like Britain, have adopted inflation targeting monetary policy regimes. In many cases, that shift has been part of a move toward greater independence of central banks. Such regimes might lend themselves to both openness and accountability. A single quantitative goal might focus explanation of policy actions and provide a simple yardstick for assessing performance. However, this approach faces at least two sets of important issues. The first is what to do about supply shocks, like large increases in oil prices, which tend to increase both inflation and unemployment. Many of the countries that have adopted inflation targeting avoid having to respond to the first-round effects of these disturbances by targeting a "core" price index that excludes goods like food and energy that are often subject to shocks. However, increases in raw materials prices will still tend to pass through to higher prices for other goods, and thus raise core inflation, perhaps above the target rate or range. In such cases, bringing inflation back down rapidly may entail high costs in terms of unusually elevated output gaps and unemployment rates. A related issue is which other price changes, beyond those induced by supply shocks, the central bank should ignore in inflation targeting. Some central banks use indexes that exclude certain prices, such as mortgage rates, in order to avoid the perversity of tightening policy feeding into higher measured inflation for the targeted index. Occasionally, central banks effectively make special adjustments for other price changes, such as excise tax hikes, that clearly have an impact on "core" price indexes but do not signal an imbalance between demand and potential supply that might give rise to broader inflationary impulses. Inflation-targeting regimes may allow some consideration of real-side costs either by specifying relatively long adjustment periods, to allow a high probability that the central bank can bring inflation down to the target within the allotted time, or by including "escape clauses" that grant temporary exemptions for large supply shocks. The question then becomes whether the use of either of these elements of flexibility maintains the credibility of the central bank better than a system of multiple objectives, like that of the Federal Reserve. The longer the policy timeframe, the less content there is to an inflation-targeting regime, and presumably the lower is its credibility. Similarly, the more often a central bank has to declare emergencies, use escape clauses, or otherwise allow price increases to go unchecked, particularly when the rationale for such decisions is difficult to communicate to the public, the less credibility it will have. Why is credibility so important? There is a large theoretical literature on credibility in central banking, arguing for the most part that central banks need independence to be credible inflation fighters, to be able to disinflate at a lower short-run output, and social, cost. However, the evidence does not support a particular empirical relationship between credibility and costs of disinflation, nor one between independence and costs of disinflation. In the real world, there are two reasons why central bankers still prize credibility, even if it cannot be shown to reduce the costs of disinflation. First, central banks need the latitude to change operating policies when circumstances warrant -- such as de-emphasizing growth rates of monetary aggregates when their velocities become unstable -- without the markets fearing that a central bank's commitment to the goal of price stability has been compromised. Second, credibility is an asset during a financial crisis, when the central bank may need temporarily to take extraordinary measures. For example, it was helpful in the fall of 1998 for the markets to understand that the FOMC's policy easings represented a response to a financial crisis, rather than a reduced concern about inflation. The second issue with inflation targeting is what rate or range of inflation to choose. A rate above zero (in terms of published inflation indexes) may be appropriate, for a couple of reasons. One is that it may be difficult to conduct monetary policy at levels of inflation near zero because nominal interest rates -- including the central bank's policy rate -- cannot be below zero. Many observers believe that monetary policy in Japan is constrained in this way, with its current policy setting an essentially zero short-term interest rate. A second reason is the statistical biases in inflation indexes, resulting from slow inclusion of new goods, improvements in quality, substitution from more expensive to cheaper goods, and other factors. In the United States, the 1996 Boskin Commission report, estimated that these biases may have overstated true inflation in the consumer price index at that time by three-quarters to one-and-one-half percentage points per year. In such an environment, a central bank mandated to pursue price stability can be flexible according to the circumstances, while one with a numerical target may need to obtain formal modification of its objectives. Conclusion The movement towards more central bank transparency, independence, and accountability that has taken place over recent years constitutes an exciting and welcome development. In striking the appropriate balance going forward, the advantages of further steps must be carefully weighed against the risks of impairing the deliberative process or indeed destabilizing financial markets. By proceeding cautiously, but keeping the goal of optimal transparency and accountability in mind, I am confident that central banks will continue to contribute to the national welfare in their respective countries.
Remarks by Governor Edward M. Gramlich Before the BusinessLINC and Small Business Technical Assistance Conference, Washington, D.C. September 15, 1999 It is a pleasure to address the BusinessLINC conference this afternoon. There have been many successful link projects between financial institutions and small business, and this conference should be instrumental in publicizing these successes to a wider audience. Before getting into questions of small business growth, let me focus on the big picture. Growth of all American business, small and large alike, depends on a healthy economy. This in turn is promoted by sound fiscal and monetary policies. For fiscal policy, this means continued efforts to preserve at least a part of the budget surplus to contribute to national saving. For monetary policy, it means continued efforts to stabilize prices and to keep unemployment as low as possible consistent with stable prices. For the past few years, policies have been sound in this sense, and it is important to maintain this posture. Small Business and the Economy Turning now to the topic of the conference, small businesses are extremely important to the health of the United States economy. There are approximately twenty-three million small businesses in the United States, representing more than 99 percent of all firms. Collectively, these firms generate almost one-half of the sales revenues of all U.S. companies and employ more than half of the private-sector workforce. Small firms provide the first job for an even higher share of the nation's workforce. Small firms also are often at the leading edge of technological innovation, fostered by a competitive need to experiment with new ideas and product development. The Small Business Administration (SBA) has estimated, for example, that small businesses help generate almost half of all the innovations in our economy and provide nearly 30 percent of our high-technology jobs. The vigorous small business community makes the American economy more flexible, enhancing our capacity to respond to the fast pace of change in an increasingly technological world marketplace. Most of our largest, multinational corporations depend on a vast array of reliable, agile suppliers that help make possible and help maintain America's competitive position in international markets. A healthy, growing small business sector makes our economy more nimble, better able to respond to new market trends and needs, and ultimately more productive as the results of small business experimentation and innovation weave their way throughout the wider economy. Small businesses also contribute to local community and economic life. It is at the local level that the often rough and risky game of small business development, job creation, innovation, and productivity improvement plays out. Businesses are created, merged, dissolved, or often simply fail. It is here that small retailers, service providers, and suppliers get their start and provide employment opportunities--often the first jobs for many--for workers with a broad range of skills and experience. It also is at the community and neighborhood level that the process of family and community asset-building takes shape and where entrepreneurship leads some, literally, out of poverty. In poorer neighborhoods and communities, small business development provides everyday retail and commercial services and employment opportunities. The development and growth of small firms at the neighborhood level often contribute to community revitalization, help stabilize real estate values, and provide impetus for additional investments by business, government, homeowners, and homebuyers. Local entrepreneurs often become valued informal community resources or leaders in more formal civic and political organizations. In many communities, they represent the heart of the way that local economic vitality is created and maintained. Financial Institutions and Small Business Given their contributions to our economic life, both nationally and locally, it is not surprising that many traditional financial institutions view the small businesses sector as a primary market for their loans and financial services. For community bankers, the small business market is their staple. Virtually all of their commercial loans are small business loans, and their small business customers also yield deposit, mortgage, financial counseling and, more recently, insurance and investment relationships. Many of our larger financial institutions serving both regional and national markets have developed highly aggressive marketing programs and new loan products and services for the small business market. Taking advantage of economies of scale and new technologies, such as credit scoring, many larger institutions have developed the capacity to make smaller credits available using processes not unlike those used in consumer lending. Specialized units within large banks can now offer products and services tailored to the highly diverse small business market--very small loans based on credit scores and limited underwriting; term loans, with or without third-party guaranties; asset-based loans; and various forms of equity financing for rapidly growing small companies. The competition for small business loans and services also has been spurred by the growth and diversity of nonbank lenders, from companies as large as GE capital to small community-development financial institutions and microloan networks that feature revolving loan funds and intensive technical assistance for very small firms. Other Financial Assistance Programs Given the importance of small businesses to our economy and our communities, it is hardly surprising that a broad array of other financial and technical assistance programs have been created to foster small business growth. In the early days, economic developers, which included utilities as front-line participants, focused on the development of public facilities as an impetus for business expansion. "Build it and they will come" was the operative small (and large) business development scheme. To expand their customer base, utilities often employed economic development professionals to work with smaller firms as well as larger industrial companies. Later, loan guarantee programs at both the federal and the state levels were created to help private lenders reduce risks, especially those associated with longer-term loans to small firms. State-administered tax-exempt revenue bond programs provided lower-cost, long-term credit for small businesses. In recognition that longer-term credit was often insufficient, various equity grant and technical assistance programs were created. These include the programs of the Economic Development Administration, the Appalachian Regional Commission, the Community Development Block Grant program and, more recently, federal and state enterprise or empowerment zone programs. Additionally, over the last decade, a seemingly disparate group of nonbank small business finance organizations has evolved into a significant industry. This group includes community and neighborhood-based nonprofit groups with revolving loan funds, quasi-public economic and business development corporations, SBA-regulated certified development companies, multi-investor private-sector intermediaries, small business investment companies, and microenterprise finance organizations. Collectively, these and other types of organizations provide virtually every kind of financing a small business might need: microloans, short-term working capital, long-term loans, asset-based loans, loans for start-up operations, and various forms of equity financing. Finally, a number of educational, training, and technical assistance programs are offered for new and experienced entrepreneurs, operated by many of these nonbank groups and specialized technical assistance providers. These may include SBA- and university-sponsored small business development centers, a variety of federal and state funded programs, microenterprise assistance organizations, community development financial institutions, and small business trade associations. Linking Small Business with Appropriate Assistance This multiplicity of approaches, resources, and tools for assisting small business development reflects our diverse economy and, perhaps, an American penchant for competition and alternatives. But for a small business seeking credit and advice, this diverse system can become a confusing maze of financial institutions and entrepreneurs. Small or start-up firms in economically distressed areas may never have heard of the SBA, small business development centers, or the local microloan network. They may have a business idea, perhaps some small savings or a credit card or second mortgage, and in the spirit of entrepreneurship, they start up, making key decisions for which they may be unprepared. Finding ways to help small businesses navigate this environment successfully is a considerable challenge, both for financial institutions and for the small firms they serve. Steering the right course is also important for financial institutions. It is in their best interest to have well-informed and prepared small business customers. This helps the customers become sound borrowers with growth potential. Growing firms help create economic value in their communities and additional profitable business relationships for their banks. While providing advice and assistance to inexperienced small business customers and loan applicants can be costly, many institutions do regularly provide such advice and assistance. Financial institutions can help screen information resources and locate assistance. This can be done through a mentoring relationship with a small business customer or supplier, through the normal loan application process, or by the selection of a knowledgeable and reliable third-part broker organization in the community to help prospective small business borrowers. For community bankers in small towns, such assistance may take the form of informal advice to bank customers or occasional free seminars sponsored by the bank on such subjects as business tax and estate planning or a business development plan. Community bankers often take pride in knowing their customers, their particular needs and capacity, and the way to tailor financial technical assistance to meet those needs. Many larger institutions, several of which you will hear from at this conference today, have invested considerable resources in operations designed to provide hands-on consultation and assistance to large numbers of small firms. As a business development strategy, the up-front costs of these activities are defrayed by business relationships with growing firms that become long-term customers of the bank. Another common strategy is to help create or fund a specialized small business development unit or subsidiary organization within the bank. Small firms in need of assistance are referred by other units within the bank or by business development organizations within the community. Such specialized bank units can usually provide a full range of financial products and services, advice, and technical assistance to small businesses as they grow. Finally, many institutions have chosen to support third-party organizations that provide technical assistance to small firms, especially those just starting up. Usually these third-party groups are experienced nonprofit organizations selected and funded because of their expertise and track record. Many are also multi-faceted financial intermediaries, such as community development financial institutions, that can provide specialized gap financing. Continuing Challenges No matter how businesses and institutions develop, we face a number of common challenges. First, there may be a need for significantly more research about how small businesses and entrepreneurs obtain financial and business development information. To the extent that research can help pinpoint the most cost-effective techniques for small business growth, it can help stimulate greater interest in technical assistance for small businesses. Second, the diversity and fragmentation of technical assistance resources and organizations often makes it difficult to identify the most appropriate and effective help for particular small businesses. Consequently, for most institutions that chose to use third-party broker organizations, considerable time and effort is often needed to identify and assess the particular expertise and capacity of each available alternative lender or technical assistance intermediary. Further, each institution seeking to use third-party intermediaries may have to conduct its own analysis of each. As in most economic relationships, lack of information makes for an inefficient market. Consequently, a valuable local activity may be a coordinated effort to organize technical-assistance providers and make available on a centralized basis regularly updated information about their specialties, activities, and track records. This makes sense both for entrepreneurs looking for assistance and for financial institutions seeking partners for their small business development activities. Third, most financial institutions are painfully aware of the need to remain on the forefront of technologies that enhance their own business capacity and improve productivity. Use of technology for most businesses, no matter how small, may be essential to their current and future long-term success. To the extent that institutions mentor, assist, or broker assistance for small firms, their small business assistance programs will need to consider how firms can use new technologies effectively. Finally, many financial institutions themselves need to learn more about the tools and techniques available to help small businesses grow. The Federal Reserve, as well as the other banking agencies co-sponsoring this conference, will continue to provide institutions with appropriate information and resources about the tools and techniques available.
Board of Governors of the Federal Reserve System Federal Deposit Insurance Corporation National Credit Union Administration Office of the Comptroller of the Currency Office of Thrift Supervision Federal Regulators Report to Nation on Y2K Progress Made by Banks, Thrifts and Credit Unions WASHINGTON -- With only 106 days remaining before the new century begins, leaders from the five federal agencies that regulate banks, thrifts and credit unions joined together today to report to the nation that insured financial institutions are prepared for the Year 2000 date change. The agency officials said that 99.7 percent of the nation's insured institutions are now rated satisfactory -- the highest rating given for Y2K readiness. The few institutions that are not yet rated satisfactory are receiving very close regulatory attention, they added. Appearing at today's press conference at the National Press Club to discuss financial industry readiness were: John D. Hawke, Jr., Comptroller of the Currency; Donna Tanoue, Chairman of the Federal Deposit Insurance Corporation; Edward W. Kelley, Jr., Member, Federal Reserve Board ; Ellen Seidman, Director, Office of Thrift Supervision; and Norman E. D'Amours, Chairman, National Credit Union Administration. The regulators stressed that they had closely evaluated the Y2K readiness of each insured financial institution. "We're not just taking their word for it," said Mr. Hawke. "Federal examiners have conducted Y2K examinations in each insured financial institution at least twice, and in some cases, three, four or more times. The largest banks have received continuous Y2K oversight.'' The results, Mr. Hawke said, show how effective the efforts of regulators and financial institutions have been. "Right now, 99.7 percent of all federally supervised financial institutions have finished their renovations and tests of their systems -- not just the systems that house their personal records and run their elevators, but the systems that bank customers rely upon for access to their funds." The remaining few, he said, "are receiving intensive, on-site supervision to ensure that they, too, will experience no disruptions of the systems their customers depend upon when the long anticipated day arrives." Consumers can also rely upon the guarantees provided by the FDIC, which oversees the insurance funds that back deposits in banks and thrifts, and the National Credit Union Share Insurance Fund, which protects credit union depositors. "There are few guarantees in life, but the FDIC and NCUSIF offer one of them," said Ms. Tanoue. "No one has ever lost a cent in a federally insured account. And no one will." Mr. Kelley explained the steps the Federal Reserve has taken to ensure bank and thrift customers continuous access to their funds. Among them, the central bank has additional currency inventory and it has created a special borrowing facility to ensure banks and thrifts have access to funds if they need it for their Y2K preparations. "We have stressed that we see no need for the public to hold additional cash," Mr. Kelley said. "We feel strongly that the most sensible thing to do with your money is to leave it where it is, but our responsibility is to make sure the public knows that currency is readily available." Ms. Seidman said consumers can take steps "to ensure that their own personal financial transition into the new century is a smooth one," and to safeguard themselves in the event there are minor glitches. Consumers should keep copies of financial records and balance their checkbooks regularly, the OTS Director said. They should keep up with news about Y2K, recognize scare stories for what they are and separate fact from fiction. Consumers should also be realistic and withdraw only as much money from their financial institution as they would for any other holiday weekend. Ms. Seidman also urged consumers to be wary of Y2K scams. "Be skeptical and tell friends and relatives to be skeptical if someone asks for account information, credit card numbers, social security numbers or your mother's maiden name," she said. "Be wary if promised that your money will be put into a Y2K safe account or told that your personal information is needed to make Y2K adjustments. Simply put, it isn't. " Mr. D'Amours noted that most banks, thrifts and credit unions are already using Y2K-ready systems successfully and said that operating those systems before January 1 provides evidence of how compliant systems will work after January 1. "I want to tell you all where my money will be and where I've advised my mother to keep her money when the clock strikes midnight December 31, 1999," the NCUA chairman said. "My money, and if she takes my advice, my mother's money, will be where it is now, in our credit union, where it's safe and insured against loss by the U.S. Government."
Remarks by Chairman Alan Greenspan Before the President's Council on Year 2000 Conversion, Financial Sector Group, Year 2000 Summit, Washington, D.C. September 17, 1999 Good morning, everyone. It's an honor to speak today to such an esteemed group. All of you are experts on the implications of the Century Date Change for our sector of the economy, and I suspect I will not have much to add beyond what already is well known. We face an exceptionally complex problem that has required and will continue to require the commitment of significant amounts of resources to fix. The good news is that evidence is becoming more persuasive that our electronic infrastructure will be ready for the Century Date Change. The public's understanding of the degree of our Y2K readiness also has grown, and fears of widespread disruptions around the CDC appear to be waning, though we are not as yet home free. There is nothing exactly like the Century Date Change in our historical annals from which we can infer its potential consequences. Nonetheless, it is the beginning of wisdom in thinking about the Y2K problem to recognize that failures and breakdowns in mechanical and electronic systems are a normal part of our everyday life. Recall the most recent example of how an electric power failure shut down the Chicago Board of Trade. Thus, as a standard for monitoring developments, it is simply unrealistic to expect our advanced technology to function any better on January 1, 2000, than it has on any other day of the year. Automated teller machines are a prime example of this. On any given day, 1 to 2 percent of the nation's ATMs are out of service for some reason. Although the banking system and ATM providers are about as prepared for Y2K as they can be, we cannot realistically expect perfection over the New Year's holiday any more than at similar periods in years past. Moreover, while systems may fail as they have in the past, these failures never have resulted in broader and persistent--that is, systemic--breakdowns in our economy. Notwithstanding, it is at least conceivable that, as a consequence of our current dependence on computers, some Y2K-related failures could have noticeable effects on the economy. But as a result of vast effort and an estimated $50 billion of expense by the private sector, enough of our critical infrastructure has been judged Y2K-compliant to view the probability of any systemic breakdown as negligible, even granting the uncertainties associated with our interconnections with less-prepared foreign countries. In the event of breakdowns short of systemic, history teaches us that businesses are remarkably adaptive, whether the adversities were the failure of AT&T's frame relay network, defective routers in the MCI-WorldCom data transmission network, or the clogged arteries of the Union Pacific railroad. In our market-based economy, economic incentives ensure that resources quickly move to their most-productive activities: In a crisis situation, whether systemic or short of that, companies expeditiously redeploy their labor and capital resources to facilitate the restoration of their key operations. Corporate management is wholly aware that a slow response to a breakdown can bring revenue losses in the short run and an erosion of their customer base in the long run. Simply put, in our competitive economic environment, the ability to recover quickly from a serious technical problem can literally be a matter of survival for some firms. Fortunately, our country has the skilled, well-educated workforce that is a precondition for such quick action. While ingenious solutions can sometimes originate from the executive suite, more often than not they grow out of the ability of engineers, technicians, and workers on the factory floor to improvise a temporary fix for a critical problem. Depth of experience and the ability of our workers to think "outside of the box" have prevented many a problem from turning into a disaster. Think back to the team of NASA engineers that developed a critical air filtration system for the Apollo 13 astronauts using only the limited materials available on that crippled spacecraft. While certainly less dramatic, similar problem solving is occurring every day in our economy, fostered by workers who can rise to a challenge and a market system that rewards extraordinary efforts. Thus, while no one knows exactly what will happen on January 1--the CDC is a truly idiosyncratic event--we do have a good idea of how our society will respond if problems develop. This morning we will hear many progress reports on the Y2K readiness of the financial industry and other key sectors. As we listen, it is most important to keep in perspective just how far we have come in our Y2K preparations. Three years ago, only the largest and arguably the most forward-looking of organizations had mobilized for the Century Date Change. Today, many firms and government agencies have completed their testing, and those institutions that were late off the block are working very diligently to be ready by the end of the year. While it is easy to obsess about the few institutions in our society that may not be ready, let us not lose sight of the fact that the overwhelming majority of us are not only prepared but have contingency plans to deal with breakdowns. Much has been learned over the past few years about how to disinfect our computer systems from the Y2K bug and how to isolate any problems that may occur. This large and growing knowledge base will serve us well as we approach the millennium and for years thereafter. While I have become increasingly persuaded that the technical breakdowns that might occur as a consequence of the CDC are readily containable, the response of businesses and households to unwarranted fears of serious disruptions does give me pause. It is the economic effects of their endeavoring to adjust to the CDC in the next few months that I see as replacing technical concerns as our major challenge. I am not saying that we would have been better off if the existence of the Y2K problem had never been publicized. In that event, the remedial actions that have been expended over the past two years would surely have fallen short. Although the desirability of publicizing the existence of a pending significant technical breakdown was never in question--and never should have been--it always raised the potential hazard of an outsized, if only partly informed, disruptive reaction by the public. Given the potentially broad range of uncertain outcomes at the CDC, the cost of advance preventative preparations in most cases is probably correctly perceived by businesses and households to be low, or at least acceptable. Thus, with their own remediation efforts either complete or nearing completion, many large businesses are currently evaluating the readiness of their suppliers and the local infrastructure on which they depend. Based on such assessments, these companies are deciding whether, for example, to hold inventory levels above their tight, just-in-time programs as a precaution against Y2K-related disruptions. Because businesses are effectively buying insurance against an uncertainty, the less uncertainty, the smaller the perceived insurance need. Thus, accurate, credible, and timely information on the general state of readiness will be essential to reducing uncertainties in the months ahead. Businesses then can make more-informed decisions as to the type and magnitude of the precautions they need to take. If only a small percentage of businesses choose to add to their inventories as a hedge, the effect on production will be insignificant. However, should a large number of companies want to hold even a few extra days of inventories, the necessary, albeit temporary, increase in production (or imports) to accommodate such stock building could be quite large. Bottlenecks could develop, and market pressure could ensue. Thus, the more we share information, the more informed our decisions and, hence, the smaller the need for precautionary hedging. While the evidence of precautionary inventory hedging to date is mixed, in the financial sphere, borrowers and lenders are clearly taking steps to build liquid assets and reduce their reliance on credit markets around the end of the year. This is reflected in a noticeable rise in deposit and commercial paper rates for funding that would be outstanding over year's end. Many corporate treasurers have moved forward their debt offerings to avoid any chance of a dearth of credit availability in the fourth quarter or difficulties funding short-term liabilities. The Century Date Change Special Liquidity Facility of the discount window that was approved by the Federal Reserve Board in July and the contingency actions of the Federal Open Market Committee announced by the Federal Reserve Bank of New York on September 8 should help to ensure an ample supply of liquidity and relieve funding pressures. The potentially most important piece in the Y2K puzzle for the rest of the year is the uncertain response of the American consumer as the year-end approaches. A small number of households, driven by fear of the unknown, tell pollsters that they are planning to build large stockpiles of food, water, fuel, and cash as the millennium approaches. Most, however, profess much more limited plans. Nonetheless, we at the Federal Reserve must be prepared for all contingencies and have made especial plans for currency availability in the remote possibility of heavy withdrawals from banks. I trust that such withdrawals will be modest since, as I have said before, the safest thing for consumers to do with their money around year-end is to leave it where it is. Consumers should prepare for the Century Date Change as they would for any long weekend. Those people who do cash out a significant part of their deposits only increase the risk that they will become victims of crime or fraud. Prudent consumers nonetheless should always have up-to-date copies of their financial records just in case of a "normal" computer glitch. In summary, no one really knows what will happen when the century rolls over. The Century Date Change, to repeat, is a unique event, and the complexity of the problem suggests that something is likely to slip through the cracks. But as I mentioned earlier, the probability of a cascading of computer failures in mission-critical systems is now negligible, given the testing that has been done, the backup plans that are in place, and the great adaptability and ingenuity of the American worker. Moreover, the evidence of an increasingly compliant computer infrastructure appears to have assuaged at least some of the public's earlier Y2K concerns, according to several recent surveys. And the year-end interest rate premiums, which rose throughout the first half of this year, appear to have receded a bit since early September. Nonetheless, we have not yet reached the period of extra heavy focus by the media on the CDC. It is too compelling a story for audiences that thrive on countdowns to the unknown. As attention heightens and rumors inevitably mushroom, it is important that what is known and what is not known be clearly articulated by those of us in both public and private leadership positions in Y2K management. In the final analysis, facts are the only antidote for rumors. We at the Federal Reserve are optimistic that computer problems associated with the Century Date Change and the response to the CDC will not be a major event for our nation. This is a testament to the extraordinary efforts of thousands of far-sighted technicians and business planners who, confronted with an intangible and abstract problem, have been able to convince businesses and governments to marshal vast resources for remedial actions. This has been a truly impressive feat. If we avoid fear-induced, significant economic responses in the months ahead, the Century Date Change will hopefully replicate the saga of "the dog that did not bark."
Remarks by Governor Edward M. Gramlich Before the Electronic Payment Symposium, University of Michigan, Ann Arbor, Michigan September 17, 1999 Although the United States economy seems to be leading the world in the adoption of new computer and internet-based technology, the picture is not uniform. The United States is not at the forefront in the adoption of electronic money systems, one area that would seem most eligible for the information revolution. Adherence to traditional payment systems, check and cash, is very strong. The United States is the only developed country in the world where check use is still increasing, with the number of checks written growing at a rate nearly as fast as the overall economy. Use of cash is extensive as well. Americans still use cash for about three-quarters of all transactions. The total U.S. supply of coin and currency now comes to $550 billion, about one-third of which is actually circulating in this country (the remainder is held abroad). But even after subtracting estimated foreign balances, the supply of outstanding coin and currency comes to $670 per capita, which strikes most people as incredibly large. For many years, observers have looked forward to the advent of electronic money, a system that uses either a computer chip or another electronic device to record payments and debits automatically. There are obvious efficiency advantages in terms of ease of handling and record-keeping for consumers, merchants, the banking system, and the Federal Reserve. Use of electronic money systems appears to be growing in at least a few foreign countries. But in this country, growth of electronic money systems is sluggish--well behind earlier predictions, well behind the growth of credit and debit card use, and way behind the growth of other types of electronic commerce. Even fervent advocates of electronic money will admit disappointment at its rate of adoption. In this talk I will try to assess the state of the electronic money transformation, here and abroad. I mention some promises, some stumbling blocks, and some technological and regulatory issues that will have to be dealt with as electronic money use proceeds. Historical Antecedents The idea of using technology to improve the efficiency of the payment system is very old. World commerce has seen an evolution from barter to valuable metals, to paper money, and to checks. In the United States, as early as 1853, more than a half-century before the Federal Reserve System came into existence, private banks in the New York area formed an exchange and settlement system that economized on the movement of paper and settlement balances. In 1970, some of these arrangements were automated and became the basis for a wire transfer system called the Clearing House Interbank Payments System (CHIPS). CHIPS is still the leading clearing and settlement system for large dollar interbank payments (average size of $6 million) originating in international trade and finance. When the Federal Reserve System came on the scene in 1913, one of its early tasks was to eliminate gold transfers and exchange rate differentials between the dollar and gold across the Federal Reserve Districts. The Fed created a "goldwire" system that involved inter-District settlements through the Gold Settlement Fund. This system quickly gave way to a telegraphic system for adjusting reserve balances. This system in turn evolved into a highly automated and centralized Fedwire system to handle large volumes of high-value payments settled in reserve balances on a real-time basis. The Fedwire system is the key interbank settlement system for the federal funds market. Average payments are around $3 million, but the system is also used for much smaller time-critical payments. There is also a system for handling smaller retail payments, now averaging about $3,000. In the early 1970s, a group of California commercial banks, in cooperation with the Federal Reserve Bank of San Francisco, organized an automated clearing system operating within the San Francisco District. A similar development occurred in the Atlanta District, with groups in other Districts studying similar systems. These efforts led to the creation of an Automated Clearing House (ACH) throughout the entire Federal Reserve System. The federal government along with some other large private employers began making some payroll deposits through this system in 1975. I vaguely remember the University of Michigan switching to automatic payroll deposit at about that time, apparently one of the early large employers to switch. Use of the ACH has grown and is still growing rapidly, with the number of payments made through ACH rising at double-digit annual rates right up to the present time (in contrast to the sluggish growth of electronic money use). But there is still vast potential for further ACH growth because only about 45 percent of payroll payments are now made through the ACH and only about 8 percent of consumer bill payments. Since the marginal cost to consumers of making payments by ACH is about half that of the cost of making payments by check, in the end the most interesting question may be why the ACH has not displaced the costlier check system even more rapidly. Electronic Money With that background, we now turn to our main topic, electronic money. This term is normally taken to refer to a stored-value product, where a prepaid balance of funds is recorded on a card or personal computer controlled by the consumer and updated automatically as payments are made in or out. The stored-value balance would be recorded as a liability of the institution, financial or otherwise, that issues the card. Early stored-value cards recorded the account balance on magnetic strip, but magnetic strip cards can be difficult to reload and are easy to tamper with. Future smart cards are likely to record stored value through an embedded microprocessor chip, which permits sophisticated encryption to protect against counterfeiting. These cards can also be used as credit cards, debit cards, or repositories of other identifying information for the consumer. While much rarer, electronic money systems can also be computer based. Under these systems, variously called ecash, cyber coins, and cyberbucks, computer software generates electronic (virtual) tokens that serve as cash. The seller has to verify the tokens, and the issuer may have to settle them--operations that at this point have proven costly and difficult. Stored-value products can be used in open or closed systems. Closed systems involve a narrowly defined group of consumers, such as riders of a metropolitan transportation system or students at the University of Michigan. The infrastructure necessary to redeem the value is in place either at transportation system terminals or owned by merchants within a relatively small geographic area. Open systems involve many consumers and merchants over an extended geographic area, with the stored-value cards really functioning as electronic money. One way to assess the potential of electronic money is to analyze the benefits and costs of electronic money, as compared with alternative ways of making payments. I will discuss this separately for consumers, merchants, financial institutions, and the Federal Reserve System. While the potential gains to all groups together may not be huge, they do seem to be positive, adding to the puzzle about the slowness of adoption of stored-value technology. From the consumer's standpoint, there could be many potential advantages of electronic money, but there are also risks. Stored-value cards are easier to handle than either cash or checks. Although most electronic money systems now do not have pin number protection, that protection is technologically possible, and protection against loss through theft could be developed in the future. There is some risk of insolvency by the issuer, but at least for financial institutions that risk is very low statistically. A greater problem is that until the stored-value system becomes universal, there is a risk that the sellers will not accept the card. Payment systems involve a network--money is not truly money unless it becomes nearly universally acceptable--and network problems have been a big impediment to the development of any currency system. This is one reason that electronic money systems may be more likely to develop in natural closed systems or in small countries. One apparent cost of electronic money for consumers, at least in comparison with credit cards, is that of float. In benefit-cost parlance, float is an internal redistribution--a cost of electronic money felt by consumers offset by gains to other sectors. Float then should not affect the overall social interest in electronic money. Moreover, in the long run we might expect the competitive market to offer electronic money products that compensate consumers for their loss of float. Electronic money systems might raise a potential tradeoff between protection and privacy. Consumer protection could be enhanced by stored-value systems because there could be a record of all transactions, and this record could be used to resolve disputes or to deal with losses due to theft, in much the same way that consumers' credit cards are now protected. At the same time, this record could also be misused to defraud consumers or infringe on their financial privacy, by irresponsible action on the part of the issuing institution or in some other way. Consumers could sacrifice their protection but retain their privacy by purchasing cards without this extensive recordkeeping or by signing "opt out" forms to prevent the disclosure of confidential information. Consumers could also limit invasions of privacy by keeping low balances on their card or restricting its use, but this limits the utility of electronic money in the first place. From the standpoint of merchants, electronic money systems raise orthodox benefit-cost questions. It is costly for merchants to invest in the infrastructure to process the stored-value cards, much more so than for credit cards because of the encryption technology or the difficulty of verifying electronic tokens. But once merchants have made the investment, costs of handling at least some types of payments should go down. Compared with credit and debit cards, there may be little cost reduction. Compared with checks, there is less risk of default because the sale will not be completed unless the stored value is adequate to make the purchase and the amount of the stored value can be determined immediately. Compared with cash, there is less risk of theft, at least out of the traditional cash register. While merchants have not eagerly invested in the infrastructure because of the network problems mentioned above, in the long run there should be some gains to them from electronic money technology. For financial institutions, stored-value products may offer new opportunities for delivering banking services and improving security through computer chips. Consumers may switch from using cash to using deposits that are linked to stored-value products, say with automatic loading from an ATM machine or with links to a personal computer. This could be a profitable new market for financial institutions, with relatively little risk. While financial institutions have not been early to climb aboard the electronic money bandwagon either, they may become strong supporters at some point. If financial institutions issue the stored-value cards, there is some protection against abuses because financial institutions are already regulated, particularly with respect to their solvency. Regulations could also encompass the institutions' disclosure policy, its privacy policy, and its following of other proper procedures. Many institutions are now announcing very strict disclosure and privacy policies, and extending these policies to stored-value products should be straightforward. In one early indication of coming complications, the Federal Deposit Insurance Corporation (FDIC) has ruled that electronic money accounts issued by insured institutions are not always legally defined as deposits under the deposit insurance act. This means that stored- value balances do not always carry deposit insurance. By contrast, in virtually all European countries, some of which seem to have more-rapid growth of electronic money products, stored- value balances are typically accorded the same protection as deposits issued by a bank. At this point it is unclear whether the negative FDIC decision could be a major reason for the sluggish growth of stored-value products in this country. In a statistical sense, risk of loss of value through financial insolvency is minor and is probably much less of a factor to consumers than risk of loss of the card or risk that merchants will not accept the card. Finally, governmental institutions could have a stake in stored-value technology. The Debt Collection Improvement Act of 1996 encourages the federal government to make most of its payments electronically. When consumers do not have bank accounts, as about 10 percent now do not, governments have at this point made most payments through debit cards. But at some point, stored-value products might become an easy way for governments to pay social security benefits, unemployment insurance benefits, and even welfare and food stamps. Innovations are often criticized because they leave low- and moderate-income groups behind, but this is an innovation that might actually benefit low- and moderate-income groups disproportionately. Returning to the protection-privacy tradeoff, government crime control agencies may find stored-valued transactions easier to trace than cash transactions. But as said above, to the extent that the government may gain records of transactions, consumers will lose a corresponding degree of privacy. When this issue has come up in the past, the desire for customer privacy has clearly won out, and there is no obvious reason that the balance of political weight will shift in the future. While the potential is there for increased crime control, this potential seems unlikely to be realized. Federal regulators have also had to face the issue of whether to regulate stored-value products. On the one hand, when networks are important, there is something to be said for having the government define a technology early to provide a standard for future development. As mentioned earlier, the role of the Federal Reserve was critical in the past development of some of our major payment systems. On the other hand, the existence of an already well-functioning payment system is an argument for a more passive governmental regulatory posture. Presumably now that the main institutions are in place, payment systems can develop in ways that effectively meet consumer desires. One can imagine network externalities, but it is not obvious that the natural development of payment systems in an environment with minimal regulation will lead to serious inefficiencies. For what it is worth, federal regulators have taken the passive posture, and there is at this time little federal regulation of stored-value products. Summing all of this up, it seems that all groups--consumers, merchants, financial institutions, and governments--can realize potential gains from electronic money. The gains will be greater, the easier it is to establish natural networks or closed systems. Net gains will be correspondingly less, the more convenient are alternative payment systems. Foreign Experience with Electronic Money Moving from the general to the specific, I now review the actual experience with electronic money in other developed countries. As said earlier, adoption of electronic money products is proceeding somewhat more rapidly in some foreign countries than at home, and it makes sense to examine the reasons. Electronic money pilot programs are at least in the development stage throughout much of the world. A recent survey found that fifty countries had at least some electronic money pilot projects. The vast majority of the projects involved stored-value card systems, with a handful of countries having at least small-scale experimental network systems. Seven countries appear to be making the most use of electronic money products. Switzerland, the Netherlands, and Hong Kong have numbers of outstanding cards exceeding 75 percent of their population, though in Hong Kong a great many of the cards are for transportation purposes. Germany, Singapore, Belgium, and Austria have numbers of outstanding cards exceeding 40 percent, though many of these are linked to debit cards. By contrast, the U.S. number is a fraction of 1 percent. But even in countries where card use is widespread, the cards are used for very small transactions and account for a small share of overall payments. Many countries have adapted their commercial codes to electronic money products. They have protocols for dealing with fraud, loss, theft, other disputes, and privacy disclosure requirements. Many of these countries have anti-money-laundering provisions, and unlike the United States, most have ruled that deposit insurance does cover electronic money products. While it is difficult to generalize, the countries where stored-value products seem to have made the biggest inroads are small, implying that electronic money systems work better in natural closed system or in isolated areas. They also work better in more-developed countries where wages and personnel costs to handle less-automated payment systems are higher. The small average size of stored-value transactions suggests that stored-value cards will substitute for currency, if they substitute for anything. In that light, the average ratio of currency holding to GDP in the seven countries where stored-value cards are common is 6.8 percent, higher than the average ratio of 6 percent for all developed countries. It is also much higher than the ratio for the United States. Nominally, the U.S. ratio is 5.2 percent, but when we consider that two-thirds of U.S. currency is held abroad, the effective U.S. ratio is less than 2 percent. This comparison implies that stored-value products have their best chance of success in countries where use of currency is widespread. Even though the United States seems to have a lot of domestic currency outstanding, currency usage is even higher in these other countries. Domestic Experience In this country there are a number of closed electronic money systems, but still relatively few open systems. In Atlanta, more than 1.7 million electronic money cards were produced for the 1996 Olympic Games, and these cards paid for about 200,000 transactions totaling $1.1 million. The Olympics are over, and this system has now been discontinued. Two separate pilot programs, involving 100,000 customers and 1,200 merchants, began in New York City but were discontinued as of December 1998, largely because of sluggish acceptance by consumers and merchants. Nationally, the firm Digicash tried a network-based system, but acceptance was again sluggish and the system was discontinued. Cyber Cash and Wells Wallet are now trying other network-based systems. There is a growing amount of commerce on the internet, but at this point only a minuscule amount takes place through stored-value products. Most takes place through credit cards, not electronic money systems. It may be easier and cheaper for consumers to make Internet transactions directly through stored-value routines, or it may not. Given the slight cost differences between Internet credit card transactions and Internet stored-value transactions, it may be realistic to look toward a continuation of the slow development of stored-value Internet transactions. Stored-value transactions would seem to offer the biggest cost savings relative to cash, not existing Internet transactions. Conclusions This brief survey of electronic money suggests some tentative conclusions. Stored-value products could have a promising future--they could become easier or cheaper for every potential stakeholder--consumers, merchants, financial institutions, and governments. At the same time, there have been continuing innovations in the payment system in this country, and part of the reason for the slow adoption of electronic money products here is surely that innovations in credit cards, debit cards, and the ACH have made it much easier and safer to conduct transactions through these systems. Stored-value products have done better in small countries and in countries where there is greater use of coin and currency. Looking ahead, the United States may be one of the countries least likely to adopt open stored-value systems. It is a big country, with many alternative ways of making payments. Americans are strongly attached to checks, and use of credit cards, debit cards, and the ACH is growing rapidly. Even cash is still widely used for small transactions, though less so than in other countries. A major hurdle for the electronic money products is the network problem. If most merchants do not accept stored-value products, most consumers will not bother with stored-value cards. This is the classic chicken-egg problem, a problem that often gets solved by slow growth on both sides of the market. In principle, the government could intervene and force or induce merchants, consumers, and financial institutions to adopt the new technology, but in practice, alternatives to stored-value products are cheap and safe enough that such intervention is both economically unwise and politically unlikely. It is also possible that this network consideration leads to the development of many closed electronic money systems--for transportation, for university students, for one-employer towns, and the like. This is perhaps not the visionary's dream, but even the development of closed systems cuts the demand for cash and check and makes the payment system more efficient.
For immediate release
Remarks by Governor Roger W. Ferguson, Jr. Before the 2000 Global Economic and Investment Outlook Conference, Carnegie Bosch Institute, Pittsburgh, Pennsylvania September 21, 1999 Is Information Technology the Key to Higher Productivity Growth in the United States and Abroad? The last few years have seen an explosion in the uses of information technology throughout the American economy. At the same time, trend U.S. productivity growth appears to have risen to its highest rate since the 1970s. Casual empiricism would suggest a connection-- that the enormous investment in computer technology that has been going on for at least twenty years has finally started to bear fruit. But although information technology is available, at least in theory, to the whole world, the recent surge in productivity growth appears to have been stronger in the United States than elsewhere, including the other industrialized countries. This raises the interesting questions of what else besides simple availability is needed to translate the promise of information technology into real productivity gains, and whether--whatever it is--the United States has more of it? These questions are impossible to answer with precision, so the purpose of my talk this afternoon is to identify particular features of the American economy that might contribute to an especially hospitable climate for translating the potential gains from information technology into actual productivity growth. I also want to be careful not to suggest that gains from the use of information technology can safely be assumed to go on indefinitely at their recent pace. The recent technical advances represent a continuation of a long string of fundamental leaps in technology that have worked their way through the economic system over many years, boosting productivity growth in the process, but how long a particular innovation has a beneficial effect on productivity growth is difficult to say. That depends on the rate of investment in the equipment that embodies the new technology, the rate at which the labor force is able to acquire needed skills, and, of course, on the fundamental potential of the technology itself. History demonstrates that the boost to productivity growth from a particular technological advance is not unlimited and eventually will be fully exploited. Just as "trees do not grow to the sky," so, too, increases in the rate of productivity growth from any given advance are not without limits. One should be cautious in extrapolating from past trends. What We Do Know What we do know is that the use of information technology, at least in the United States, has been growing by leaps and bounds. For example, personal and business Internet sites have proliferated at an astounding rate, and a wide variety of tasks that used to require person-to-person contact, such as making airline and other travel reservations and choosing and ordering merchandise, can now be done via the Web. Search engines have cut the time needed to track down a person or an item to a fraction of amount previously required, clearly raising productivity. Improved communication and information flow are only part of the story; increases in computing power also allow workers to complete a variety of tasks more quickly. Although it sometimes feels as if these changes are taking place at lightning speed, most of us know that this is partly an illusion. Much of the basic technology has been around for decades. In fact, as productivity growth slowed in the 1970s and continued to languish in the 1980s, many observers wondered whether the supposed benefits of cheaper and more powerful computers would ever be realized. This highlights an important facet of the innovation process: The benefits of a new technology are in no sense automatically conferred on the economy but will show up only after the technology is widely adopted, capital facilities are refitted and adapted to it, and workers learn to use it. For instance, Paul David, a Stanford University economist, notes that electric motors did not boost productivity growth appreciably until more than forty years after Edison installed the first dynamo in 1881. Though a new technology typically will not be fully incorporated overnight, the speed of its adoption can be faster or slower depending on the institutional and other features of the economy. For information technology, the process of incorporation appears to be taking place at a considerably faster rate in the United States than in other parts of the industrialized world. For example, there are more than 23 million Internet hosts in the United States--roughly one for every 11 people. Canada is second among the major industrial economies with 1.6 million, one for every 19 people. In contrast, the ratios are about 1 to 128 people in France, and 1 to 174 people in Italy. These statistics suggest that technology is being used more widely in the United States, but is its use paying off? The answer appears to be "Yes." One pertinent piece of evidence is a recent study of costs of managing cash flow in American versus European firms. The study showed European costs to be roughly 30 percent above those of U.S. firms, largely because of the slow adoption of information and other computer-based technologies. This is pertinent because finance operations of this type are an essential activity in every firm. If it is the case that the United States, at least at this juncture, is somewhat ahead of the rest of the world in realizing the benefits of information technology, does this indicate that our economy or society possess certain characteristics that are particularly conducive to rapid diffusion of technical change? A recent study of innovation by the Agamus Consulting firm provides some interesting insights. The United States placed second to the Netherlands out of thirteen countries in a survey that asked companies to rate the "innovation climate" in their home countries. A second and possibly more objective ranking, based on a measure of innovative success developed by Agamus, placed the United States first. The most important factor cited as conducive to innovation was the overall educational standard. Given various cross-country comparisons of educational systems, I assume that this finding must be based on our broad-based attainment of higher education. Additional Possible Relevant Factors I would like to suggest several other factors that might also make some difference. This is not meant to be either a comprehensive or a definitive list. Instead, it is an attempt to advance some hypotheses that might help to explain the recent dynamism of the American economy compared with some of its major trading partners--in particular its link to technological change--and to see to what extent the evidence supports them. The particular features that I would like to discuss (not necessarily in order of importance) are corporate governance and, especially, a focus on maximization of shareholder value as opposed to other objectives; flexibility of labor markets and the willingness to accept high rates of labor turnover; willingness on the part of labor to continue to invest in human capital over a lifetime; the regulatory environment; and the friendliness of the institutional environment to entrepreneurship. Corporate Governance Clearly, the effectiveness of the system of corporate governance is important in overall corporate performance. Adoption of new technology may require considerable effort and short-term expense, and it is important that managers have the appropriate incentives to search for improvements that reduce costs over the longer-term. In recent years, aggressive cost-cutting in the United States has been linked to greater emphasis on maximization of shareholder value and less on growth and diversification, which was more prominent in the 1970s and 1980s. This shift in perspective appears to have been driven, at least in part, by the increasing dominance of large institutional investors in U.S. financial markets. In the past, maximizing shareholder value had not been as widely embraced abroad, although there are indications that views are changing. For instance, members of corporate boards in Japanese companies often have been promoted from within, fostering control by allied industrial concerns, family interests, banks and holding companies, which may be more motivated by concerns other than maximizing shareholder value. In Europe, a reason sometimes cited for the delay in the adoption of shareholder value as a motivating factor for corporations is the greater involvement by the public sector in the economy, with a strong emphasis on job preservation. Another important incentive for managers to maximize shareholder value is pay-for-performance through avenues such as stock options. Although now commonplace in the United States, such instruments were not legal in Germany and Finland until 1998. However, pressure for change clearly has started to emerge. Financial market liberalization has increased the importance of equity and publicly traded debt as sources of finance. Anecdotal reports suggest that the concept of maximization of shareholder value has been gaining greater acceptance as firms turn more to stock and bond markets for financing and as governments, particularly in Asia, have increased disclosure requirements. If the trend continues and other countries move further toward the U.S. model, it will be interesting to see whether improvements in productivity growth follow. Labor Market Flexibility Another factor that is often cited as a major element in the dynamism of the U.S. economy is the extraordinary flexibility of our labor markets, especially in contrast to those of continental European countries. Although much European regulation has been directed at saving jobs, it can be argued, in fact, to have had the opposite effect in the aggregate, as evidenced by the marked upward drift in continental European unemployment rates over the past two to three decades. These jobless rates are now much higher than those in the United States and the United Kingdom, which has also undergone a period of substantial labor market deregulation. It is also noteworthy that the unemployment rate differential is particularly large in younger age groups; youth unemployment rates were around 30 percent in 1997 in France and Italy, for example. To the extent that younger people are likely to have had more exposure to information technology in the educational process, this bias by itself could potentially be an important obstacle to the incorporation of technology into business processes. In addition, workers who are unemployed for long periods of time are likely to see their technology skills deteriorate. But why might regulations designed to protect jobs have such a perverse effect? The evidence suggests that new technology often results in more growth in employment in innovating industries. However, it also tends to shift demand from unskilled to more highly skilled workers, potentially displacing unskilled workers in the process. Job protection regulations that affect a firm's flexibility to recruit and dismiss workers can interfere with this process, making it difficult both for newcomers to find jobs and for firms to adopt new technologies. The inability to adjust hours flexibly through the use of overtime, part-time, and temporary work may also stifle innovation. According to the European Car Assembly Association, similar research projects take much longer to complete in Germany than in the United States because of shorter working hours and less-flexible working conditions. As a result, they argue that German automobile manufacturers are less able to exploit the shorter product life cycles associated with more fashionable and high-tech cars. It is axiomatic that in a truly flexible labor market everyone who wants a job can find one--at some price. As technical change increases demand for skilled relative to unskilled labor, the unskilled workers must acquire new skills, find new jobs at lower relative wages, or become unemployed. OECD data suggest that the United States and Canada have been more successful than the other industrialized countries in achieving these adjustments and, therefore, in maintaining aggregate employment growth on a par with labor force growth in the face of differential rates of job growth by occupations. White-collar, high-skilled employment increased at a much faster rate than employment in the other categories in nearly all cases in the G-7 countries over 1979-95. In the United States and Canada, white-collar, low-skilled employment also rose at healthy rates, while blue-collar employment was little changed. In contrast, blue-collar employment fell sharply in most of the other countries. Human Capital Of the three choices facing an unskilled worker in a fast-changing economy, acquiring new skills--that is, increasing one's human capital--would seem, in general, to be preferable to either taking a pay cut or becoming unemployed. To what extent do American workers take advantage of such opportunities relative to the rest of world? Here the evidence is somewhat mixed. The U.S. adult population has the highest rates of completion of upper secondary or higher education of any of the major industrialized countries. However, educational attainment rates are only part of the story, as skills need to be continually upgraded in a world of rapid technical change. This does not suggest that educational attainment is unimportant; in fact, there is a clear interaction between educational attainment and continuing education, as more-highly-educated people are also more likely to participate in continuing education. Nevertheless, it is hard to make the case that the United States is ahead of other countries in terms of participation in continuing education. In an OECD study of the role of continuing education and employability, the rate of participation in these programs in the United States was about average for the six countries in the sample. However, one noteworthy result was that rates of participation in training programs were below average among the young but above average among older workers in the United States. This suggests that American workers tend to keep improving existing skills or acquiring new ones as they age to a greater extent than do their counterparts in other countries. Other Business Regulations In addition to job protection legislation, other forms of business regulation may also have an impact on the climate for innovation. A 1994 survey of more than 2000 European companies by the Union of Industrial and Employers' Confederations of Europe found that regulations made it more difficult to minimize costs, organize production in a flexible way, reduce time to market, and reduce uncertainty. The incidence of product market regulation is lower in the United States than in continental Europe. A cross-country comparison of macroeconomic performance in terms of productivity growth and utilization of resources with the OECD's index of the overall regulatory environment suggests that a country's performance does improve as the regulatory environment becomes less restrictive. On a micro level, differences in the regulatory regimes of the biotechnology industry in Europe and the United States have been cited as playing an important role in explaining why U.S. firms are ahead of European firms in important measures of innovation such as R&D expenditures and patents. Surveys of the European biotechnology industry suggest that regulatory restrictions tend to push product development toward existing technologies and force firms to conduct research abroad, although I should note that it is also claimed that American pharmaceutical companies are conducting an increasing amount of research abroad as well because of regulatory obstacles at home. In addition, regulatory regimes that promote competition foster innovation and diffusion of technologies. According to an OECD study, the United States has policies that are effective in preventing anti-competitive behavior, but Germany is not far behind. Why does a more-competitive environment foster innovation? One hypothesis is that competition forces firms to innovate and adopt new technologies and, therefore, it increases the speed of diffusion of technology. In contrast, monopolists may have little incentive to innovate because they already control most of the market. Competition will also tend to result in the failure of unproductive businesses and facilitate the entry and success of more-innovative ones. In addition, more heavily-regulated firms may be less motivated to choose an efficient technology. In recent years, industries such as telecommunications, transportation, electricity, and banking have undergone privatization, deregulation, and increased competition in a number of countries. In many cases, these reforms were in fact prompted by technological change, which reduced large fixed costs and thus the scope for natural monopolies. Furthermore, in some of these industries, there is evidence that the move toward a more liberalized regulatory regime induced further innovation. A good example is the telecommunications industry. Evidence on patents (one measure of innovation) and measures of productivity suggests that those countries that have extensively liberalized (such as Japan, the United Kingdom, Finland, and the United States) have experienced greater innovation and larger gains in efficiency. Evidence from the telecommunications industry also suggests that the technological diffusion rate is faster under a more competitive regulatory regime. For instance, both growth in cellular phone usage and the penetration rate for Internet hosts is much higher in more-competitive market structures. This is not to suggest that regulation is necessarily a bad thing. Regulations that protect intellectual property rights reward those with creative ideas and therefore can act to stimulate cost-reducing innovations. From a broader perspective, productivity growth is obviously not society's only priority--worker health and safety, pollution control, and other societal values are important as well. Although it has been argued that regulations requiring mandated approaches to pollution reduction or worker safety tend to divert managerial energies from pursuing cost-reducing innovations, studies have shown that some regulatory changes can in fact enhance productivity by forcing a firm to develop new and more-efficient production techniques. For example, the cotton dust standard mandated by OSHA is claimed to have led to the adoption of new and more cost-effective technologies utilized by the textile industry. What I think this suggests is the need, as with so much in economics, to recognize trade-offs. We should recognize the broad range of society's interests and continue to seek balance by striving for regulation that serves well-defined purposes with minimal burden. Other Institutional Features Other institutional features are also important to the climate for innovation. For instance, entrepreneurship is fostered by access of small firms to capital markets. A lack of breadth and depth of financial institutions and markets can inhibit the financing of innovative projects by small firms. Again, the United States appears to have an advantage in this regard relative to Europe and Japan. In particular, venture capital markets here are both more developed and more geared to financing higher-risk projects, mainly in technology-based sectors by start-ups with prospects of rapid growth. Furthermore, the range of investors is wide and includes pension funds, insurance companies, and even private individuals. In contrast, in Europe, venture capital is geared toward more mainstream projects, and banks dominate lending. In Japan, a venture capitalist is typically a subsidiary of a large financial institution and invests mainly in established firms. Conclusion Obviously, the United States does not have a monopoly on technological advance. We should not be smug nor complacent because certainly the U.S. experience will be adopted and adapted by other countries. Although the United States arguably has led the way into the information technology revolution, there is evidence that others are following. Scandinavia in particular appears to be embracing computer-based technology; Sweden has begun to market itself as Europe's "Silicon Valley." Adoption of new technologies in the United States may also have been spurred in recent years by the cyclical strength of the economy in combination with strong domestic and international competitive pressures. With new workers increasingly difficult to hire in a tight labor market, firms have an increased incentive to find new and more-efficient ways to use existing labor resources. I might add that the current low inflation environment also helps this process. In the presence of subdued inflation expectations, the first inclination of firms in the face of rising demand for their output, thus far at least, appears not to have been to raise prices but rather to find ways to expand output via more-efficient means of production. It is clear that other countries, many of which are less far along in reaping the benefits associated with the revolution in information technology, have the potential to gain more over the period ahead. The extent to which they do realize these gains will depend on how successful they are in adapting to their unique circumstances policies that foster efficiency and competition in labor and product markets. I wish them well.
For immediate release The Federal Reserve Board today announced its approval of the application of The Dai-Ichi Kangyo Bank, Limited, Tokyo, Japan, to acquire through its subsidiary, the CIT Group, Inc., Livingston, New Jersey, all the outstanding voting shares of Newcourt Credit Group, Inc., Toronto, Canada. Attached is the Board's Order relating to this action.
For immediate release The Federal Reserve Board announced today its approval of the application of Caixa Geral de Depósitos S.A., Lisbon, Portugal, to establish a state-licensed branch in New York, New York. Attached is the Board's Order relating to this action.
For immediate release The Federal Reserve Board announced today that the Consumer Advisory Council will hold its next meeting on Thursday, October 21. The meeting will take place in the Board Room on the 2nd floor in the Board's Eccles Building. The session will begin at 8:45 a.m. and is open to the public. The Council's function is to advise the Board on the exercise of its responsibilities under the Consumer Credit Protection Act and on other matters on which the Board seeks its advice. Time permitting, the Council will discuss the following topics: Electronic Delivery of Disclosures Proposals Regulation B Proposal Subprime Lending Reports by committees and other matters previously considered by the Council or initiated by the Council members may also be discussed. The Board invites comment from the public on any of these matters. FEDERAL RESERVE SYSTEM Consumer Advisory Council Notice of Meeting of Consumer Advisory Council The Consumer Advisory Council will meet on Thursday, October 21, 1999. The meeting, which will be open to public observation, will take place at the Federal Reserve Board's offices in Washington, D.C., in the Board Room of the Eccles Building (2nd floor). The meeting will begin at 8:45 a.m. and is expected to conclude at 1:00 p.m. The Eccles Building is located on C Street, Northwest, between 20th and 21st Streets. The Council's function is to advise the Board on the exercise of the Board's responsibilities under the Consumer Credit Protection Act and on other matters on which the Board seeks its advice. Time permitting, the Council will discuss the following topics: Electronic Delivery of Disclosures Proposals The Depository and Delivery Systems and the Consumer Credit Committees will lead a discussion about the proposals to permit electronic delivery of federally mandated disclosures under certain consumer financial services and fair lending laws such as the Truth in Lending and Equal Credit Opportunity Acts. Regulation B Proposal The Bank Regulations Committee will lead a discussion of proposed revisions to Regulation B which implements the Equal Credit Opportunity Act. Subprime Lending The Community Affairs and Housing Committee will lead a discussion of issues regarding lenders' subprime lending practices. Members Forum Individual Council members will present views on economic conditions present within their industries or local economies. Committee Reports Council committees will report on their work. Other matters previously considered by the Council or initiated by Council members also may be discussed. Persons wishing to submit views to the Council regarding any of the above topics may do so by sending written statements to Ann Bistay, Secretary of the Consumer Advisory Council, Division of Consumer and Community Affairs, Board of Governors of the Federal Reserve System, Washington, D.C. 20551. Information about this meeting may be obtained from Ms. Bistay, 202-452-6470. Telecommunications Device for the Deaf (TDD) users may contact Diane Jenkins, 202-452-3544. Board of Governors of the Federal Reserve System, September 27, 1999. (Signed Jennifer J. Johnson) Jennifer J. Johnson Secretary of the Board
For immediate release The Federal Reserve today issued examination guidance cautioning against possible relaxation of credit discipline at banks. Although at this time loan portfolios remain sound overall, indications of departures from proven sound lending practices--in particular, over-reliance on optimistic views of the borrowers' prospects and favorable economic and financial conditions--have been a recurring theme emerging from recent supervisory reviews of bank credit quality. At the same time, over the past several quarters the volume of weak or potentially weak loans--that is, those falling into the classified or special mention categories used by supervisors--has risen at some institutions. Although the increases are generally attributable to industry-specific or global economic developments, these increases are significant because they have appeared despite the continued favorable economic and financial climate in the United States. Supervisory reviews indicate that the vulnerability of these loans was heightened in some cases by weak underwriting practices. The guidance, contained in a supervisory letter sent to Federal Reserve bank examiners and supervisors as well as banking organizations supervised by the Federal Reserve, describes three key areas in which some banks may have strayed from historically sound lending practice: Approving loans based on a very optimistic assessment of a borrower's operating prospects or on the assumption a borrower will always have ready access to financial markets. Failing to perform meaningful stress tests--or, if performed, to take such tests adequately into account--of a borrower's ability to withstand events such as unexpected shocks to operating revenue. Weakening internal controls critical to maintaining the rigor and discipline of lending decisions. "While loan portfolios are currently sound at the vast majority of banks, any trends toward laxity need to be reversed where they exist to ensure that the banking system remains strong and vibrant and retains the ability to lend to sound borrowers in good times and in bad," wrote Richard Spillenkothen, director of the Federal Reserve's Division of Banking Supervision and Regulation. The guidance instructs Federal Reserve examiners and supervisors to be alert for indications that undue reliance on favorable assumptions about borrowers or the economy and financial markets more generally has led banks to reduce the rigor of their credit decisions or delay recognition of emerging loan weakness. If examiners observe such undue reliance, delays in recognition of problem loans, or significant weakening of internal risk management processes, they should carefully consider whether these developments warrant a downgrade in one or more elements of the bank's overall supervisory rating for safety and soundness. Supervisory letters are the Federal Reserve's primary means of communicating key policy directives to its examiners, supervisory staff, and the banking industry. Supervisory letters can be viewed on the Board's World Wide Web home page at The supervisory letter is .
Board of Governors of the Federal Reserve System Federal Deposit Insurance Corporation National Credit Union Administration Office of the Comptroller of the Currency Office of Thrift Supervision The four Federal banking agencies issued today the attached joint statement that addresses the agencies' supervisory approach to possible temporary balance sheet growth due to potential unusual market responses around the century date change. Joint Interagency Statement September 28, 1999 Introduction As part of their efforts to foster readiness for Year 2000, the Federal banking agencies have issued guidance to banking organizations calling for the development of contingency plans to address funding, liquidity, and other issues. In this regard, bank and thrift management are responsible for establishing realistic liquidity and funding plans and programs that are supported by the organization's financial strength, capital position, and risk management capabilities. Unusual market responses to the century date change could lead to temporary balance sheet growth at some banking organizations during the century date change period. This growth could occur if a banking organization were to receive unusually large deposit inflows during the period. Similarly, such temporary asset growth could occur if corporate borrowers make unusual draws on their existing lines of credit, or request new lines, in response to a perceived need for extra liquidity during the century date change period. Absent other factors, large deposit inflows or increases in extensions of credit would likely result in an increase in total assets. Supervisory Approach to Temporary Balance Sheet Growth All banking organizations are responsible for managing prudently any temporary balance sheet growth that may occur. As part of the Federal banking agencies' Year 2000 supervisory program, supervisors will assess the development and content of banking organizations' contingency plans, including those that address funding and liquidity needs. These plans should address possible effects on the organization's balance sheet that may arise as a result of unusually large deposit inflows and significantly increased lending. It is likely that relatively few banking organizations will experience Year 2000-related asset growth that is significant in relation to their size, and any such asset growth is expected to be temporary. Some organizations that experience significant Year 2000-related asset growth may, despite prudent balance sheet management techniques, also experience a temporary decline in their regulatory capital ratios as a result of responding to customers' needs over the century date change period. Such a decline has the potential to result in certain consequences for the organization under statutes and regulations that the Federal banking agencies administer. If an organization believes such a situation could arise, management is urged to contact its primary supervisor to discuss options to address these issues. In assessing supervisory options, the Federal banking agencies will consider whether the institution exercises prudent and responsible measures to manage its balance sheet, maintains a fundamentally sound financial condition, and provides evidence that any drop in capital ratios is temporary. Any questions on this issue should be directed to the banking organization's primary supervisor.
Remarks by Chairman Alan Greenspan Trade and technology Before the Minnesota Meeting, Minneapolis, Minnesota September 30, 1999 I am pleased to be back in the Twin Cities and to appear before this gathering. I last addressed the Minnesota Meeting in 1990, speaking in the wake of events in the Persian Gulf. In the near-decade since, I have written many words on the crucial nexus between global events and U.S. economic well-being. Thus, knowing the interests of this audience, my good friend Gary Stern, the president of the Minneapolis Federal Reserve Bank, suggested that I focus on some issues related to international trade and technology that I have raised recently in other forums. Recognizing that those topics are of vital concern almost everywhere in our economy, I had no trouble following his advice. The evidence is overwhelmingly persuasive that the massive increase in world competition--a consequence of broadening trade flows--has fostered markedly higher standards of living for almost all countries that have participated in cross-border trade. I include most especially the United States. And yet, there are reasons to be concerned that the benefits of increasingly open trade may not be allowed to be as readily forthcoming in the future as they have been in the past half-century. Although many forces have been at play, the post-World War II surge in trade has clearly owed, in large part, to significant advances in technological innovation. Since the dawn of the industrial revolution, there has been an inexorable drive to leverage physical brawn and material resources into ever-greater value added or output. New insights into the laws of nature brought steam and later electric power. The development of precision production standards that facilitated interchangeable parts brought assembly-line production. And the development of railroads facilitated the evolution of mass markets. Almost all the rise in value added relative to physical input has reflected the substitution of ideas--new insights--for brute human effort and material bulk. Some of the most impressive advances in labor productivity over the decades have come in sectors of the economy in which physical effort was the most demanding--agriculture, for example. Aggregate value added by the farm sector has more than tripled in real terms over the past half-century, but the number of hours required to produce it has fallen by three-fourths. Manufacturing has displayed similar, though less pronounced, tendencies in recent decades--factory employment is down about 13 percent from its peak of twenty years ago, but output of the sector has risen about 75 percent. Like labor inputs, material requirements per unit of output also have fallen in many sectors of the economy. The insights of metallurgy and architectural and engineering design, for example, enabled the construction of buildings that use far less physical material per unit of space than, say, a half-century ago. The insights that led to central heating, as well as synthetic fiber, facilitated reduced clothing weight, while the development of the jet engine brought far greater annual passenger miles per unit of aircraft size. But doubtless it has been the advent in recent decades of the synergies of the microprocessor, lasers, and fiber optics that has fostered a distinct quickening in the displacement of physical weight of output with concepts. The ability to miniaturize transistor electronic circuits has displaced huge tonnages of copper and enhanced the speed of calculation. As high tech became an increasing part of our national product, the relative physical dimensions of our value added fell dramatically. The per capita physical weight of our gross domestic product is evidently only scarcely higher today than it was fifty or one hundred years ago. By far the largest contributor to growth of our price-adjusted GDP, or value added, has been ideas--insights that leveraged physical reality. The consequent downsizing of output, of course, meant that products were easier, and hence less costly, to move, and most especially across national borders as well as domestically. It is thus not surprising that the price-adjusted level of international trade, as I indicated earlier, has expanded at a far faster pace than gains in real domestic demand. Imports of goods and services as a percentage of gross domestic products worldwide, on average, have risen from approximately 14 percent twenty-five years ago to 24 percent today. The growth in physical weight of such trade, as with the national product generally, has been far less. Real U.S. exports of computers and other high-tech products, which pack considerable value into small volumes, have posted spectacular gains, while exports of weightier commodities have grown much less rapidly. In agriculture, exports of bulk products, such as grain, have lagged, trade having been dominated, to a considerable degree, by international transfer of the technologies for growing these commodities more efficiently. The expansion of farm exports that has taken place has tended to be concentrated more among high-value perishables, shipments of which have been bolstered, in part, by transportation and storage innovations. Technology has augmented international trade for reasons beyond the downsizing of material output. New telecommunications technologies made it very difficult for the autarchic societies of the former Soviet Union to sustain their isolation in the face of the growing relative affluence of the West. News could no longer be bottled up. Even in the West, the stultification of protectionism became increasingly evident as new consumer products entered the world markets en masse . The political pressures to deregulate moribund industries and open up borders to trade soon became irresistible. The international trading system that evolved has enhanced competition and fostered the continuous scrapping of old technologies to make way for the new. Standards of living rise because the depreciation and other cash flows of industries employing older, increasingly obsolescent, technologies are marshaled to finance the newly produced capital assets that almost always embody the cutting-edge technologies. This is the process by which wealth is created incremental step by incremental step. It presupposes a continuous churning of an economy in which the new displaces the old, and its most concrete and meaningful result is that a growing and increasingly diverse mix of goods and services is made available to consumers here and abroad. With future changes in technology likely to create greater possibilities for tailoring products that meet consumer needs more precisely, new trade opportunities may open up over time even for producers of some of the bulk commodities that have lagged. But there is also little doubt that this transition to the new high-tech economy, of which rising trade is a part, is proving difficult for a large segment of our workforce that interfaces with our rapidly changing capital stock day-by-day. Moreover, while major advances in standards of living are evident among virtually all nations that have opened their borders to increased competition, the adjustment trauma has also distressed those who once thrived in companies that were then at the cutting edge of technology, but which have since become increasingly less competitive in both domestic and foreign markets. Economists will say that workers should move from the steel districts of western Pennsylvania to a vibrant Silicon Valley. And eventually they, or more likely, their children, will. But the adjustment process is wrenching to an existing workforce made redundant largely through no fault of their own. It may be argued that all workers should have the foresight to recognize shifts in long-term job opportunities and move in advance of obsolescence. This regrettably is a skill not in great abundance--among business managers or the economists who counsel them, as well as among workers. Yet the protectionist propensity to thwart the process of the competitive flow of capital from failing technologies to the more productive is unwise and surely self-defeating. History tells us that not only is it unwise to try to hold back innovations, it is also not possible over the longer run. Generation after generation has experienced episodes in which the technologically obsolescent endeavored to undermine progress, often appealing to the very real short-term costs of adjusting to a changing economic environment. From the Luddites to the Smoots and the Hawleys, competitive forces have been attacked. In the end, the opponents of change did not prevail, and long-term advances in standards of living resumed. Nonetheless, the campaign to expand free trade is never won. Legislation to further lower trade barriers, for example, is becoming increasingly more difficult to pass in our Congress. It is a continuing battle. Further, while tariffs in industrial countries have come down sharply over the past half-century, other barriers have become more prevalent. Administrative protection in the form of antidumping suits and countervailing duties is a case in point. While these forms of protection have often been imposed under the label of promoting "fair trade," oftentimes they are just simple guises for inhibiting competition. Typically, antidumping duties are levied when foreign average prices are below average cost of production. But that also describes a practice that often emerges as a wholly appropriate response to a softening in demand. It is the rare case that prices fall below marginal cost, which would be a more relevant standard. Antidumping initiatives should be reserved, in the view of many economists, for those cases in which anticompetitive behavior is involved. Contrary to popular notions about antidumping suits, under U.S. and WTO law, it is not required to show evidence of predatory behavior, or intention to monopolize, or of any other intentional efforts to drive competitors out of business. In the end it is clear that all economic progress rests on competition. It would be a great tragedy were we to stop the wheels of progress because of an incapacity to assist the victims of progress. Our efforts should be directed at job skills enhancement and retraining--a process in which the private market is already engaged. Thwarting competition, by placing barriers to imports, will prevent the needed transitions of the productive capital stock of the United States and other nations that enable it to continuously concentrate on producing those goods and services most desired by consumers. Protectionism will also slow the inevitable transition of the workforce to more productive endeavors. To be sure, an added few years may enable some workers to reach retirement with dignity, but it will also keep frozen in place younger workers whose better job opportunities decline with time. I regret that trade policy has been inextricably linked with job creation. We too often try to promote free trade on the mistaken ground that it will create jobs. The reason should be that it enhances standards of living through the effects of competition on productivity and widens the range of choices available to consumers. It is difficult to find credible evidence that trade has affected the level of total employment in this country over the long run. Indeed, we are currently experiencing the widest trade deficit in history with a level of unemployment close to record lows. Certainly, the distribution of jobs by industry is affected by international trade--but it is also affected by domestic trade. It is the relative balance of supply and demand in a competitive market economy that determines the mix of employment. When exports fall or imports rise, domestic demand and relative prices have invariably adjusted in the long run to leave total employment relatively unaffected. As economists like to say, all imports are eventually paid for with exports. I also regret that despite the remarkable success over a near half-century of GATT, the General Agreement on Trade and Tariffs, and its successor, the World Trade Organization, in reducing trade barriers, our trade laws and negotiating practices are essentially adversarial. They presume that a trade concession extracted from us by our trading partners is to their advantage at our expense, and must be countered. Few economists see the world that way. And I am rash enough to suggest that we economists are correct, at least in this regard: Trade is not a zero sum game. If trade barriers are lowered by both parties, each clearly benefits. But if one lowers barriers and the other does not, the country that lowered barriers unilaterally will still be better off having done so. Raising barriers to achieve protectionist equality with reluctant trading partners would be neither to our benefit nor to theirs. The best of all possible worlds for competition is for both parties to lower trade barriers. The worst is for both to keep them up. For these reasons, I am concerned about the recent evident weakening of support for free trade in this country. Should we endeavor to freeze competitive progress in place, we will almost certainly slow economic growth overall and impart substantial harm to those workers who would otherwise seek more effective longer-term job opportunities. Protecting markets from new technologies has never succeeded. Adjustments to newer technologies have been delayed, but only at significant cost. Even should our trading partners not retaliate in the face of increased American trade barriers, an unlikely event, we do ourselves great harm by lessening the vigor of American competitiveness. The United States has been in the forefront of the postwar opening up of international markets, much to our, and the rest of the world's, benefit. It would be a great tragedy were that process reversed. I do not believe that will be allowed to happen. There is too much at stake for us and our trading partners.