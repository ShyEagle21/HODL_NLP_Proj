No content found
Release Date: December 2, 2004 For immediate release The Federal Reserve Board on Thursday announced the issuance of a Cease and Desist Order against Thomas C. Darden, a former institution-affiliated party of Kenco Bancshares, Inc., Jayton, Texas. The Federal Reserve Board also announced the execution of two Written Agreements. One Agreement is between William J. Collier and the Federal Reserve Bank of Dallas and the other Agreement is between Jesse L. Reese and the Federal Reserve Bank of Dallas. Both William J. Collier and Jesse L. Reese are institution-affiliated parties of Kenco Bancshares, Inc., Jayton, Texas. The Order and the Written Agreements address conduct relating to a commitment made in connection with an application involving Kenco Bancshares, Inc. Copies of the Order and the Agreements are attached.
Remarks by Governor Ben S. Bernanke Before the National Economists Club, Washington, D.C. December 2, 2004 The Logic of Monetary Policy When I speak about monetary policy at occasions like this one, more often than not the focus of my remarks is on recent policy actions or the near-term outlook. However, just as good tactics are useful only insofar as they serve a larger strategic purpose, so individual policy decisions are best understood in the context of an encompassing policy framework. Today I would like to step back a bit from current policy concerns to address the broader topic of the logical framework within which monetary policy is made. Specifically, I will introduce and briefly discuss two competing frameworks for making monetary-policy decisions, each of which has vigorous proponents among leading monetary economists, and I will relate these frameworks to policy practice at the Federal Reserve. In the latter portion of my remarks I will argue that the choice of policy framework has important practical implications, most notably for the communication policy of the central bank. Before proceeding further, I should say that the views I will express today are my own and are not necessarily shared by my colleagues in the Federal Reserve System. Car and Driver: A Misleading Analogy Eight times each year the Federal Open Market Committee (FOMC) meets to set U.S. monetary policy--which, under current operating procedures, amounts to choosing a target for the federal funds rate, a short-term interest rate that the Federal Reserve influences by controlling the supply of bank reserves. What logical framework guides these decisions? Superficially, the FOMC decision process may appear straightforward. A commonly used analogy takes the U.S. economy to be an automobile, the FOMC to be the driver, and monetary policy actions to be taps on the accelerator or brake. According to this analogy, when the economy is running too slowly (say, unemployment is high and growth is below its potential rate), the FOMC increases pressure on the accelerator by lowering its target for the federal funds rate, thereby stimulating aggregate spending and economic activity. When the economy is running too quickly (say, inflation appears likely to rise), the FOMC switches to the brake by raising its funds rate target, thereby depressing spending and cooling the economy. What could be simpler than that? I wish it were that easy. Unfortunately, the simplistic view of monetary policymaking derived from the automobile analogy can be seriously misleading, for at least two reasons. First, policymakers working to keep the economy from going off the road must deal with informational constraints that are far more severe than those faced by real-world drivers. Despite the best efforts of the statistical agencies and other data collectors, economic data provide incomplete coverage of economic activity, are subject to substantial sampling error, and become available only with a lag. Determining even the economy's current "speed," consequently, is not easy, as can be seen by the fact that economists' estimates of the nation's gross domestic product (GDP) for the current quarter may vary widely. Forecasting the economy's performance a few quarters ahead is even more difficult, reflecting not only problems of economic measurement and the effects of unanticipated shocks but also the complex and constantly changing nature of the economy itself. Policymakers are unable to predict with great confidence even how (or how quickly) their own actions are likely to affect the economy. In short, if making monetary policy is like driving a car, then the car is one that has an unreliable speedometer, a foggy windshield, and a tendency to respond unpredictably and with a delay to the accelerator or the brake. The second problem with the automobile analogy arises from the central role of private-sector expectations in determining the impact of monetary policy actions. If the automobile analogy were valid, then the current setting of the federal funds rate would summarize the degree of monetary stimulus being applied to the economy, just as the pressure a driver exerts on the accelerator at any particular moment determines whether the automobile speeds up or slows down. However, in fact, the current level of the federal funds rate is at best a partial indicator of the degree of monetary ease or restraint. The current funds rate imperfectly measures policy stimulus because the most important economic decisions, such as a family's decision to buy a new home or a firm's decision to acquire new capital goods, depend much more on longer-term interest rates, such as mortgage rates and corporate bond rates, than on the federal funds rate. Long-term rates, in turn, depend primarily not on the current funds rate but on how financial market participants expect the funds rate and other short-term rates to evolve over time. For example, if financial market participants anticipate that future short-term rates will be relatively high, they will usually bid up long-term yields as well; if long-term yields did not rise, then investors would expect to earn a higher return by rolling over short-term investments and consequently would decline to hold the existing supply of long-term bonds. Likewise, if market participants expect future short-term rates to be low, then long-term yields will also tend to be low, all else being equal. Monetary policy makers can affect private-sector expectations through their actions and statements, but the need to think about such things significantly complicates the policymakers' task (Bernanke, 2004). In short, if the economy is like a car, then it is a car whose speed at a particular moment depends not on the pressure on the accelerator at that moment but rather on the expected average pressure on the accelerator over the rest of the trip--not a vehicle for inexperienced drivers, I should think. I hope that this short discussion convinces you that making effective monetary policy is no Sunday drive in the park. With both the informational limitations facing policymakers and the role of private-sector expectations in mind, I turn next to a discussion of two alternative frameworks for thinking about monetary policy. Making Monetary Policy: Two Candidate Frameworks The two frameworks for monetary policymaking I will compare today are generally referred to in the recent economics literature as instrument rules and targeting rules (Svensson, 2003; McCallum and Nelson, 2004; Svensson, 2004b). Unfortunately, for my purposes at least, this terminology is somewhat misleading. First, the term "rule" suggests a rigid and mechanistic policy prescription that leaves no room for discretion or judgment. However, the argument that monetary policy should adhere mechanically to a strict rule, made by some economists in the past, has fallen out of favor in recent years. Today most monetary economists use the term "rule" more loosely to describe a general policy strategy, one that may include substantial scope for policymaker discretion and judgment. Here I will use the term "policy" instead of "rule" to avoid the connotations of the latter. Second, the terms "instrument" and "targeting" are products of the intellectual history of the debate and, to my mind, are not particularly descriptive. I will refer to the approaches known in the literature as instrument rules and targeting rules instead as simple feedback policies and forecast-based policies , respectively. I hope that the benefits of greater descriptive accuracy will outweigh the costs arising from any terminological confusion. How do these two policy approaches differ, and what are their underlying rationales? Under a simple feedback policy , the central bank's policy instrument--the federal funds rate in the United States--is closely linked to the behavior of a relatively small number of macroeconomic variables, variables that either are directly observable (such as employment or inflation) or can be estimated from current information (such as the economy's full-employment level of output). I exclude from my definition of simple feedback policies any policy that links the policy instrument to forecasts of macroeconomic variables, such as output and inflation; hybrid policies of this type raise difficult issues that would take me too far afield today. Generally, the macroeconomic variables that drive simple feedback policies are chosen to reflect the central bank's objectives, and policymakers are directed to adjust the short-term interest rate (or other policy instrument) as needed to offset deviations of these variables from their desired levels. That is, current macroeconomic conditions "feed back" into the setting of the short-term rate. The adjective "simple" refers to the presumption that, in this regime, policymakers will respond to only a relatively short list of economic variables. However, as I have already mentioned, contemporary advocates of simple feedback policies generally recommend applying these policies flexibly, subject to modification in special circumstances and as judgment dictates. A classic example of a simple feedback policy is the famous Taylor rule (Taylor, 1993). In its most basic version the Taylor rule is an equation that relates the current setting of the federal funds rate to two variables: the level of the output gap (the deviation of output from its full-employment level) and the difference between the inflation rate and the policy committee's preferred inflation rate. Like most feedback policies, the Taylor rule instructs policymakers to "lean against the wind"; for example, when output is above its potential or inflation is above the target, the Taylor rule implies that the federal funds rate should be set above its average level, which (all else being equal) should slow the economy and bring output or inflation back toward the desired range. Numerous simple feedback policies other than those based on the Taylor rule have been proposed and analyzed. How does the use of simple feedback policies address the issues I raised earlier--namely, the problem of limited information and the need to account for private-sector expectations when making policy? With respect to the informational constraints, advocates argue that policies of this type, if judiciously chosen, are likely to give reasonably good results even when policymakers' knowledge about the economy and its underlying structure is severely limited. The principal evidence for this claim comes from computer simulations of mathematical models of the economy. Specifically, researchers have attempted to identify particular simple feedback policies that lead to good economic outcomes when applied to a range of alternative economic models (McCallum, 1988; Taylor, 1999; Orphanides and Williams, 2002; Levin and Williams, 2003). As it turns out, simple feedback policies that produce good results in a variety of simulated economic environments--so-called robust policies--can often be found. Proponents of simple feedback policies argue that, as we are far from certain about which of many possible economic models provides the best description of the U.S. economy, the safest course is to adopt one of these robust feedback policies, modified as necessary by policymakers' insight and judgment, and to stick with it. The use of simple feedback policies also addresses to some degree the complications raised by the role of private-sector expectations. Because simple feedback policies link the central bank's policy instrument to a short list of macroeconomic variables, these policies should be relatively easy for the public to understand and to use in forming expectations of the way monetary policy will evolve in the future. Say, for example, that the feedback policy employed by the central bank stipulates that, when inflation rises 10 basis points, policymakers will raise the short-term interest rate 15 basis points on average, subject to possible judgmental adjustments. Armed with this information and their own estimates of how strong inflation pressures are likely to be over the next few quarters, financial market participants should be able to forecast future values of the short-term interest rate, allowing them to price bonds and other financial assets more efficiently. Because, under a simple feedback policy, private-sector expectations are likely to be broadly consistent with the central bank's plans, the effectiveness of monetary policy would be enhanced as well. The second general approach to making monetary policy is what I am today calling a forecast-based policy (Svensson, 2004c). As the name suggests, under a forecast-based policy regime, policymakers must predict how the economy is likely to respond in the medium term--say, over the next six to eight quarters--to alternative plans for monetary policy. For example, monetary policy makers might be interested in evaluating a strategy of keeping the federal funds rate low for a period against an alternative plan that implies a gradual rise in rates. Under a forecast-based approach, for each policy plan under consideration, the policymakers and their staffs must make their best guess of how the economy is likely to evolve should that plan be implemented. They may also try to assess the likelihood of outcomes other than their principal scenario. For example, they might conclude that a certain policy plan is likely to produce good results under most circumstances but that less-probable scenarios also exist under which the policy plan under consideration would lead to very bad results. Taking both their baseline forecast and the various risks to that forecast into account, policymakers then choose the plan that seems most likely to produce the best results overall. Their current choice of interest rate corresponds to the first step in implementing the preferred plan. This process is to be repeated at each meeting, with the policy plan being modified as necessary in response to new information or new knowledge about the economy. How do forecast-based policies deal with informational constraints and the role of expectations, the two issues I raised at the beginning of my remarks? Clearly, forecast-based policies require more information to implement than simple feedback policies, a fact that is often stressed by opponents of this approach. Indeed, opponents of this approach argue, a major risk of using forecast-based policies is that monetary policy makers, like other human beings, may be prone to thinking that they know more than they really do. Excessive optimism about what monetary policy can realistically accomplish, some claim, might conceivably lead to worse economic outcomes than would a more intellectually modest stance. In response to this critique, proponents of forecast-based policies suggest that our ability to forecast the economy, though modest, is not nil, and that we should make use of the knowledge we do have. In particular, they note, the use of forecast-based policies does not require that the policy committee members adhere strictly to a particular econometric model or economic theory. Economic forecasts, in central banks as in the private sector, typically reflect the output of a suite of models and statistical methods, plus a heavy dose of the judgment and insights of experienced economists. Forecasts can also be structured to take into account model and data uncertainty--for example by down-weighting changes in variables, such as the output gap, that are known to be poorly measured (Williams, 2004). So long as policymakers and their staffs remain appropriately humble about their forecasting ability and their knowledge of the economy, proponents argue, forecast-based policies are likely to provide better results than simple feedback policies. Supporters also point out that, in comparison with simple feedback policies, the forecast-based approach provides more guidance about how to incorporate judgment and special information into policymaking. As I have noted, advocates of both approaches agree that, in principle, good policy practice leaves scope for discretion and the use of expert judgment. But how, specifically, is this to be done? Simple feedback policies, which presume a good deal of agnosticism about the economy's underlying structure, do not provide a clear framework for answering this question, except in very qualitative terms. Under a forecast-based approach, in contrast, the answer is straightforward: Judgment or special information should affect policy choice to the extent that it affects the forecast or the risks to the forecast. For example, simulations of an econometric model may imply that policy A is likely to produce better outcomes than policy B; but if expert judgment suggests that the model's forecasts do not take special information or circumstances fully into account, or that policy A entails some economic risks not captured by the model, then policy B may be preferred instead. What about the role of private-sector expectations in determining the effect of policy? Again the information requirements of forecast-based policies are relatively more demanding. In contrast to the case of simple feedback policies, under a forecast-based policy financial market participants have no simple formula to guide them in forming expectations about future short-term rates; instead, they must infer the likely course of policy based on their own economic forecasts and their knowledge of policymakers' outlook and objectives. Given the complexity of the central bank's forecasting and policy evaluation processes, making these inferences is a daunting challenge. Clearly, under a forecast-based policy, central bankers have scope to provide considerable help to the private sector in its attempts to anticipate policy changes. To the extent that policymakers can accurately communicate their outlook, objectives, and tactics to the public, financial markets will be more efficient and monetary policy more effective (Bernanke, 2004). I will return to the issue of communication shortly. The Policy Framework of the Federal Reserve It would be nice, at this point, if I could tell you definitively whether simple feedback policies or forecast-based policies represent the superior approach to making monetary policy. But drawing strong conclusions at this juncture would be premature, to say the least. The debate in the monetary economics literature remains lively. Moreover, to some degree, central bank practice remains eclectic. At the Federal Reserve, both simple feedback and forecast-based approaches are used to provide information to policymakers. For example, FOMC members routinely compare their policy choices with both the prescriptions of various forms of the Taylor rule (as noted, a type of simple feedback policy) and the results of model simulations and forecasting exercises undertaken by staff at the Board and at the twelve Reserve Banks (as required by the forecast-based approach). Although I will not try here to resolve the deeper debate about frameworks, I can say something about the degree to which central banks rely on these approaches in practice. Both simple feedback policies and forecast-based policies influence how policymakers think about their decisions, as I have just noted. However, in my judgment, reliance on these two approaches is not symmetric; instead, the forecast-based approach has become increasingly dominant in the monetary policymaking of leading central banks. This dominance is reflected in the resources that central banks devote to data collection and modeling and in the increasing sophistication and detail of central bank forecasts. Indeed, a number of central banks with explicit inflation objectives publish regular forecasts and closely link monetary policy decisions to those forecasts. The Federal Reserve does not explicitly link policy actions to forecasts, but projections of how the economy is likely to perform under different policy plans are nevertheless central to the monetary policy process in the United States; that is, the Federal Reserve relies primarily on the forecast-based approach for making policy. To provide some evidence for this assertion, as well as some reasons for it, I draw your attention to a speech that Chairman Greenspan made earlier this year, entitled "Risk and Uncertainty in Monetary Policy" (Greenspan, 2004). To be sure, the Chairman's remarks make clear that he is profoundly aware of the uncertainties that policymakers must face in making their decisions, and he is appropriately cautious about relying too heavily on any particular model, theory, or data series. Nevertheless, his speech presents several reasons for concluding that good policies must be primarily forecast-based. These reasons include the need for preemptive policymaking, the importance of taking account of the changing structure of the economy, and the value of what he terms a risk-management approach to policy. I will discuss each of these briefly. Preemption refers to the idea that policymakers achieve better results when they act in advance to forestall developing problems. Early action works best both because monetary policy works with a lag and because developing problems (such as rising inflation) may often be defused at lower cost in their early stages. In principle, simple feedback policies are not inconsistent with a preemptive approach; however, to the extent that each episode has unique features, more information than can be captured in a simple feedback policy may be needed to deal effectively with emerging issues. Referring to monetary policy developments in the 1980s, Greenspan writes: In recognition of the lag in monetary policy's impact on economic activity, a preemptive response to the potential for building inflationary pressures was made an even more important feature of policy. As a consequence, this approach elevated forecasting to an even more prominent place in policy deliberations. Structural changes in the economy are notoriously difficult to assess as they happen, but to the extent that such changes can be identified and incorporated into the central bank's forecast, forecast-based policies are again likely to perform better than simple feedback policies. In the mid-1990s, Chairman Greenspan and his FOMC colleagues famously recognized an important structural change, an apparent increase in trend productivity growth. To quote the Chairman's speech: As a consequence of the improving trend in structural productivity growth that was apparent from 1995 forward, we at the Fed were able to be much more accommodative to the rise in economic growth than our past experiences would have deemed prudent. In short, upon determining that an important structural change was occurring, the FOMC did not feel constrained to respond to current developments in output and inflation as it had in the past--an indication that the Committee's policymaking was based on a forecast-based analysis, not a simple feedback approach. Perhaps the most interesting confirmation of the role of forecasts in Federal Reserve policymaking, however, is Chairman Greenspan's description of what he calls the risk-management approach to monetary policy. The risk-management approach is clearly a forecast-based policy. In describing the implementation of this approach, Greenspan describes how models and expert judgment are combined to project not only the most likely scenarios for the economy but also what amounts to a probability distribution of possible economic outcomes. Under the risk-management approach, policymakers choose the policy strategy that implies the most desirable of these probability distributions. To quote Greenspan one more time: Given our inevitably incomplete knowledge. . . . a central bank needs to consider not only the most likely future path for the economy but also the distribution of possible outcomes about that path. The decisionmakers then need to reach a judgment about the probabilities, costs, and benefits of the various possible outcomes under alternative choices for policy. Moreover, under the risk-management approach, models, judgments, forecasts, and policies are continually updated in light of new information, as theory would suggest. Operationally, the risk-management approach differs from the forecast-based policies described in much of the monetary economics literature in only one important respect. For simplicity, researchers have generally analyzed forecast-based policies under the assumption that policymakers care only about average economic outcomes. However, in practice, policymakers are often concerned not only with the average or most likely outcomes but also with the risks to their objectives posed by relatively low-probability events. For example, although the probability last year of a pernicious deflation in the United States was small, the potential consequences of that event were sufficiently worrisome that the possibility of its occurring could not be ignored. In that spirit, Greenspan's risk-management approach sensibly reflects the fact that the entire distribution of possible outcomes, not just the average or most likely expected outcome, matters for policy choice. This view is certainly realistic, and the analysis of forecast-based policies when the central bank cares about the whole probability distribution of potential outcomes is beginning to receive more attention in the literature (see, for example, Svensson, 2004a). To reiterate, however, the policy framework espoused by Chairman Greenspan is very much a forecast-based approach; indeed, because it requires making judgments about unlikely as well as likely economic outcomes, it places greater demands on our ability to forecast and to assess risks than do simpler forecast-based approaches that focus on average outcomes only. As I mentioned earlier, I will not try to draw definite conclusions today about the relative merits of simple feedback policies and forecast-based policies. I note, however, that not only have most central banks chosen to rely most heavily on forecast-based policies but also that the results, at least in recent years, have generally been quite good, as most economies have enjoyed low inflation and overall economic stability. So long as this good performance persists, at least, the simple feedback approach will face a sort of Catch-22 problem: Without a demonstrated record of success, central banks will be reluctant to adopt this approach; but unless some central banks begin to rely on simple feedback policies, real-world evidence in support of this approach will be lacking. Flexibility and Communication In previous talks I have argued that clear communication by the central bank is an important element of effective monetary policy (Bernanke, 2004). I will conclude with a few brief remarks about how the distinction between simple feedback policies and forecast-based policies bears on the value of central bank communication and on the relationship between central bank communication and policy flexibility. Central bank communication and transparency are important precisely because of the role of private-sector expectations in determining the effectiveness of monetary policy, a theme I have highlighted today. The economic stimulus provided by monetary policy depends mostly on longer-term interest rates, which in turn are largely determined by the expectations of financial market participants about the future course of monetary policy. As a general matter, the more guidance the central bank can provide the public about how policy is likely to evolve (or about the principles on which policy decisions will be based), the greater the chance that market participants will make appropriate inferences--and thus the greater the probability that long-term interest rates will move in a manner consistent with the outlook and objectives of the monetary policy committee. My discussion today suggests, however, that the benefit of central bank communication depends crucially on the policy approach that is used. As I have noted, if the central bank follows a simple feedback policy (with relatively few judgmental deviations), the private sector's problem of inferring future policy actions is greatly simplified. The principal task of market participants in this case would be to forecast the macroeconomic variables featured in the simple feedback policy, from which the course of policy could be inferred with reasonable accuracy. Extensive communication by the central bank may not be essential when a simple feedback policy is employed, except in those cases when policymakers decide to deviate substantially from the prescriptions of the simple feedback relationship. Under the forecast-based approach, in contrast, the public will generally find inferring the likely course of policy to be a great deal more difficult. In that regime, policy plans depend in a complex way on policymakers' outlooks, risk assessments, and objectives, which the public is unlikely to deduce accurately without guidance. Clear communication thus appears to be especially important for central banks that employ a forecast-based approach to policy--a category that includes most contemporary central banks, including the Federal Reserve. This conclusion bears in turn on the relationship between communication and policy flexibility in modern central banking. One sometimes hears the view that providing information to the public about the central bank's forecasts, plans, and objectives inhibits the flexibility of policy by effectively restricting policymakers' future choices. This claim might be correct if the current setting of the federal funds rate fully described the overall degree of monetary stimulus or restraint, in the same way that the position of the gas pedal at a particular moment fully describes the impetus that a driver is providing to his or her vehicle. In that case, central bank talk could only limit future policy options, much as providing an advance itinerary for an automobile trip may reduce the flexibility to take unplanned detours if required. However, as we have seen, the automotive analogy is a poor one. Monetary policy works largely through indirect channels--in particular, by influencing private-sector expectations and thus long-term interest rates. Consequently, failing to communicate with the public does not create genuine policy flexibility but only reduces the potency and predictability of the effects of given policy actions. To keep monetary policy both flexible and effective, particularly under a forecast-based approach to policy like that employed by the Federal Reserve, clear communication on the part of the central bank is essential. References Bernanke, Ben (2003). " 'Constrained Discretion' and Monetary Policy," speech before the Money Marketeers of New York University, New York, New York, February 3. Bernanke, Ben (2004). "Central Bank Talk and Monetary Policy," speech before the Japan Society, New York, New York, October 7. Fracasso, Andrea, Hans Genberg, and Charles Wyplosz (2001). How Do Central Banks Write? An Evaluation of Inflation Targeting Central Banks , International Center for Monetary and Banking Studies (Geneva) and the Center for Economic Policy Research (London). Greenspan, Alan (2004). "Risk and Uncertainty in Monetary Policy," remarks at the Meetings of the American Economic Association, San Diego, California, January 3. Kozicki, Sharon (1999). "How Useful Are Taylor Rules for Monetary Policy?" Federal Reserve Bank of Kansas City, Economic Review , Second Quarter, pp. 5-33. Levin, Andrew, and John Williams (2003). "Robust Monetary Policy with Competing Models," Journal of Monetary Economics , vol. 50, pp. 945-75. McCallum, Bennet (1988). "Robustness Properties of a Rule for Monetary Policy," Carnegie-Rochester Conference Series on Public Policy , vol. 29, pp. 173-203. McCallum, Bennett, and Edward Nelson (2004). "Targeting versus Instrument Rules for Monetary Policy," Carnegie Mellon University and Federal Reserve Bank of St. Louis, unpublished manuscript, February. Orphanides, Athanasios and John Williams (2002). "Robust Monetary Policy Rules with Unknown Natural Rates," Brookings Papers on Economic Activity , no. 2, pp. 63-118. Reifschneider, David, David Stockton, and David Wilcox (1997). "Econometric Models and the Monetary Policy Process," C arnegie-Rochester Conference Series on Public Policy , vol. 47, pp. 1-37. Sims, Christopher (2002). "The Role of Models and Probabilities in the Monetary Policy Process," Brookings Papers on Economic Activity , no. 2, pp. 1-62. Svensson, Lars (2001). "Requiem for Forecast-Based Instrument Rules," unpublished manuscript, Stockholm University and Princeton University, April. Svensson, Lars (2003). "What is Wrong with Taylor Rules? Using Judgment in Monetary Policy through Targeting Rules," Journal of Economic Literature , vol. 41, pp. 426-77. Svensson, Lars (2004a). "Optimal Policy with Low-Probability Extreme Events," unpublished manuscript, Princeton University, May. Svensson, Lars (2004b). "Targeting Rules versus Instrument Rules for Monetary Policy? What Is Wrong with McCallum and Nelson?" unpublished manuscript, Princeton University, August. Svensson, Lars (2004c). "Monetary Policy with Judgment: Forecast Targeting," unpublished manuscript, Princeton University, September. Taylor, John B. (1993). "Discretion versus Policy Rules in Practice," Carnegie-Rochester Conference Series on Public Policy , vol. 39, pp. 195-214. Taylor, John B., ed. (1999). Monetary Policy Rules . Chicago: University of Chicago Press for the National Bureau of Economic Research. Williams, John C. (2004). "Robust Estimation and Monetary Policy with Unobserved Structural Change," Federal Reserve Bank of San Francisco working paper 2004-11. Footnotes I thank a number of colleagues who provided constructive comments on an earlier draft. One complication that is not easily captured by the automobile analogy arises when the Fed's objectives of price stability and maximum sustainable employment come into potential conflict, as sometimes occurs over short periods (for example, following an aggregate supply shock). In that case the choice of whether to slow down or speed up the economy is not straightforward and depends on a variety of considerations, such as the stability of inflation expectations and the credibility of the central bank. This issue is not central to the points I wish to make today and so I will not discuss it further. Svensson (2001) presents some arguments against the use of feedback policies based on forecasts. McCallum and Nelson (2004) and Svensson (2004b) debate this issue. Actually, the Taylor rule has the stronger implication that the nominal funds rate should rise more than one-for-one when inflation rises, implying an increase in the real funds rate as well (this prescription is the so-called Taylor principle). The Taylor principle seems likely to be a feature of any good monetary policy and thus provides an important guidepost for policymakers. For example, one proposal would have the Federal Reserve adjust the growth of the monetary base in response to current macroeconomic conditions (McCallum, 1988). Another would replace the output gap in the Taylor rule with output growth, a variable that (unlike the output gap) can be measured directly without reference to unobserved variables like "full-employment output" (Orphanides and Williams, 2002). Robust feedback policies may also be useful when members of the policy committee disagree among themselves about how the economy works, if such a policy seems likely to produce acceptable results in each of the competing models or frameworks. Although the simulation-based literature described in the text has produced many valuable insights, it also has a number of shortcomings: First, in conducting simulation exercises, researchers of necessity can consider only a relatively small number of simple and highly stylized economic models out of the large universe of possible alternatives. Thus, simulations cannot demonstrate conclusively that any particular simple feedback policy would be robust to the types of uncertainty actually faced by policymakers. Second, although these analyses are predicated on the assumption that policymakers do not know the structure of the economy, they usually assume (somewhat inconsistently) that private agents not only know the true model but also know the central bank's feedback policy, which allows them to form accurate policy expectations. Allowing for symmetrical uncertainty on the part of policymakers and private agents is difficult analytically but might produce different results. Third, in assessing competing policies, these analyses (with some exceptions) often ignore the possibilities that policymakers will learn from their mistakes, modify or abandon policies that are producing bad results, or take into account their uncertainty about the underlying economic structure when they form their policies. More realistically, the plan might include contingent elements; for example, it might involve keeping the funds rate low unless inflation begins to rise, in which case rates would be allowed to rise as well. Stipulating a policy plan, at least in general terms, is essential in the context of forecast-based policies, as specifying only the current value of the policy rate does not provide enough information to construct a forecast. Reifschneider, Stockton, and Wilcox (1997) describe the forecasting process and its role in the policymaking process at the Federal Reserve. Sims (2002) analyzes the policy process in four major central banks, including the Federal Reserve. He describes the central role played by forecasts of inflation and output, developed by a combination of models and expert judgment. Forecasts may be contingent on different assumptions about the policy path or about the shocks hitting the economy. To an increasing degree, forecasts are accompanied with measures of uncertainty, such as the "fan charts" published by the Bank of England to describe the range of statistically probable outcomes. To obtain a more accurate assessment of the current state of the economy and to improve forecasts, central bank staffs collect and analyze large amounts of data; the hunger for data seems inconsistent with a simple feedback policy, which uses limited data inputs. Sims criticizes certain aspects of the forecasting methodologies that are used and suggests improvements, but he does not question that these central banks put forecasting at the center of their policymaking. In a comprehensive study of twenty inflation-targeting central banks, Fracasso, Genberg, and Wyplosz (2001) find that nineteen of the twenty routinely report their forecasts to the public. Forecast horizons typically range from one to two years, with two countries (New Zealand and Switzerland) extending their forecasts to three years. About half the central banks studied use fan charts to communicate the uncertainty of the forecasts. I attribute this improved performance not only to technical improvements in modeling and forecasting but, perhaps more importantly, to increased attention by policymakers to the objective of keeping inflation low and stable. The framework of what has been called constrained discretion --the idea that short-run stabilization policy must be constrained by the requirement that inflation remain low and stable--has enhanced overall stability by anchoring inflation expectations while giving policymakers some leeway to respond to short-run disturbances (Bernanke, 2003). An alternative approach to evaluating feedback policies is to compare their predictions to actual policy decisions during periods in which monetary policy was judged to be "successful." Kozicki (1999) performs this exercise for Taylor rules and concludes that they do not provide robust guides to policy. In any case, this evaluation method has the shortcoming that it does not allow for the possibility that the use of an alternative policy approach would change how the private sector forms its expectations, which in turn would affect the optimal policy path.
No content found
Financial Services Policy Committee A Committee of the Conference of Presidents Federal Reserve System David Fettig FSPC Spokesman (612) 204-5274 Federal Reserve Studies Confirm Electronic Payments Exceed Check Payments for the First Time Minneapolis, Minn., December 6, 2004--Surveys conducted by the Federal Reserve confirm that electronic payment transactions in the United States have exceeded check payments for the first time. The number of electronic payment transactions totaled 44.5 billion in 2003, while the number of checks paid totaled 36.7 billion, according to recent surveys of U.S. depository financial institutions and electronic payments organizations. Previous research by the Federal Reserve found that the number of checks paid in 2000 was 41.9 billion transactions, compared with 30.6 billion electronic payments. Electronic payments consist of such payment methods as credit cards, debit cards and automated clearinghouse (ACH) transactions, like direct debit. The decline in the number of checks paid from 41.9 billion transactions to 36.7 billion reflects an annual average rate of decline of 4.3 percent from 2000 to 2003. As for electronic forms of payment, the increase from 30.6 billion to 44.5 billion reflects an average annual rate of increase of 13.2 percent for the same period. "The balance has shifted from check writing to electronic payments, and we expect this trend to continue," said Richard Oliver, senior vice president of the Federal Reserve Bank of Atlanta and the Federal Reserve Banks' product manager for retail payments. "Indeed, at current growth rates, credit cards and debit cards will both surpass checks in terms of total annual transactions in 2007. Such rapid change presents opportunities and challenges for an industry traditionally geared toward paper-based payments. The value of these surveys is that they quantify this shift and provide important insight for all industry participants." The consists of two research efforts commissioned to estimate the annual number, dollar value, and makeup of payments in the United States, and to estimate the annual volume of electronic payments. The first survey, the Depository Institutions Payments Survey , included responses from more than 1,500 depository financial institutions (commercial banks, savings institutions and credit unions). The second research effort, the Electronic Payment Instruments Study , included responses from 68 organizations involved in originating, switching or processing electronic payments. "The Fed's 2004 Payments Study is part of an ongoing effort by the Federal Reserve System to measure trends in noncash payments in the United States," Oliver said. "This year's studies repeat critical aspects of the studies we conducted three years ago to provide a second series of point-in-time estimates from which inferences can be drawn about the rate and nature of change of the U.S. payments system." According to the Depository Institutions Payments Study , the 36.7 billion checks paid in 2003 had a total value of about $39.3 trillion. These estimates do not include checks that are written and subsequently converted to electronic transactions for clearing. Also, the study found that approximately 77 percent of checks are interbank checks, which are cleared between financial institutions, and the remaining 23 percent are so-called "on-us" checks, or those for which the financial institution of first deposit is also the paying institution. The second survey, the Electronic Payment Instruments Study , revealed that the 44.5 billion electronic payments had a dollar value of $27.4 trillion. These payments include consumer, business, and government-initiated electronic payments. Debit card transactions, with an estimated annual growth rate of 23.5 percent, are the fastest growing type of electronic payment. ACH transactions increased 13.4 percent on an annual basis and credit cards grew at a 6.7 percent rate. The relatively slow growth of credit card transactions is likely owing to its mature status as a payment option, according to Oliver. Complete reports on the 2004 Federal Reserve Payments Study can be found at . Fact Sheet Background The 2004 Federal Reserve Payments Study includes two research efforts to estimate the annual number, dollar value, and makeup of noncash payments in the United States. The study estimated the number and value of payments by check, automated clearinghouse (ACH), credit card, debit card and electronic benefits transfer (EBT). The Depository Institutions Payments Study is based on a national survey of approximately 1,500 financial institutions, and estimates the annual number and value of check and other noncash transactions in the United States. It was conducted as a joint effort of the Federal Reserve System, Global Concepts and its subcontractor International Communications Research. The Electronic Payment Instruments Study was conducted by Dove Consulting and included 2003 statistics from 68 payments organizations to estimate the annual number and value of electronic payments. Those organizations are involved in originating, switching or processing electronic payments. Findings (All are annual estimates based on survey data) Check Payments Number of check payments: 36.7 billion Value: $39.3 trillion Annual rate of decline in transactions from 2000 to 2003: 4.3 percent Electronic Payments Number of electronic payments: 44.5 billion Value: $27.4 trillion Annual growth rate of transactions from 2000 to 2003: 13.2 percent Debit Cards Number of debit card transactions: 15.6 billion Value: $0.6 trillion Annual growth rate of transactions from 2000 to 2003: 23.5 percent ACH Transactions Number of ACH payments: 9.1 billion Value: $25.1 trillion Annual growth rate of transactions from 2000 to 2003: 13.4 percent Credit Cards Number of credit card payments: 19 billion Value: $1.7 trillion Annual growth rate of transactions from 2000 to 2003: 6.7 percent
For immediate release The Federal Reserve Board on Tuesday announced its approval of the application and notice under sections 3 and 4 of the Bank Holding Company Act for Park National Corporation, Newark, Ohio, to effect the following transactions: (1) merge with First Federal Bancorp, Inc. and thereby acquire its wholly owned federal savings bank, First Federal Savings Bank of Eastern Ohio ("FFSB"), both in Zanesville, Ohio; and (2) control Century National Bank, Zanesville, Ohio, after FFSB converts to a national bank and merges with Century National Bank. Attached is the Board's Order relating to this action.
Remarks by Governor Susan Schmidt Bies At the International Center for Business Informations Risk Management 2004 Conference Geneva, Switzerland December 7, 2004 Its Not Just about the Models: Recognizing the Importance of Qualitative Factors in an Effective Risk-Management Process Good afternoon. I am delighted to join you today. I have spent much of my career in the field of risk management, and of course the Federal Reserve has a keen interest in this topic. For those of us who have spent more than a few years in the business, it is easy to see the recent progress in the quantitative or scientific aspects of risk management brought about by improved databases and technological advances. These increased capabilities have opened doors and minds to new ways of measuring and managing risk. These advances have made possible the development of new markets and products that are widely relied upon by both financial and nonfinancial firms, and that in turn have helped to promote the adoption of the best risk measurement and management practices. They have also made the practice of risk management far more sophisticated and complex. These changes have come about because of better risk-measurement techniques and have the potential, I believe, to substantially improve the efficiency of U.S. and world financial markets. Although the importance of the quantitative aspects of risk measurement may be quite apparent--at least to practitioners of the art--the importance of the qualitative aspects may be less so. In practice, though, these qualitative aspects are no less important to the successful operation of a business--as events continue to demonstrate. Some qualitative factors--such as experience and judgment--affect what one does with model results. It is important that we not let models make the decisions, that we keep in mind that they are just tools, because in many cases it is management experience--aided by models to be sure--that helps to limit losses. Some qualitative factors affect what the models and risk measures say, including the parameters and modeling assumptions used as well as choices that relate to the characteristics of the underlying data and the historical period from which the data are drawn. In my comments today, I will address three topics. First, sound risk management is more than technical skill in building internal models. Models of risk need to be integrated into a robust enterprise-wide program that encompasses even line management's routine business practices. Second, regular testing of "data integrity" in its broadest sense as it relates to these risk measurement and management processes is essential to the effectiveness of these processes. Finally, I want to raise some issues around accounting and disclosure of risk. If there is a single theme to my remarks today, it is simply this: Keep improving, refining, and innovating in risk management. Although Basel II is a remarkable achievement, and the subject of this conference, don't let your best practices be limited by what Basel II does or requires. Indeed, one of the more desirable aspects of Basel II is that it anticipates that it can evolve with best industry practices without creating a new framework. The designers have not intended to build a straitjacket, and the policymakers have insisted on this characteristic, which my colleague Vice Chairman Ferguson has referred to as Basel II's "evergreen" aspect. Enterprise-wide risk management Within financial institutions, the better and greater focus on risk measurement has helped to bridge the gap between the perspective of the traditional credit risk officer and that of the "quant." This is no small accomplishment, and a significant advance in credit culture. If you will pardon my use of stereotypes, historically the credit risk officer has been the fellow who always says "no" because it is the conservative thing to do, while the financial modeler has been the proponent of active-trading strategies that fulfill the promise of a data-mined efficient frontier that has been estimated to several decimal places of precision. The logic of risk and return in a competitive marketplace, as measured by return on economic capital that is founded in empirical analysis, has provided both sides with a common language and set of standards. It is not unusual anymore to hear chief credit officers describe their appetite for risk in terms of risk-adjusted return on capital (RAROC) or monitor current spreads on credit default swaps to look for market signals on borrower credit quality. With better credit cultures and improved tools, institutions can also measure and evaluate more objectively the results of their business strategies and use that information to enhance future performance. They can decide how much risk to take, rather than letting their risk profile be the consequence of other decisions. The evolution of interest rate risk management in the United States is a great illustration of how an enterprise-wide approach can help institutions customize products that better serve customers, set prices to reflect risk exposures and attain profit targets, and ensure that corporate earnings contributions are met. Thirty years ago, bankers who were used to taking fixed-rate deposits--capped under the old Regulation Q ceilings--and making fixed-rate term loans, found the cost of their deposits rising with the market after short-term rates rose dramatically late in 1979. Financial institutions found that, to meet market interest rates, they had to pay higher rates of interest on deposits than they were receiving on loans. As a banker, I went through that period in 1980 when the popular new six-month CDs that were booked in March, at annualized interest rates of around 15 percent, were funding loans at a negative carry when the prime rate fell to 11 percent by August. The roller coaster continued as lower CD rates in the second half of 1980 were funding loans at a prime rate of more than 20 percent by January 1981. One of the first challenges bankers faced in this environment was developing the information and analytical systems needed to manage the institution's overall interest rate sensitivity. So, in the early 1980s, taking advantage of the newly emerging computer technology and software, they developed asset-liability management models that integrated information on deposit and loan repricing. Further, the management committees responsible for interest rate risk changed. Instead of committees that included only management from the funding-desk and investment-portfolio management, a new group was created--the Asset/Liability Committee, or ALCO. This committee included the old finance committee members and new ALCO staff but also, most important, added business-line managers responsible for major corporate and retail banking activities. For the first time, pricing of loans and deposits was moved from the silos of business-line management, recognizing that the enterprise as a whole had to coordinate balance sheet usage in order to maintain the net interest margin around a targeted level. While this enterprise-wide approach to market risk evolved at varying paces in different depository institutions, by the latter part of the 1980s all of the basic elements were in place and, by the 1990s, the process had matured. The ALCO process is now widely recognized as a critical element in both management and board governance processes. This discipline emerged not only because of better asset-liability risk measures, but also because these new risk measurement and management techniques and computer technology facilitated rapid innovation in financial instruments. The industry turned to new securitization techniques to pool mortgages and remove the interest rate risk from balance sheets, techniques that eventually expanded to other loan types as well. Interest rate derivatives, structured investment securities, and callable debt have allowed financial institutions to meet customer demands more effectively while managing the liquidity and interest rate risk exposures those relationships entail. Enterprise-wide market-risk management became a value-added activity and has been widely accepted as a critical element of the governance and strategic processes at financial institutions. The example I presented of how effective asset-liability management committees and processes can support business-line strategies as well as governance, is intended to illustrate that effective enterprise-wide risk-management processes are not built just to comply with regulations, such as banking regulations or Sarbanes-Oxley requirements in the United States. Rather, these processes can add value when they become an integral part of both strategic and tactical business-decision processes. Corporate strategies often focus on the "most likely" future scenario and the benefits of a strategic initiative. A sound governance, risk-management, and internal-control environment starts by stretching the strategic planning exercise to consider alternative outcomes. That is, while the strategy is being developed, management and the board should consider how risk exposures will change as part of the planning process. Then, appropriate controls can be built into the process design, the costs of errors and rework in the initial rollout can be reduced, and the ongoing initiative can be more successful because monitoring processes can signal when activities and results are missing their intended goals and corrective actions can be initiated more promptly. According to a global survey of governance at financial institutions conducted by PricewaterhouseCoopers (and reported in April), one of the reasons financial institutions are not making the grade is that they equate effective governance with meeting the demands of regulators and legislators, without recognizing that sound governance is also good for business. In other words, they tend to look at this as another compliance exercise. The study goes on to state that this compliance mentality is limiting these institutions' ability to achieve strategic advantages through governance. I agree that any institution that views corporate governance as merely a compliance exercise is missing the mark. Over the years, corporate managers have learned that focusing on better process management and quality can enhance financial returns and customer satisfaction. They have learned that correcting errors, downtime in critical systems, and undertraining of staff all result in higher costs and lost revenue opportunities. I challenge you to consider the corporate governance structure appropriate to your bank's unique business strategy and scale as an important investment, and to consider returns on that investment in terms of the avoidance of the costs of poor internal controls and of customer dissatisfaction. As you know, once an organization gets lax in its approach to corporate governance, problems tend to follow. We have some experience in that regard. Some of you may recall the time and attention that management of U.S. banks devoted to section 112 of the Federal Deposit Insurance Corporation Improvement Act, which first required bank management reports on internal controls and auditor attestations in the early 1990s. Then the process became routine, delegated to lower levels of management and unresponsive to changes in the way the business was being run. Unfortunately, for organizations with weak governance, trying to change the culture again to meet Sarbanes-Oxley requirements is taking an exceptional amount of senior management and directors' time--time taken away from building the business. The challenge, therefore, is not only to achieve the proper control environment at one point in time, but also to maintain that discipline and, indeed, ensure that corporate governance keeps pace with the changing risks that you will face in the coming years. One weakness we have seen is the delegation by management of both the development and the assessment of the internal-control structure to the same risk-management, internal-control, or compliance group. It is important to emphasize that line management has the responsibility for identifying risks and ensuring that the mitigating controls are effective--and to leave the assessments to a group that is independent of that line organization. Managers should be expected to evaluate the risks and controls within their scope of authority at least annually and to report the results of this process to the chief risk officer and the audit committee of the board of directors. An independent group, such as internal audit, should perform a separate assessment to confirm management's assessment. Credit risk management The evolution of a portfolio approach to credit risk management has followed a path similar to that of asset-liability management. It began in the late 1980s, in the aftermath of serious credit-quality deterioration. Models and databases on defaults and credit spreads have since become more sophisticated, and loan review committees have evolved into committees that consider more broadly the various aspects of portfolio risk management. As a result, loans are priced better to reflect their varying levels of risk, they are syndicated and securitized to mitigate lenders' risk, and credit derivatives have been created to limit credit-risk exposures that are retained. On that last topic, I would be remiss not to draw your attention to a recent consultative paper on credit-risk transfer issued by the Joint Forum in October. This consultative paper focuses on credit derivatives and related transactions--themselves an outgrowth of better risk measurement--and is open to public comment through January 2005. It documents the remarkable growth and innovation in these credit products, with aggregate notional value of $2.3 trillion and with about 1,200 regularly traded reference entities or "names." The paper emphasizes that much more growth is likely, because these products are still in the early phases of their life cycle, and that the most important issue now facing market participants is the continuing development of their risk measurement and management capabilities. In that context, the consultative paper responds to three questions: whether the transactions accomplish a clean transfer of risk, whether participants understand the risks involved, and whether undue concentrations of risk are developing. Overall, the paper offers seventeen specific recommendations for improving the practice and supervisory oversight of credit risk transfer activity, drawing heavily on discussions with sophisticated market participants. I encourage you to review this paper and consider its recommendations seriously. Let me discuss two of them briefly. One recommendation is that firms understand fully and apply discipline to their credit models in order to ensure quality and manage the usage of these models appropriately. Correlation assumptions receive special attention here, including the growing presence of "correlation trading desks" and the observation from several market participants that there may be too much commonality in these assumptions across market participants. Insufficient diversity in views could lead to the kind of turmoil that occurred in markets for longer-term Treasury instruments in mid-2003. In that case, unexpected increases in rates led a large number of similarly positioned financial institutions to seek to take the same side of transactions simultaneously. Another recommendation is that participants properly understand the economic meaning of external ratings that are applied to credit risk transfer instruments, especially collateralized debt obligations or CDOs--as compared with ratings given to more traditional obligations. Identical ratings across different types of instruments do not guarantee identical risk characteristics, and in particular may imply equal probability of a loss event but unequal severity of loss. It is important to understand both the specific methodology used by the rating agency--which the agencies make available in extensive detail, including how default correlation is addressed--and the structure of a specific transaction in order to properly assess its contribution to a portfolio's risk profile. Data integrity, broadly defined Even the best of processes suffers if the data used to measure risk and performance are flawed. In understanding the drivers of good risk management, qualitative factors are a critical influence on the reliability and characteristics of the "data" used to evaluate risk and performance. In this broader sense, "data integrity" can refer not only to the consistency, accuracy and appropriateness of the information in the data base and model, but also to the processes that produce and utilize these measures. Used this way, "data integrity" includes the quality of credit files, tracking of key customer characteristics, internal processes and controls, and even the training that supports them all. When one says "data integrity" in risk-management circles these days, most people think of the qualifying standards for the internal-ratings-based approaches to credit risk capital under Basel II. I think it is a broader concept, so let me spend a moment on that subject. The proposed timetable for U.S. implementation of Basel II reflects the requirements of our rulemaking processes and the need for banks and supervisors to prepare for the introduction of the new standards. As you probably know, the U.S. banking agencies envision formal release of the proposed regulations in mid-2005, with parallel running of Basel I and Basel II in 2007 and full implementation in 2008. Even on that timetable, the regulatory community recognizes that substantial data limitations may prevent banks from developing viable and robust parameter estimates in the near term--even for probability of default in some cases. For this reason, both banks and their supervisors will have to wait while data accumulate before banks can estimate and validate parameter inputs in a reliable, robust manner. In the interim, banks and supervisors will have to rely heavily on qualitative validation approaches--although not entirely. Supervisors across countries are working together to address validation issues, and, I believe, will develop useful guidelines for banks and supervisors alike. In the early years, more weight may need to be placed on qualitative reviews of a bank's internal policies and procedures, including its internal validation and documentation. But we expect that, soon after implementation, banks should have the ability to generate the needed parameters from actual data, and supervisors will want to see positive steps being taken by banking organizations to develop good databases to provide the sort of data integrity I am discussing. Qualitative and quantitative benchmarking studies, which compare methodologies and parameter estimates across banks, will be important tools for validation and for encouraging the diffusion of best practices throughout the industry during both the initial, more qualitative and the later more quantitative, intervals. But, as I noted, high-quality data are important for strong risk management, and not just for Basel II. Data are needed for other models and risk measures used in financial services, including credit scoring models, market-based measures such as KMV, and value-at-risk and other economic capital models. As you know, these economic capital models are a key element of Pillar 2. The broader concept of data integrity also applies to the development and maintenance of well-controlled processes including those that measure risk and performance. If the environment in which the models operate is not appropriate--if an institution considers internal controls just to be a checklist--its risk measures will not provide the performance it hopes to achieve. Accounting, disclosure, and market discipline Strong risk measurement and disciplined maintenance of data also improve the communication between the institution and its investors and counterparties. This sense of data integrity relates just as well to the information provided to these parties. I would like, now, to turn to some of the recent accounting issues surrounding complex instruments and the role of financial disclosure in promoting risk management. Some of you may have experienced earnings volatility resulting from the use of credit derivatives. Under U. S. generally accepted accounting principles, credit derivatives are generally required to be recognized as an asset or liability and measured at fair value, and the gain or loss resulting from the change in fair value must be recorded in earnings. Most credit derivatives do not qualify for hedge accounting treatment, implying greater earnings volatility if the hedged portfolio or securities are carried at historic cost in their banking book. As a bank supervisor, I am concerned if the accounting treatment discourages the use of new risk-management financial instruments. You may be wondering if the answer to this volatility issue is fair value accounting. If the hedged asset were measured at fair value, the changes in values of the hedged item and the credit derivative would offset each other, reducing the volatility that arises when only the derivative is marked to market, depending of course on the effectiveness of the hedge. But some volatility is likely to remain, since it is the lack of close correlation that prevents hedge accounting treatment. The IASB developed the new "fair value option" under International Accounting Standard (IAS) 39, under which firms could mark to market both the credit derivative and the hedged position and report changes in their fair values in current earnings. While at first glance the fair value option might be viewed as the solution to addressing the problems of the current accounting model, it also raises a number of concerns. Without observable market prices and sound valuation approaches, fair value measurements are difficult to determine, verify, and audit. Reporting would become less comparable across institutions. Moreover, if an entity's creditworthiness deteriorates significantly, there is potentially a peculiar result. In this circumstance, financial liabilities would be marked down to fair value and a gain would be recorded in the entity's profit and loss statement. In the most dramatic case, an insolvent entity might appear solvent as a result of marking to market its own deteriorated credit risk. Many of these concerns, as well as recommendations to address them, were included in a July comment letter to the IASB from the Basel Committee. As heavy users of customer and investor corporate financial statements, bankers should also consider how a fair value measure may mask underlying reasons for the change in fair value. As institutions using IASB standards consider how to use the fair value option for their own financial reporting purposes, they should be aware of certain related complexities. For example, if loans are reported using the fair value option, changes in fair value would presumably affect loan loss allowances and thus regulatory capital, important asset-quality measures like nonperforming assets, and even net interest margins. One area in which improved disclosures by banking organizations are needed involves credit risk and the allowance for loan losses. As you know, a high degree of management judgment is involved in estimating the loan-loss allowance, and that estimate can have a significant impact on an institution's balance sheet and earnings. Expanded disclosures in this area would improve market participants' understanding of an institution's risk profile and whether the firm has adequately provided for its estimated credit losses in a consistent, well-disciplined manner. Accordingly, I strongly encourage institutions to provide additional disclosures in this area. Examples include a breakdown of credit exposures by internal credit grade, allowance estimates broken down by key components, more-thorough discussions of why allowance components have changed from period to period, and enhanced discussions of the rationale behind changes in the more-subjective allowance estimates, including unallocated amounts. Thus, as both users and preparers of financial statements, bankers should encourage transparency in accounting and disclosure of risk positions. Conclusion In my remarks today, I have sought to encourage you to continue your efforts to support the evolution of risk measurement and risk management practices and to heighten the degree of professionalism that every effective risk manager should demonstrate. I want to leave by reminding all of you not to become so caught up in the latest technical development that you lose sight of the qualitative aspects of your responsibilities. Models alone do not guarantee an effective risk-management process. You should encourage continuous improvement in all aspects, including data integrity, legal clarity, transparent disclosures, and internal controls. For the risk managers at this conference, I hope the message you have heard is that you should be actively engaged with managers throughout the organization, talking about the merits of a consistent, sound enterprise-wide risk management culture. In doing so, you can help managers see that the risk-management process will allow them to better understand the inherent risks of their activities so that they in turn can more effectively mitigate these risks and achieve their profit goals. Footnotes PricewaterhouseCoopers and the Economist Intelligence Unit, (April 2004). A copy of the letter, dated July 30, 2004, can be found at
No content found
No content found
Joint Press Release Board of Governors of the Federal Reserve System Federal Deposit Insurance Corporation Office of the Comptroller of the Currency Office of Thrift Supervision For Immediate Release December 16, 2004 Agencies Propose Data Collection Changes for Shared National Credits The federal banking and thrift regulatory agencies today requested public comment on proposed changes to the data collection process that supports the Shared National Credit review of large syndicated loans. The program, which has been in place since 1977, is an interagency examination and supervision effort designed to evaluate loan commitments aggregating $20 million or more that are shared by three or more supervised institutions. The program provides a process to assign uniform credit ratings for shared national credits in addition to collecting and analyzing data that regulators use to monitor credit conditions and trends at the nation's largest banks. The proposed data collection changes would enable the agencies to improve the efficiency and effectiveness of credit reviews, support continued risk-focusing efforts in the program, and provide comparative credit risk information to banks and regulatory supervisors. Under the proposal, the data collection changes would be implemented with the 2007 review, employing data as of December 31, 2006. The interagency notice is attached. Comments are requested by February 15, 2005. # # # Media Contacts: Federal Reserve Andrew Williams 202-452-2955 FDIC David Barr 202-898-6992 OCC Kevin Mukri 202-874-5770 OCC Erin Hickman 202-906-6677
Joint Press Release Board of Governors of the Federal Reserve System Federal Deposit Insurance Corporation Office of the Comptroller of the Currency Office of Thrift Supervision For Immediate Release December 21, 2004 Agencies Announce Final Rules on Disposal of Consumer Information The federal bank and thrift regulatory agencies today announced interagency final rules to require financial institutions to adopt measures for properly disposing of consumer information derived from credit reports. Current law requires financial institutions to protect customer information by implementing information security programs. The final rules require institutions to make modest adjustments to their information security programs to include measures for the proper disposal of consumer information. They also add a new definition of "consumer information." The agencies' final rules implement section 216 of the Fair and Accurate Credit Transactions Act of 2003 (FACT Act) and include this new statutory requirement in the Interagency Guidelines Establishing Standards for Safeguarding Customer Information (retitled the Interagency Guidelines Establishing Standards for Information Security ), which were adopted in 2001. The final rules will take effect on July 1, 2005. The Federal Register notice is attached. # # # Agencies Announce Final Rules on Disposal of Consumer Information The federal bank and thrift regulatory agencies today announced interagency final rules to require financial institutions to adopt measures for properly disposing of consumer information derived from credit reports. Current law requires financial institutions to protect customer information by implementing information security programs. The final rules require institutions to make modest adjustments to their information security programs to include measures for the proper disposal of consumer information. They also add a new definition of "consumer information." The agencies' final rules implement section 216 of the Fair and Accurate Credit Transactions Act of 2003 (FACT Act) and include this new statutory requirement in the Interagency Guidelines Establishing Standards for Safeguarding Customer Information (retitled the Interagency Guidelines Establishing Standards for Information Security ), which were adopted in 2001. The final rules will take effect on July 1, 2005. The Federal Register notice is attached. # # # Media Contacts: Federal Reserve Susan Stawick 202-452-2955 FDIC David Barr 202-898-6992 OCC Dean DeBuck 202-874-5770 OTS Erin Hickman 202-906-6677